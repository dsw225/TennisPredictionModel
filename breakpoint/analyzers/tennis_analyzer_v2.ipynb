{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32397\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "RD_CUTOFF = 125\n",
    "\n",
    "df = pd.read_csv('../testcsvs/glickoUpdated.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_surface_glicko_rd'] <= RD_CUTOFF) & (df['b_surface_glicko_rd'] <= RD_CUTOFF) & (df['a_glicko_rd'] <= RD_CUTOFF) & (df['b_glicko_rd'] <= RD_CUTOFF)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug', 'sets', 'games', 'tiebreaks'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>tourney_round</th>\n",
       "      <th>a_player_age</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_age</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>...</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_surface_return_second_won_glicko_rd</th>\n",
       "      <th>b_surface_second_won_glicko_rd</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-156.391443</td>\n",
       "      <td>1648.533408</td>\n",
       "      <td>1804.924851</td>\n",
       "      <td>73.590383</td>\n",
       "      <td>...</td>\n",
       "      <td>1509.676357</td>\n",
       "      <td>1525.177533</td>\n",
       "      <td>68.224718</td>\n",
       "      <td>63.306180</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.779397</td>\n",
       "      <td>1684.801531</td>\n",
       "      <td>1682.022134</td>\n",
       "      <td>68.576543</td>\n",
       "      <td>...</td>\n",
       "      <td>1520.481105</td>\n",
       "      <td>1493.441526</td>\n",
       "      <td>64.043825</td>\n",
       "      <td>72.508115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-61.159514</td>\n",
       "      <td>1621.512570</td>\n",
       "      <td>1682.672084</td>\n",
       "      <td>76.007944</td>\n",
       "      <td>...</td>\n",
       "      <td>1506.545429</td>\n",
       "      <td>1507.882055</td>\n",
       "      <td>67.307645</td>\n",
       "      <td>65.888687</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5381</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-65.573917</td>\n",
       "      <td>1697.467187</td>\n",
       "      <td>1763.041103</td>\n",
       "      <td>68.133731</td>\n",
       "      <td>...</td>\n",
       "      <td>1517.206849</td>\n",
       "      <td>1534.732931</td>\n",
       "      <td>63.710167</td>\n",
       "      <td>64.101412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-75.476705</td>\n",
       "      <td>1697.646993</td>\n",
       "      <td>1773.123697</td>\n",
       "      <td>71.445373</td>\n",
       "      <td>...</td>\n",
       "      <td>1494.807275</td>\n",
       "      <td>1535.923504</td>\n",
       "      <td>68.493754</td>\n",
       "      <td>63.744405</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  tourney_round  a_player_age  a_player_rank  b_player_age  \\\n",
       "5373      3.0            0.8          28.0           74.0          23.0   \n",
       "5375      3.0            0.8          28.0           65.0          25.0   \n",
       "5378      3.0            0.8          26.0           83.0          24.0   \n",
       "5381      3.0            0.8          28.0           65.0          27.0   \n",
       "5382      3.0            0.8          29.0           57.0          27.0   \n",
       "\n",
       "      b_player_rank  glicko_rating_diff  a_glicko_rating  b_glicko_rating  \\\n",
       "5373           15.0         -156.391443      1648.533408      1804.924851   \n",
       "5375           46.0            2.779397      1684.801531      1682.022134   \n",
       "5378           48.0          -61.159514      1621.512570      1682.672084   \n",
       "5381           38.0          -65.573917      1697.467187      1763.041103   \n",
       "5382           38.0          -75.476705      1697.646993      1773.123697   \n",
       "\n",
       "      a_glicko_rd  ...  a_surface_return_second_won_glicko_rating  \\\n",
       "5373    73.590383  ...                                1509.676357   \n",
       "5375    68.576543  ...                                1520.481105   \n",
       "5378    76.007944  ...                                1506.545429   \n",
       "5381    68.133731  ...                                1517.206849   \n",
       "5382    71.445373  ...                                1494.807275   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  \\\n",
       "5373                         1525.177533   \n",
       "5375                         1493.441526   \n",
       "5378                         1507.882055   \n",
       "5381                         1534.732931   \n",
       "5382                         1535.923504   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rd  b_surface_second_won_glicko_rd  \\\n",
       "5373                              68.224718                       63.306180   \n",
       "5375                              64.043825                       72.508115   \n",
       "5378                              67.307645                       65.888687   \n",
       "5381                              63.710167                       64.101412   \n",
       "5382                              68.493754                       63.744405   \n",
       "\n",
       "      a_odds  b_odds  a_b_win  surface_Clay  surface_Grass  surface_Hard  \n",
       "5373    3.59    1.28      0.0           0.0            0.0           1.0  \n",
       "5375     NaN     NaN      1.0           0.0            0.0           1.0  \n",
       "5378    1.54    2.40      1.0           0.0            0.0           1.0  \n",
       "5381     NaN     NaN      0.0           0.0            0.0           1.0  \n",
       "5382    2.34    1.56      0.0           0.0            0.0           1.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6096, Val Loss: 0.6006, Accuracy: 67.29%\n",
      "Epoch [20/100], Loss: 0.6053, Val Loss: 0.5945, Accuracy: 68.31%\n",
      "Epoch [30/100], Loss: 0.5982, Val Loss: 0.5941, Accuracy: 67.61%\n",
      "Epoch [40/100], Loss: 0.5960, Val Loss: 0.5944, Accuracy: 68.50%\n",
      "Epoch [50/100], Loss: 0.5916, Val Loss: 0.5934, Accuracy: 67.80%\n",
      "Epoch [60/100], Loss: 0.5912, Val Loss: 0.5931, Accuracy: 67.48%\n",
      "Epoch [70/100], Loss: 0.5877, Val Loss: 0.5932, Accuracy: 67.54%\n",
      "Epoch [80/100], Loss: 0.5897, Val Loss: 0.5933, Accuracy: 67.67%\n",
      "Epoch [90/100], Loss: 0.5874, Val Loss: 0.5932, Accuracy: 67.73%\n",
      "Epoch [100/100], Loss: 0.5923, Val Loss: 0.5933, Accuracy: 67.67%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6049, Val Loss: 0.6072, Accuracy: 66.71%\n",
      "Epoch [20/100], Loss: 0.6067, Val Loss: 0.6012, Accuracy: 67.16%\n",
      "Epoch [30/100], Loss: 0.5969, Val Loss: 0.6025, Accuracy: 66.90%\n",
      "Epoch [40/100], Loss: 0.5936, Val Loss: 0.6020, Accuracy: 66.90%\n",
      "Epoch [50/100], Loss: 0.5970, Val Loss: 0.6028, Accuracy: 66.77%\n",
      "Epoch [60/100], Loss: 0.5921, Val Loss: 0.6021, Accuracy: 66.90%\n",
      "Epoch [70/100], Loss: 0.5978, Val Loss: 0.6021, Accuracy: 66.52%\n",
      "Epoch [80/100], Loss: 0.5936, Val Loss: 0.6017, Accuracy: 66.77%\n",
      "Epoch [90/100], Loss: 0.5969, Val Loss: 0.6015, Accuracy: 66.77%\n",
      "Epoch [100/100], Loss: 0.5935, Val Loss: 0.6019, Accuracy: 66.77%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6043, Val Loss: 0.6060, Accuracy: 66.11%\n",
      "Epoch [20/100], Loss: 0.6039, Val Loss: 0.6025, Accuracy: 66.50%\n",
      "Epoch [30/100], Loss: 0.5997, Val Loss: 0.6007, Accuracy: 66.18%\n",
      "Epoch [40/100], Loss: 0.5971, Val Loss: 0.6006, Accuracy: 65.92%\n",
      "Epoch [50/100], Loss: 0.5882, Val Loss: 0.6011, Accuracy: 65.28%\n",
      "Epoch [60/100], Loss: 0.5919, Val Loss: 0.5998, Accuracy: 65.86%\n",
      "Epoch [70/100], Loss: 0.5876, Val Loss: 0.5998, Accuracy: 65.79%\n",
      "Epoch [80/100], Loss: 0.5897, Val Loss: 0.5996, Accuracy: 65.86%\n",
      "Epoch [90/100], Loss: 0.5871, Val Loss: 0.5992, Accuracy: 66.05%\n",
      "Epoch [100/100], Loss: 0.5918, Val Loss: 0.5995, Accuracy: 65.98%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6092, Val Loss: 0.5979, Accuracy: 67.26%\n",
      "Epoch [20/100], Loss: 0.6039, Val Loss: 0.5959, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.5989, Val Loss: 0.5950, Accuracy: 66.75%\n",
      "Epoch [40/100], Loss: 0.5982, Val Loss: 0.5954, Accuracy: 66.62%\n",
      "Epoch [50/100], Loss: 0.5956, Val Loss: 0.5956, Accuracy: 66.30%\n",
      "Epoch [60/100], Loss: 0.5991, Val Loss: 0.5956, Accuracy: 66.56%\n",
      "Epoch [70/100], Loss: 0.5983, Val Loss: 0.5955, Accuracy: 66.50%\n",
      "Epoch [80/100], Loss: 0.5972, Val Loss: 0.5957, Accuracy: 66.56%\n",
      "Epoch [90/100], Loss: 0.5952, Val Loss: 0.5957, Accuracy: 66.37%\n",
      "Epoch [100/100], Loss: 0.5986, Val Loss: 0.5954, Accuracy: 66.50%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6142, Val Loss: 0.6010, Accuracy: 67.65%\n",
      "Epoch [20/100], Loss: 0.6036, Val Loss: 0.6007, Accuracy: 67.26%\n",
      "Epoch [30/100], Loss: 0.5960, Val Loss: 0.5991, Accuracy: 66.94%\n",
      "Epoch [40/100], Loss: 0.5941, Val Loss: 0.5974, Accuracy: 67.52%\n",
      "Epoch [50/100], Loss: 0.5959, Val Loss: 0.5977, Accuracy: 67.26%\n",
      "Epoch [60/100], Loss: 0.5901, Val Loss: 0.5976, Accuracy: 67.26%\n",
      "Epoch [70/100], Loss: 0.5958, Val Loss: 0.5977, Accuracy: 67.20%\n",
      "Epoch [80/100], Loss: 0.5926, Val Loss: 0.5975, Accuracy: 67.07%\n",
      "Epoch [90/100], Loss: 0.5939, Val Loss: 0.5983, Accuracy: 66.88%\n",
      "Epoch [100/100], Loss: 0.5937, Val Loss: 0.5976, Accuracy: 67.14%\n",
      "Best cross-validated accuracy: 68.69%\n",
      "Epoch [10/100], Loss: 0.5973, Val Loss: 0.5764, Accuracy: 69.31%\n",
      "Epoch [20/100], Loss: 0.5911, Val Loss: 0.5761, Accuracy: 68.85%\n",
      "Epoch [30/100], Loss: 0.5915, Val Loss: 0.5754, Accuracy: 68.90%\n",
      "Epoch [40/100], Loss: 0.5889, Val Loss: 0.5760, Accuracy: 68.70%\n",
      "Epoch [50/100], Loss: 0.5889, Val Loss: 0.5752, Accuracy: 69.21%\n",
      "Epoch [60/100], Loss: 0.5890, Val Loss: 0.5754, Accuracy: 68.80%\n",
      "Epoch [70/100], Loss: 0.5927, Val Loss: 0.5754, Accuracy: 68.95%\n",
      "Epoch [80/100], Loss: 0.5863, Val Loss: 0.5756, Accuracy: 68.60%\n",
      "Epoch [90/100], Loss: 0.5876, Val Loss: 0.5750, Accuracy: 68.95%\n",
      "Epoch [100/100], Loss: 0.5881, Val Loss: 0.5752, Accuracy: 69.01%\n",
      "Final model accuracy on test set: 69.47%\n",
      "Accuracy 69.47% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6111, Val Loss: 0.5988, Accuracy: 68.31%\n",
      "Epoch [20/100], Loss: 0.6062, Val Loss: 0.5975, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.5988, Val Loss: 0.5933, Accuracy: 68.25%\n",
      "Epoch [40/100], Loss: 0.5978, Val Loss: 0.5937, Accuracy: 67.99%\n",
      "Epoch [50/100], Loss: 0.5975, Val Loss: 0.5932, Accuracy: 67.93%\n",
      "Epoch [60/100], Loss: 0.5962, Val Loss: 0.5929, Accuracy: 68.18%\n",
      "Epoch [70/100], Loss: 0.5972, Val Loss: 0.5931, Accuracy: 67.99%\n",
      "Epoch [80/100], Loss: 0.5928, Val Loss: 0.5925, Accuracy: 68.12%\n",
      "Epoch [90/100], Loss: 0.5979, Val Loss: 0.5928, Accuracy: 68.05%\n",
      "Epoch [100/100], Loss: 0.5948, Val Loss: 0.5928, Accuracy: 67.99%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6081, Val Loss: 0.6044, Accuracy: 66.52%\n",
      "Epoch [20/100], Loss: 0.6025, Val Loss: 0.6027, Accuracy: 67.09%\n",
      "Epoch [30/100], Loss: 0.5939, Val Loss: 0.6012, Accuracy: 67.03%\n",
      "Epoch [40/100], Loss: 0.5909, Val Loss: 0.6016, Accuracy: 66.97%\n",
      "Epoch [50/100], Loss: 0.5966, Val Loss: 0.6009, Accuracy: 66.97%\n",
      "Epoch [60/100], Loss: 0.5952, Val Loss: 0.6009, Accuracy: 66.65%\n",
      "Epoch [70/100], Loss: 0.5906, Val Loss: 0.6011, Accuracy: 66.71%\n",
      "Epoch [80/100], Loss: 0.5914, Val Loss: 0.6009, Accuracy: 66.84%\n",
      "Epoch [90/100], Loss: 0.5957, Val Loss: 0.6011, Accuracy: 66.71%\n",
      "Epoch [100/100], Loss: 0.5959, Val Loss: 0.6010, Accuracy: 66.77%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6094, Val Loss: 0.6054, Accuracy: 66.11%\n",
      "Epoch [20/100], Loss: 0.6040, Val Loss: 0.6010, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.5986, Val Loss: 0.5990, Accuracy: 66.18%\n",
      "Epoch [40/100], Loss: 0.5962, Val Loss: 0.6007, Accuracy: 65.92%\n",
      "Epoch [50/100], Loss: 0.5930, Val Loss: 0.5982, Accuracy: 66.50%\n",
      "Epoch [60/100], Loss: 0.5906, Val Loss: 0.5982, Accuracy: 66.18%\n",
      "Epoch [70/100], Loss: 0.5907, Val Loss: 0.5980, Accuracy: 66.69%\n",
      "Epoch [80/100], Loss: 0.5950, Val Loss: 0.5981, Accuracy: 66.30%\n",
      "Epoch [90/100], Loss: 0.5907, Val Loss: 0.5983, Accuracy: 66.24%\n",
      "Epoch [100/100], Loss: 0.5909, Val Loss: 0.5984, Accuracy: 66.30%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6146, Val Loss: 0.5960, Accuracy: 66.30%\n",
      "Epoch [20/100], Loss: 0.6034, Val Loss: 0.5945, Accuracy: 66.05%\n",
      "Epoch [30/100], Loss: 0.6017, Val Loss: 0.5945, Accuracy: 66.43%\n",
      "Epoch [40/100], Loss: 0.5961, Val Loss: 0.5941, Accuracy: 67.26%\n",
      "Epoch [50/100], Loss: 0.5931, Val Loss: 0.5938, Accuracy: 67.39%\n",
      "Epoch [60/100], Loss: 0.5945, Val Loss: 0.5943, Accuracy: 66.88%\n",
      "Epoch [70/100], Loss: 0.5948, Val Loss: 0.5937, Accuracy: 67.20%\n",
      "Epoch [80/100], Loss: 0.5935, Val Loss: 0.5939, Accuracy: 67.26%\n",
      "Epoch [90/100], Loss: 0.5965, Val Loss: 0.5940, Accuracy: 67.20%\n",
      "Epoch [100/100], Loss: 0.5955, Val Loss: 0.5938, Accuracy: 67.20%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6053, Val Loss: 0.5988, Accuracy: 67.65%\n",
      "Epoch [20/100], Loss: 0.5991, Val Loss: 0.5959, Accuracy: 67.65%\n",
      "Epoch [30/100], Loss: 0.5981, Val Loss: 0.5955, Accuracy: 67.52%\n",
      "Epoch [40/100], Loss: 0.5931, Val Loss: 0.5955, Accuracy: 67.52%\n",
      "Epoch [50/100], Loss: 0.5907, Val Loss: 0.5962, Accuracy: 66.75%\n",
      "Epoch [60/100], Loss: 0.5901, Val Loss: 0.5956, Accuracy: 67.46%\n",
      "Epoch [70/100], Loss: 0.5924, Val Loss: 0.5959, Accuracy: 67.26%\n",
      "Epoch [80/100], Loss: 0.5907, Val Loss: 0.5956, Accuracy: 67.46%\n",
      "Epoch [90/100], Loss: 0.5939, Val Loss: 0.5956, Accuracy: 67.71%\n",
      "Epoch [100/100], Loss: 0.5961, Val Loss: 0.5961, Accuracy: 66.94%\n",
      "Best cross-validated accuracy: 68.76%\n",
      "Epoch [10/100], Loss: 0.6011, Val Loss: 0.5768, Accuracy: 68.70%\n",
      "Epoch [20/100], Loss: 0.5996, Val Loss: 0.5779, Accuracy: 68.95%\n",
      "Epoch [30/100], Loss: 0.5950, Val Loss: 0.5753, Accuracy: 69.06%\n",
      "Epoch [40/100], Loss: 0.5920, Val Loss: 0.5761, Accuracy: 69.47%\n",
      "Epoch [50/100], Loss: 0.5920, Val Loss: 0.5749, Accuracy: 69.21%\n",
      "Epoch [60/100], Loss: 0.5872, Val Loss: 0.5749, Accuracy: 69.01%\n",
      "Epoch [70/100], Loss: 0.5860, Val Loss: 0.5750, Accuracy: 69.01%\n",
      "Epoch [80/100], Loss: 0.5860, Val Loss: 0.5750, Accuracy: 69.16%\n",
      "Epoch [90/100], Loss: 0.5873, Val Loss: 0.5744, Accuracy: 69.31%\n",
      "Epoch [100/100], Loss: 0.5873, Val Loss: 0.5744, Accuracy: 69.36%\n",
      "Final model accuracy on test set: 69.77%\n",
      "Accuracy 69.77% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6106, Val Loss: 0.5977, Accuracy: 68.18%\n",
      "Epoch [20/100], Loss: 0.6018, Val Loss: 0.5944, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.5987, Val Loss: 0.5942, Accuracy: 68.31%\n",
      "Epoch [40/100], Loss: 0.5989, Val Loss: 0.5915, Accuracy: 68.37%\n",
      "Epoch [50/100], Loss: 0.5946, Val Loss: 0.5930, Accuracy: 67.61%\n",
      "Epoch [60/100], Loss: 0.5908, Val Loss: 0.5916, Accuracy: 67.41%\n",
      "Epoch [70/100], Loss: 0.5949, Val Loss: 0.5914, Accuracy: 67.41%\n",
      "Epoch [80/100], Loss: 0.5903, Val Loss: 0.5914, Accuracy: 67.73%\n",
      "Epoch [90/100], Loss: 0.5915, Val Loss: 0.5913, Accuracy: 67.48%\n",
      "Epoch [100/100], Loss: 0.5891, Val Loss: 0.5908, Accuracy: 67.41%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6087, Val Loss: 0.6069, Accuracy: 66.26%\n",
      "Epoch [20/100], Loss: 0.6013, Val Loss: 0.6027, Accuracy: 66.39%\n",
      "Epoch [30/100], Loss: 0.5973, Val Loss: 0.6030, Accuracy: 66.45%\n",
      "Epoch [40/100], Loss: 0.5934, Val Loss: 0.6021, Accuracy: 66.33%\n",
      "Epoch [50/100], Loss: 0.5936, Val Loss: 0.6018, Accuracy: 66.45%\n",
      "Epoch [60/100], Loss: 0.5944, Val Loss: 0.6020, Accuracy: 66.39%\n",
      "Epoch [70/100], Loss: 0.5927, Val Loss: 0.6023, Accuracy: 66.33%\n",
      "Epoch [80/100], Loss: 0.5940, Val Loss: 0.6017, Accuracy: 66.45%\n",
      "Epoch [90/100], Loss: 0.5910, Val Loss: 0.6021, Accuracy: 66.45%\n",
      "Epoch [100/100], Loss: 0.5950, Val Loss: 0.6021, Accuracy: 66.26%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6104, Val Loss: 0.6069, Accuracy: 66.43%\n",
      "Epoch [20/100], Loss: 0.5987, Val Loss: 0.6018, Accuracy: 66.50%\n",
      "Epoch [30/100], Loss: 0.6020, Val Loss: 0.6002, Accuracy: 65.53%\n",
      "Epoch [40/100], Loss: 0.5935, Val Loss: 0.5991, Accuracy: 66.37%\n",
      "Epoch [50/100], Loss: 0.5918, Val Loss: 0.5986, Accuracy: 66.30%\n",
      "Epoch [60/100], Loss: 0.5913, Val Loss: 0.5986, Accuracy: 66.37%\n",
      "Epoch [70/100], Loss: 0.5894, Val Loss: 0.5981, Accuracy: 65.79%\n",
      "Epoch [80/100], Loss: 0.5920, Val Loss: 0.5984, Accuracy: 66.18%\n",
      "Epoch [90/100], Loss: 0.5883, Val Loss: 0.5986, Accuracy: 66.24%\n",
      "Epoch [100/100], Loss: 0.5914, Val Loss: 0.5987, Accuracy: 66.05%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6122, Val Loss: 0.5990, Accuracy: 67.78%\n",
      "Epoch [20/100], Loss: 0.6049, Val Loss: 0.5955, Accuracy: 66.50%\n",
      "Epoch [30/100], Loss: 0.5970, Val Loss: 0.5958, Accuracy: 66.69%\n",
      "Epoch [40/100], Loss: 0.5923, Val Loss: 0.5955, Accuracy: 66.56%\n",
      "Epoch [50/100], Loss: 0.5901, Val Loss: 0.5956, Accuracy: 66.94%\n",
      "Epoch [60/100], Loss: 0.5945, Val Loss: 0.5955, Accuracy: 66.75%\n",
      "Epoch [70/100], Loss: 0.5895, Val Loss: 0.5954, Accuracy: 67.33%\n",
      "Epoch [80/100], Loss: 0.5907, Val Loss: 0.5955, Accuracy: 67.14%\n",
      "Epoch [90/100], Loss: 0.5928, Val Loss: 0.5955, Accuracy: 67.01%\n",
      "Epoch [100/100], Loss: 0.5903, Val Loss: 0.5954, Accuracy: 67.07%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6092, Val Loss: 0.5992, Accuracy: 67.33%\n",
      "Epoch [20/100], Loss: 0.6019, Val Loss: 0.5982, Accuracy: 67.26%\n",
      "Epoch [30/100], Loss: 0.5945, Val Loss: 0.5971, Accuracy: 66.69%\n",
      "Epoch [40/100], Loss: 0.5904, Val Loss: 0.5974, Accuracy: 66.56%\n",
      "Epoch [50/100], Loss: 0.5957, Val Loss: 0.5964, Accuracy: 67.01%\n",
      "Epoch [60/100], Loss: 0.5915, Val Loss: 0.5966, Accuracy: 66.88%\n",
      "Epoch [70/100], Loss: 0.5914, Val Loss: 0.5965, Accuracy: 67.01%\n",
      "Epoch [80/100], Loss: 0.5890, Val Loss: 0.5965, Accuracy: 66.75%\n",
      "Epoch [90/100], Loss: 0.5935, Val Loss: 0.5963, Accuracy: 67.01%\n",
      "Epoch [100/100], Loss: 0.5934, Val Loss: 0.5965, Accuracy: 66.69%\n",
      "Best cross-validated accuracy: 68.57%\n",
      "Epoch [10/100], Loss: 0.5954, Val Loss: 0.5770, Accuracy: 68.90%\n",
      "Epoch [20/100], Loss: 0.5899, Val Loss: 0.5760, Accuracy: 69.26%\n",
      "Epoch [30/100], Loss: 0.5906, Val Loss: 0.5751, Accuracy: 69.01%\n",
      "Epoch [40/100], Loss: 0.5898, Val Loss: 0.5748, Accuracy: 69.11%\n",
      "Epoch [50/100], Loss: 0.5929, Val Loss: 0.5752, Accuracy: 69.21%\n",
      "Epoch [60/100], Loss: 0.5896, Val Loss: 0.5756, Accuracy: 69.11%\n",
      "Epoch [70/100], Loss: 0.5891, Val Loss: 0.5754, Accuracy: 69.01%\n",
      "Epoch [80/100], Loss: 0.5871, Val Loss: 0.5751, Accuracy: 69.01%\n",
      "Epoch [90/100], Loss: 0.5921, Val Loss: 0.5755, Accuracy: 69.11%\n",
      "Epoch [100/100], Loss: 0.5884, Val Loss: 0.5754, Accuracy: 69.11%\n",
      "Final model accuracy on test set: 69.42%\n",
      "Accuracy 69.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6097, Val Loss: 0.5963, Accuracy: 67.41%\n",
      "Epoch [20/100], Loss: 0.6047, Val Loss: 0.5933, Accuracy: 67.80%\n",
      "Epoch [30/100], Loss: 0.5984, Val Loss: 0.5930, Accuracy: 68.18%\n",
      "Epoch [40/100], Loss: 0.5967, Val Loss: 0.5919, Accuracy: 67.86%\n",
      "Epoch [50/100], Loss: 0.5918, Val Loss: 0.5914, Accuracy: 67.93%\n",
      "Epoch [60/100], Loss: 0.5945, Val Loss: 0.5919, Accuracy: 68.25%\n",
      "Epoch [70/100], Loss: 0.5959, Val Loss: 0.5916, Accuracy: 68.12%\n",
      "Epoch [80/100], Loss: 0.5941, Val Loss: 0.5917, Accuracy: 68.18%\n",
      "Epoch [90/100], Loss: 0.5932, Val Loss: 0.5914, Accuracy: 68.25%\n",
      "Epoch [100/100], Loss: 0.5911, Val Loss: 0.5916, Accuracy: 67.99%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6080, Val Loss: 0.6053, Accuracy: 66.71%\n",
      "Epoch [20/100], Loss: 0.6006, Val Loss: 0.6022, Accuracy: 66.01%\n",
      "Epoch [30/100], Loss: 0.5943, Val Loss: 0.6013, Accuracy: 66.90%\n",
      "Epoch [40/100], Loss: 0.5918, Val Loss: 0.6010, Accuracy: 66.65%\n",
      "Epoch [50/100], Loss: 0.5884, Val Loss: 0.6008, Accuracy: 66.90%\n",
      "Epoch [60/100], Loss: 0.5914, Val Loss: 0.6013, Accuracy: 66.52%\n",
      "Epoch [70/100], Loss: 0.5936, Val Loss: 0.6009, Accuracy: 66.84%\n",
      "Epoch [80/100], Loss: 0.5937, Val Loss: 0.6006, Accuracy: 66.90%\n",
      "Epoch [90/100], Loss: 0.5893, Val Loss: 0.6017, Accuracy: 66.52%\n",
      "Epoch [100/100], Loss: 0.5892, Val Loss: 0.6014, Accuracy: 66.39%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6079, Val Loss: 0.6058, Accuracy: 65.53%\n",
      "Epoch [20/100], Loss: 0.6034, Val Loss: 0.6036, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.5956, Val Loss: 0.6014, Accuracy: 66.30%\n",
      "Epoch [40/100], Loss: 0.5950, Val Loss: 0.6005, Accuracy: 66.56%\n",
      "Epoch [50/100], Loss: 0.5906, Val Loss: 0.5988, Accuracy: 66.50%\n",
      "Epoch [60/100], Loss: 0.5903, Val Loss: 0.5992, Accuracy: 65.98%\n",
      "Epoch [70/100], Loss: 0.5891, Val Loss: 0.5992, Accuracy: 66.24%\n",
      "Epoch [80/100], Loss: 0.5881, Val Loss: 0.5986, Accuracy: 66.11%\n",
      "Epoch [90/100], Loss: 0.5895, Val Loss: 0.5991, Accuracy: 66.30%\n",
      "Epoch [100/100], Loss: 0.5872, Val Loss: 0.5988, Accuracy: 66.37%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6112, Val Loss: 0.5968, Accuracy: 67.58%\n",
      "Epoch [20/100], Loss: 0.6002, Val Loss: 0.5992, Accuracy: 66.94%\n",
      "Epoch [30/100], Loss: 0.5991, Val Loss: 0.5966, Accuracy: 67.20%\n",
      "Epoch [40/100], Loss: 0.5951, Val Loss: 0.5957, Accuracy: 66.62%\n",
      "Epoch [50/100], Loss: 0.5943, Val Loss: 0.5959, Accuracy: 66.18%\n",
      "Epoch [60/100], Loss: 0.5921, Val Loss: 0.5959, Accuracy: 66.30%\n",
      "Epoch [70/100], Loss: 0.5927, Val Loss: 0.5964, Accuracy: 67.26%\n",
      "Epoch [80/100], Loss: 0.5939, Val Loss: 0.5966, Accuracy: 66.75%\n",
      "Epoch [90/100], Loss: 0.5910, Val Loss: 0.5962, Accuracy: 66.62%\n",
      "Epoch [100/100], Loss: 0.5932, Val Loss: 0.5962, Accuracy: 67.01%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6145, Val Loss: 0.5975, Accuracy: 67.91%\n",
      "Epoch [20/100], Loss: 0.6033, Val Loss: 0.5968, Accuracy: 67.65%\n",
      "Epoch [30/100], Loss: 0.5983, Val Loss: 0.5972, Accuracy: 67.78%\n",
      "Epoch [40/100], Loss: 0.5971, Val Loss: 0.5965, Accuracy: 67.58%\n",
      "Epoch [50/100], Loss: 0.5962, Val Loss: 0.5965, Accuracy: 67.71%\n",
      "Epoch [60/100], Loss: 0.5964, Val Loss: 0.5963, Accuracy: 67.65%\n",
      "Epoch [70/100], Loss: 0.5960, Val Loss: 0.5962, Accuracy: 67.52%\n",
      "Epoch [80/100], Loss: 0.5951, Val Loss: 0.5963, Accuracy: 67.65%\n",
      "Epoch [90/100], Loss: 0.5988, Val Loss: 0.5963, Accuracy: 67.78%\n",
      "Epoch [100/100], Loss: 0.5962, Val Loss: 0.5961, Accuracy: 67.84%\n",
      "Best cross-validated accuracy: 68.57%\n",
      "Epoch [10/100], Loss: 0.5931, Val Loss: 0.5753, Accuracy: 69.62%\n",
      "Epoch [20/100], Loss: 0.5928, Val Loss: 0.5749, Accuracy: 69.57%\n",
      "Epoch [30/100], Loss: 0.5909, Val Loss: 0.5749, Accuracy: 69.77%\n",
      "Epoch [40/100], Loss: 0.5869, Val Loss: 0.5753, Accuracy: 68.49%\n",
      "Epoch [50/100], Loss: 0.5853, Val Loss: 0.5756, Accuracy: 69.47%\n",
      "Epoch [60/100], Loss: 0.5850, Val Loss: 0.5749, Accuracy: 69.67%\n",
      "Epoch [70/100], Loss: 0.5850, Val Loss: 0.5753, Accuracy: 69.26%\n",
      "Epoch [80/100], Loss: 0.5862, Val Loss: 0.5755, Accuracy: 68.90%\n",
      "Epoch [90/100], Loss: 0.5882, Val Loss: 0.5750, Accuracy: 69.26%\n",
      "Epoch [100/100], Loss: 0.5869, Val Loss: 0.5747, Accuracy: 69.62%\n",
      "Final model accuracy on test set: 69.77%\n",
      "Accuracy 69.77% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6111, Val Loss: 0.5969, Accuracy: 68.18%\n",
      "Epoch [20/100], Loss: 0.6049, Val Loss: 0.5951, Accuracy: 67.61%\n",
      "Epoch [30/100], Loss: 0.6004, Val Loss: 0.5926, Accuracy: 68.25%\n",
      "Epoch [40/100], Loss: 0.5984, Val Loss: 0.5926, Accuracy: 68.25%\n",
      "Epoch [50/100], Loss: 0.5923, Val Loss: 0.5930, Accuracy: 68.37%\n",
      "Epoch [60/100], Loss: 0.5931, Val Loss: 0.5924, Accuracy: 67.99%\n",
      "Epoch [70/100], Loss: 0.5954, Val Loss: 0.5928, Accuracy: 68.37%\n",
      "Epoch [80/100], Loss: 0.5942, Val Loss: 0.5927, Accuracy: 68.25%\n",
      "Epoch [90/100], Loss: 0.5991, Val Loss: 0.5927, Accuracy: 68.31%\n",
      "Epoch [100/100], Loss: 0.5930, Val Loss: 0.5928, Accuracy: 68.44%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6154, Val Loss: 0.6066, Accuracy: 66.13%\n",
      "Epoch [20/100], Loss: 0.6013, Val Loss: 0.6021, Accuracy: 66.65%\n",
      "Epoch [30/100], Loss: 0.5968, Val Loss: 0.6050, Accuracy: 66.65%\n",
      "Epoch [40/100], Loss: 0.5965, Val Loss: 0.6009, Accuracy: 67.16%\n",
      "Epoch [50/100], Loss: 0.5909, Val Loss: 0.6017, Accuracy: 67.03%\n",
      "Epoch [60/100], Loss: 0.5906, Val Loss: 0.6017, Accuracy: 66.77%\n",
      "Epoch [70/100], Loss: 0.5915, Val Loss: 0.6020, Accuracy: 66.97%\n",
      "Epoch [80/100], Loss: 0.5901, Val Loss: 0.6015, Accuracy: 67.16%\n",
      "Epoch [90/100], Loss: 0.5921, Val Loss: 0.6023, Accuracy: 66.84%\n",
      "Epoch [100/100], Loss: 0.5895, Val Loss: 0.6019, Accuracy: 67.09%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6129, Val Loss: 0.6055, Accuracy: 65.66%\n",
      "Epoch [20/100], Loss: 0.6018, Val Loss: 0.6034, Accuracy: 66.11%\n",
      "Epoch [30/100], Loss: 0.6010, Val Loss: 0.6019, Accuracy: 65.79%\n",
      "Epoch [40/100], Loss: 0.5972, Val Loss: 0.5986, Accuracy: 66.24%\n",
      "Epoch [50/100], Loss: 0.5936, Val Loss: 0.5992, Accuracy: 65.15%\n",
      "Epoch [60/100], Loss: 0.5909, Val Loss: 0.5978, Accuracy: 66.37%\n",
      "Epoch [70/100], Loss: 0.5922, Val Loss: 0.5984, Accuracy: 66.37%\n",
      "Epoch [80/100], Loss: 0.5877, Val Loss: 0.5976, Accuracy: 66.24%\n",
      "Epoch [90/100], Loss: 0.5873, Val Loss: 0.5977, Accuracy: 65.98%\n",
      "Epoch [100/100], Loss: 0.5854, Val Loss: 0.5976, Accuracy: 65.98%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6095, Val Loss: 0.5989, Accuracy: 67.26%\n",
      "Epoch [20/100], Loss: 0.6010, Val Loss: 0.5973, Accuracy: 67.52%\n",
      "Epoch [30/100], Loss: 0.5964, Val Loss: 0.5956, Accuracy: 66.05%\n",
      "Epoch [40/100], Loss: 0.5957, Val Loss: 0.5977, Accuracy: 67.26%\n",
      "Epoch [50/100], Loss: 0.5920, Val Loss: 0.5968, Accuracy: 67.33%\n",
      "Epoch [60/100], Loss: 0.5916, Val Loss: 0.5968, Accuracy: 67.20%\n",
      "Epoch [70/100], Loss: 0.5947, Val Loss: 0.5967, Accuracy: 67.20%\n",
      "Epoch [80/100], Loss: 0.5926, Val Loss: 0.5971, Accuracy: 67.14%\n",
      "Epoch [90/100], Loss: 0.5896, Val Loss: 0.5968, Accuracy: 67.14%\n",
      "Epoch [100/100], Loss: 0.5928, Val Loss: 0.5969, Accuracy: 67.20%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6082, Val Loss: 0.6007, Accuracy: 67.46%\n",
      "Epoch [20/100], Loss: 0.5991, Val Loss: 0.5975, Accuracy: 67.26%\n",
      "Epoch [30/100], Loss: 0.5954, Val Loss: 0.5968, Accuracy: 67.84%\n",
      "Epoch [40/100], Loss: 0.6026, Val Loss: 0.5962, Accuracy: 67.46%\n",
      "Epoch [50/100], Loss: 0.5943, Val Loss: 0.5965, Accuracy: 67.65%\n",
      "Epoch [60/100], Loss: 0.5964, Val Loss: 0.5966, Accuracy: 67.97%\n",
      "Epoch [70/100], Loss: 0.5949, Val Loss: 0.5964, Accuracy: 68.03%\n",
      "Epoch [80/100], Loss: 0.5958, Val Loss: 0.5961, Accuracy: 67.39%\n",
      "Epoch [90/100], Loss: 0.5935, Val Loss: 0.5966, Accuracy: 67.91%\n",
      "Epoch [100/100], Loss: 0.5952, Val Loss: 0.5964, Accuracy: 67.71%\n",
      "Best cross-validated accuracy: 68.63%\n",
      "Epoch [10/100], Loss: 0.5976, Val Loss: 0.5758, Accuracy: 68.90%\n",
      "Epoch [20/100], Loss: 0.5962, Val Loss: 0.5794, Accuracy: 69.11%\n",
      "Epoch [30/100], Loss: 0.5911, Val Loss: 0.5754, Accuracy: 68.95%\n",
      "Epoch [40/100], Loss: 0.5888, Val Loss: 0.5747, Accuracy: 69.26%\n",
      "Epoch [50/100], Loss: 0.5896, Val Loss: 0.5748, Accuracy: 69.36%\n",
      "Epoch [60/100], Loss: 0.5902, Val Loss: 0.5749, Accuracy: 69.21%\n",
      "Epoch [70/100], Loss: 0.5882, Val Loss: 0.5751, Accuracy: 69.06%\n",
      "Epoch [80/100], Loss: 0.5890, Val Loss: 0.5755, Accuracy: 68.90%\n",
      "Epoch [90/100], Loss: 0.5888, Val Loss: 0.5750, Accuracy: 69.06%\n",
      "Epoch [100/100], Loss: 0.5884, Val Loss: 0.5748, Accuracy: 69.31%\n",
      "Final model accuracy on test set: 69.52%\n",
      "Accuracy 69.52% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6113, Val Loss: 0.5991, Accuracy: 67.35%\n",
      "Epoch [20/100], Loss: 0.6043, Val Loss: 0.5934, Accuracy: 68.31%\n",
      "Epoch [30/100], Loss: 0.5972, Val Loss: 0.5926, Accuracy: 68.12%\n",
      "Epoch [40/100], Loss: 0.5984, Val Loss: 0.5924, Accuracy: 67.93%\n",
      "Epoch [50/100], Loss: 0.5926, Val Loss: 0.5912, Accuracy: 67.61%\n",
      "Epoch [60/100], Loss: 0.5940, Val Loss: 0.5909, Accuracy: 67.67%\n",
      "Epoch [70/100], Loss: 0.5922, Val Loss: 0.5910, Accuracy: 67.80%\n",
      "Epoch [80/100], Loss: 0.5882, Val Loss: 0.5908, Accuracy: 67.67%\n",
      "Epoch [90/100], Loss: 0.5918, Val Loss: 0.5908, Accuracy: 67.73%\n",
      "Epoch [100/100], Loss: 0.5894, Val Loss: 0.5908, Accuracy: 67.80%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6098, Val Loss: 0.6056, Accuracy: 66.52%\n",
      "Epoch [20/100], Loss: 0.6011, Val Loss: 0.6018, Accuracy: 66.20%\n",
      "Epoch [30/100], Loss: 0.5974, Val Loss: 0.6013, Accuracy: 67.48%\n",
      "Epoch [40/100], Loss: 0.5905, Val Loss: 0.6013, Accuracy: 67.29%\n",
      "Epoch [50/100], Loss: 0.5892, Val Loss: 0.6016, Accuracy: 67.22%\n",
      "Epoch [60/100], Loss: 0.5874, Val Loss: 0.6014, Accuracy: 67.41%\n",
      "Epoch [70/100], Loss: 0.5921, Val Loss: 0.6016, Accuracy: 67.35%\n",
      "Epoch [80/100], Loss: 0.5927, Val Loss: 0.6015, Accuracy: 67.35%\n",
      "Epoch [90/100], Loss: 0.5909, Val Loss: 0.6014, Accuracy: 67.09%\n",
      "Epoch [100/100], Loss: 0.5917, Val Loss: 0.6019, Accuracy: 67.29%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6099, Val Loss: 0.6060, Accuracy: 66.11%\n",
      "Epoch [20/100], Loss: 0.6059, Val Loss: 0.6018, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.5941, Val Loss: 0.6007, Accuracy: 66.62%\n",
      "Epoch [40/100], Loss: 0.5939, Val Loss: 0.5991, Accuracy: 66.69%\n",
      "Epoch [50/100], Loss: 0.5933, Val Loss: 0.6001, Accuracy: 65.86%\n",
      "Epoch [60/100], Loss: 0.5876, Val Loss: 0.5988, Accuracy: 66.69%\n",
      "Epoch [70/100], Loss: 0.5887, Val Loss: 0.5991, Accuracy: 66.69%\n",
      "Epoch [80/100], Loss: 0.5920, Val Loss: 0.5991, Accuracy: 66.56%\n",
      "Epoch [90/100], Loss: 0.5917, Val Loss: 0.5991, Accuracy: 66.75%\n",
      "Epoch [100/100], Loss: 0.5853, Val Loss: 0.5991, Accuracy: 66.56%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6088, Val Loss: 0.5948, Accuracy: 66.88%\n",
      "Epoch [20/100], Loss: 0.6023, Val Loss: 0.5942, Accuracy: 67.14%\n",
      "Epoch [30/100], Loss: 0.6001, Val Loss: 0.5942, Accuracy: 67.07%\n",
      "Epoch [40/100], Loss: 0.5927, Val Loss: 0.5947, Accuracy: 67.26%\n",
      "Epoch [50/100], Loss: 0.5937, Val Loss: 0.5948, Accuracy: 67.20%\n",
      "Epoch [60/100], Loss: 0.5950, Val Loss: 0.5950, Accuracy: 67.20%\n",
      "Epoch [70/100], Loss: 0.5936, Val Loss: 0.5951, Accuracy: 66.82%\n",
      "Epoch [80/100], Loss: 0.5963, Val Loss: 0.5952, Accuracy: 67.20%\n",
      "Epoch [90/100], Loss: 0.5942, Val Loss: 0.5953, Accuracy: 67.39%\n",
      "Epoch [100/100], Loss: 0.5962, Val Loss: 0.5952, Accuracy: 67.14%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6053, Val Loss: 0.5983, Accuracy: 67.46%\n",
      "Epoch [20/100], Loss: 0.5998, Val Loss: 0.5959, Accuracy: 67.91%\n",
      "Epoch [30/100], Loss: 0.5948, Val Loss: 0.5977, Accuracy: 67.26%\n",
      "Epoch [40/100], Loss: 0.5940, Val Loss: 0.5966, Accuracy: 67.26%\n",
      "Epoch [50/100], Loss: 0.5922, Val Loss: 0.5957, Accuracy: 68.03%\n",
      "Epoch [60/100], Loss: 0.5921, Val Loss: 0.5965, Accuracy: 67.52%\n",
      "Epoch [70/100], Loss: 0.5929, Val Loss: 0.5959, Accuracy: 67.71%\n",
      "Epoch [80/100], Loss: 0.5907, Val Loss: 0.5962, Accuracy: 67.46%\n",
      "Epoch [90/100], Loss: 0.5922, Val Loss: 0.5957, Accuracy: 68.03%\n",
      "Epoch [100/100], Loss: 0.5914, Val Loss: 0.5962, Accuracy: 67.78%\n",
      "Best cross-validated accuracy: 68.63%\n",
      "Epoch [10/100], Loss: 0.5932, Val Loss: 0.5746, Accuracy: 69.16%\n",
      "Epoch [20/100], Loss: 0.5957, Val Loss: 0.5739, Accuracy: 69.11%\n",
      "Epoch [30/100], Loss: 0.5918, Val Loss: 0.5742, Accuracy: 69.57%\n",
      "Epoch [40/100], Loss: 0.5889, Val Loss: 0.5745, Accuracy: 69.31%\n",
      "Epoch [50/100], Loss: 0.5838, Val Loss: 0.5745, Accuracy: 69.72%\n",
      "Epoch [60/100], Loss: 0.5892, Val Loss: 0.5738, Accuracy: 69.16%\n",
      "Final model accuracy on test set: 69.93%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "# df = df[~(df == -20).any(axis=1)]\n",
    "# df = df[(df['surface_Hard'] == 1.0)]\n",
    "\n",
    "# df = df.drop(df.columns[df.columns.str.contains('_rd')], axis=1)\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.699\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "    # Quick train break\n",
    "    # break\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        0.0   0.479734    1.53    2.45\n",
      "1        0.0   0.756106    1.63    2.25\n",
      "2        0.0   0.430342    2.76    1.42\n",
      "3        1.0   0.772055    1.19    4.53\n",
      "4        0.0   0.488264    2.78    1.43\n",
      "...      ...        ...     ...     ...\n",
      "1947     0.0   0.323867    3.03    1.36\n",
      "1948     1.0   0.490777    1.58    2.33\n",
      "1949     0.0   0.693062    1.33    3.24\n",
      "1950     1.0   0.742201    1.36    3.12\n",
      "1951     0.0   0.488637    3.10    1.36\n",
      "\n",
      "[1952 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly Criterion\n",
    "def kelly_criterion(vegas_odds, calculated_probability):\n",
    "    corrected = vegas_odds - 1\n",
    "    # Calculate the Kelly fraction\n",
    "    kelly_fraction = calculated_probability - ((1 - calculated_probability)/corrected)\n",
    "    # Ensure that the fraction is not negative\n",
    "    kelly_fraction = max(0, kelly_fraction)\n",
    "    \n",
    "    return kelly_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total returned on Starting $10 bankroll: 8.82 on a total # bets: 131 from a total of 1952 games\n",
      "ROI : -0.1 X\n",
      "Avg Bankroll Bet % : 0.239 %\n",
      "Amount of differing favorites %: 0.12295081967213115\n",
      "Amount of upset bets correct % : 0.509 with Unit $10 won $70.79 on 216 bets\n",
      "Amount of incorrect bet % : 0.3511\n",
      "Correct Bet %: 0.8321\n",
      "Model % Correct : 0.6993 Vegas Correct % : 0.6988\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "upset_predict = 0\n",
    "upset_correct = 0\n",
    "upset_won = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "avg_bet = 0\n",
    "\n",
    "confidence_pct = .8\n",
    "confidence_top_pct = 1\n",
    "\n",
    "START_UNIT = 10\n",
    "UNIT = START_UNIT\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    current = START_UNIT\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct :\n",
    "        kelly = (kelly_criterion(row['A_Odds'], row['Predicted']) * START_UNIT)\n",
    "        better += 1 if kelly > 0 else 0\n",
    "        avg_bet += kelly_criterion(row['A_Odds'], row['Predicted'])\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1 if kelly > 0 else 0\n",
    "            START_UNIT += (row['A_Odds']-1) * kelly\n",
    "        else:\n",
    "            wrong += 1\n",
    "            START_UNIT -= kelly\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct :\n",
    "        kelly = (kelly_criterion(row['B_Odds'], 1-row['Predicted']) * START_UNIT)\n",
    "        better += 1 if kelly > 0 else 0\n",
    "        avg_bet += kelly_criterion(row['B_Odds'], 1-row['Predicted'])\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1 if kelly > 0 else 0\n",
    "            START_UNIT += (row['B_Odds']-1) * kelly\n",
    "        else:\n",
    "            wrong += 1\n",
    "            START_UNIT -= kelly\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        kelly_A  = (kelly_criterion(row['A_Odds'],row['Predicted']) * UNIT)\n",
    "        kelly_B  = (kelly_criterion(row['B_Odds'],1-row['Predicted']) * UNIT)\n",
    "\n",
    "        upset_predict += 1 if (kelly_A > 0 and round(row['Predicted']) == 1) or (kelly_B > 0 and round(row['Predicted']) == 0) else 0\n",
    "\n",
    "        if round(row['Predicted']) == round(row['Actual']):\n",
    "            if(row['Actual'] == 1):\n",
    "                upset_correct += 1 if kelly_A > 0 else 0\n",
    "                upset_won += (row['A_Odds']-1) * kelly_A\n",
    "            else:\n",
    "                upset_correct += 1 if kelly_B > 0 else 0\n",
    "                upset_won += (row['B_Odds']-1) * kelly_B\n",
    "        elif round(row['Predicted']) == 1:\n",
    "            upset_won -= kelly_A\n",
    "        else:\n",
    "            upset_won -= kelly_B\n",
    "            \n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total returned on Starting ${UNIT} bankroll: {START_UNIT:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"ROI : {((START_UNIT-UNIT)/UNIT):.1f} X\")\n",
    "print(f\"Avg Bankroll Bet % : {(avg_bet/better):.3f} %\")\n",
    "print(f\"Amount of differing favorites %: {diff_fav/length}\")\n",
    "print(f\"Amount of upset bets correct % : {(upset_correct/upset_predict):.3f} with Unit ${UNIT} won ${upset_won:.2f} on {upset_predict} bets\")\n",
    "print(f\"Amount of incorrect bet % : {(wrong/better):.4f}\")\n",
    "print(f\"Correct Bet %: {(bet_correct/better):.4f}\")\n",
    "print(f\"Model % Correct : {(model_correct/length):.4f} Vegas Correct % : {(vegas_correct/length):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
