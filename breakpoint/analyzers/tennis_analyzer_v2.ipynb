{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33946\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('../testcsvs/glickoUpdated.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_surface_glicko_rd'] <= 150) & (df['b_surface_glicko_rd'] <= 150)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug', 'sets', 'games', 'tiebreaks'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>tourney_round</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>point_glicko_rating_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_surface_return_second_won_glicko_rd</th>\n",
       "      <th>b_surface_second_won_glicko_rd</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>156.391443</td>\n",
       "      <td>1804.924851</td>\n",
       "      <td>1648.533408</td>\n",
       "      <td>67.389371</td>\n",
       "      <td>73.590383</td>\n",
       "      <td>16.685910</td>\n",
       "      <td>...</td>\n",
       "      <td>1525.177533</td>\n",
       "      <td>1509.676357</td>\n",
       "      <td>63.306180</td>\n",
       "      <td>68.224718</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.779397</td>\n",
       "      <td>1682.022134</td>\n",
       "      <td>1684.801531</td>\n",
       "      <td>68.795819</td>\n",
       "      <td>68.576543</td>\n",
       "      <td>-1.222118</td>\n",
       "      <td>...</td>\n",
       "      <td>1493.441526</td>\n",
       "      <td>1520.481105</td>\n",
       "      <td>72.508115</td>\n",
       "      <td>64.043825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-9.017080</td>\n",
       "      <td>1538.048641</td>\n",
       "      <td>1547.065721</td>\n",
       "      <td>103.131955</td>\n",
       "      <td>76.124524</td>\n",
       "      <td>12.273373</td>\n",
       "      <td>...</td>\n",
       "      <td>1526.254795</td>\n",
       "      <td>1490.089208</td>\n",
       "      <td>85.431004</td>\n",
       "      <td>80.757452</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>48.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>61.159514</td>\n",
       "      <td>1682.672084</td>\n",
       "      <td>1621.512570</td>\n",
       "      <td>68.943340</td>\n",
       "      <td>76.007944</td>\n",
       "      <td>2.275771</td>\n",
       "      <td>...</td>\n",
       "      <td>1507.882055</td>\n",
       "      <td>1506.545429</td>\n",
       "      <td>65.888687</td>\n",
       "      <td>67.307645</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-316.512544</td>\n",
       "      <td>1510.091085</td>\n",
       "      <td>1826.603629</td>\n",
       "      <td>99.646450</td>\n",
       "      <td>68.052204</td>\n",
       "      <td>-37.932451</td>\n",
       "      <td>...</td>\n",
       "      <td>1474.448007</td>\n",
       "      <td>1505.288480</td>\n",
       "      <td>82.646052</td>\n",
       "      <td>63.415511</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  tourney_round  a_player_rank  b_player_rank  \\\n",
       "5373      3.0            0.8           15.0           74.0   \n",
       "5375      3.0            0.8           46.0           65.0   \n",
       "5377      3.0            0.8           95.0           89.0   \n",
       "5378      3.0            0.8           48.0           83.0   \n",
       "5380      3.0            0.8           70.0           22.0   \n",
       "\n",
       "      glicko_rating_diff  a_glicko_rating  b_glicko_rating  a_glicko_rd  \\\n",
       "5373          156.391443      1804.924851      1648.533408    67.389371   \n",
       "5375           -2.779397      1682.022134      1684.801531    68.795819   \n",
       "5377           -9.017080      1538.048641      1547.065721   103.131955   \n",
       "5378           61.159514      1682.672084      1621.512570    68.943340   \n",
       "5380         -316.512544      1510.091085      1826.603629    99.646450   \n",
       "\n",
       "      b_glicko_rd  point_glicko_rating_diff  ...  \\\n",
       "5373    73.590383                 16.685910  ...   \n",
       "5375    68.576543                 -1.222118  ...   \n",
       "5377    76.124524                 12.273373  ...   \n",
       "5378    76.007944                  2.275771  ...   \n",
       "5380    68.052204                -37.932451  ...   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rating  \\\n",
       "5373                                1525.177533   \n",
       "5375                                1493.441526   \n",
       "5377                                1526.254795   \n",
       "5378                                1507.882055   \n",
       "5380                                1474.448007   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  \\\n",
       "5373                         1509.676357   \n",
       "5375                         1520.481105   \n",
       "5377                         1490.089208   \n",
       "5378                         1506.545429   \n",
       "5380                         1505.288480   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rd  b_surface_second_won_glicko_rd  \\\n",
       "5373                              63.306180                       68.224718   \n",
       "5375                              72.508115                       64.043825   \n",
       "5377                              85.431004                       80.757452   \n",
       "5378                              65.888687                       67.307645   \n",
       "5380                              82.646052                       63.415511   \n",
       "\n",
       "      a_odds  b_odds  a_b_win  surface_Clay  surface_Grass  surface_Hard  \n",
       "5373    1.28    3.59      1.0           1.0            0.0           0.0  \n",
       "5375     NaN     NaN      0.0           1.0            0.0           0.0  \n",
       "5377    1.59    2.29      1.0           0.0            0.0           1.0  \n",
       "5378    2.40    1.54      0.0           0.0            0.0           1.0  \n",
       "5380    4.44    1.19      0.0           1.0            0.0           0.0  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6100, Val Loss: 0.6014, Accuracy: 66.55%\n",
      "Epoch [20/100], Loss: 0.5994, Val Loss: 0.5993, Accuracy: 66.91%\n",
      "Epoch [30/100], Loss: 0.5940, Val Loss: 0.5997, Accuracy: 66.55%\n",
      "Epoch [40/100], Loss: 0.5931, Val Loss: 0.6003, Accuracy: 66.65%\n",
      "Epoch [50/100], Loss: 0.5924, Val Loss: 0.6002, Accuracy: 66.60%\n",
      "Epoch [60/100], Loss: 0.5932, Val Loss: 0.6007, Accuracy: 66.39%\n",
      "Epoch [70/100], Loss: 0.5962, Val Loss: 0.6003, Accuracy: 66.65%\n",
      "Epoch [80/100], Loss: 0.5946, Val Loss: 0.6003, Accuracy: 66.39%\n",
      "Epoch [90/100], Loss: 0.5962, Val Loss: 0.6005, Accuracy: 66.49%\n",
      "Epoch [100/100], Loss: 0.5947, Val Loss: 0.6003, Accuracy: 66.44%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6111, Val Loss: 0.5927, Accuracy: 67.06%\n",
      "Epoch [20/100], Loss: 0.6047, Val Loss: 0.5888, Accuracy: 67.27%\n",
      "Epoch [30/100], Loss: 0.6014, Val Loss: 0.5907, Accuracy: 67.48%\n",
      "Epoch [40/100], Loss: 0.5970, Val Loss: 0.5896, Accuracy: 67.68%\n",
      "Epoch [50/100], Loss: 0.5932, Val Loss: 0.5883, Accuracy: 67.84%\n",
      "Epoch [60/100], Loss: 0.5942, Val Loss: 0.5887, Accuracy: 67.53%\n",
      "Epoch [70/100], Loss: 0.5965, Val Loss: 0.5886, Accuracy: 67.73%\n",
      "Epoch [80/100], Loss: 0.5951, Val Loss: 0.5885, Accuracy: 67.63%\n",
      "Epoch [90/100], Loss: 0.5948, Val Loss: 0.5882, Accuracy: 67.84%\n",
      "Epoch [100/100], Loss: 0.5978, Val Loss: 0.5886, Accuracy: 67.68%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6046, Val Loss: 0.6143, Accuracy: 64.95%\n",
      "Epoch [20/100], Loss: 0.5998, Val Loss: 0.6118, Accuracy: 65.31%\n",
      "Epoch [30/100], Loss: 0.5957, Val Loss: 0.6090, Accuracy: 66.08%\n",
      "Epoch [40/100], Loss: 0.5932, Val Loss: 0.6093, Accuracy: 66.60%\n",
      "Epoch [50/100], Loss: 0.5884, Val Loss: 0.6082, Accuracy: 65.98%\n",
      "Epoch [60/100], Loss: 0.5894, Val Loss: 0.6089, Accuracy: 66.60%\n",
      "Epoch [70/100], Loss: 0.5879, Val Loss: 0.6088, Accuracy: 66.55%\n",
      "Epoch [80/100], Loss: 0.5881, Val Loss: 0.6088, Accuracy: 66.44%\n",
      "Epoch [90/100], Loss: 0.5906, Val Loss: 0.6085, Accuracy: 66.55%\n",
      "Epoch [100/100], Loss: 0.5891, Val Loss: 0.6085, Accuracy: 66.75%\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6082, Val Loss: 0.6143, Accuracy: 65.34%\n",
      "Epoch [20/100], Loss: 0.5992, Val Loss: 0.6098, Accuracy: 65.50%\n",
      "Epoch [30/100], Loss: 0.5923, Val Loss: 0.6104, Accuracy: 65.19%\n",
      "Epoch [40/100], Loss: 0.5877, Val Loss: 0.6100, Accuracy: 65.29%\n",
      "Epoch [50/100], Loss: 0.5872, Val Loss: 0.6099, Accuracy: 65.65%\n",
      "Epoch [60/100], Loss: 0.5906, Val Loss: 0.6098, Accuracy: 65.39%\n",
      "Epoch [70/100], Loss: 0.5876, Val Loss: 0.6096, Accuracy: 65.60%\n",
      "Epoch [80/100], Loss: 0.5877, Val Loss: 0.6100, Accuracy: 65.60%\n",
      "Epoch [90/100], Loss: 0.5898, Val Loss: 0.6100, Accuracy: 65.60%\n",
      "Epoch [100/100], Loss: 0.5897, Val Loss: 0.6102, Accuracy: 65.39%\n",
      "Best cross-validated accuracy: 69.02%\n",
      "Epoch [10/100], Loss: 0.6059, Val Loss: 0.6058, Accuracy: 67.53%\n",
      "Epoch [20/100], Loss: 0.5981, Val Loss: 0.6040, Accuracy: 67.45%\n",
      "Epoch [30/100], Loss: 0.5966, Val Loss: 0.6017, Accuracy: 67.49%\n",
      "Epoch [40/100], Loss: 0.5959, Val Loss: 0.6014, Accuracy: 68.11%\n",
      "Epoch [50/100], Loss: 0.5960, Val Loss: 0.6013, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.5926, Val Loss: 0.6010, Accuracy: 67.62%\n",
      "Epoch [70/100], Loss: 0.5955, Val Loss: 0.6019, Accuracy: 67.41%\n",
      "Epoch [80/100], Loss: 0.5921, Val Loss: 0.6019, Accuracy: 68.03%\n",
      "Epoch [90/100], Loss: 0.5944, Val Loss: 0.6015, Accuracy: 67.82%\n",
      "Epoch [100/100], Loss: 0.5914, Val Loss: 0.6023, Accuracy: 67.49%\n",
      "Final model accuracy on test set: 68.11%\n",
      "Accuracy 68.11% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6150, Val Loss: 0.6000, Accuracy: 65.82%\n",
      "Epoch [20/100], Loss: 0.5994, Val Loss: 0.6001, Accuracy: 66.91%\n",
      "Epoch [30/100], Loss: 0.5919, Val Loss: 0.5995, Accuracy: 66.60%\n",
      "Epoch [40/100], Loss: 0.5942, Val Loss: 0.6001, Accuracy: 66.49%\n",
      "Epoch [50/100], Loss: 0.5920, Val Loss: 0.6009, Accuracy: 66.55%\n",
      "Epoch [60/100], Loss: 0.5924, Val Loss: 0.6006, Accuracy: 66.34%\n",
      "Epoch [70/100], Loss: 0.5899, Val Loss: 0.6004, Accuracy: 66.49%\n",
      "Epoch [80/100], Loss: 0.5932, Val Loss: 0.6005, Accuracy: 66.49%\n",
      "Epoch [90/100], Loss: 0.5928, Val Loss: 0.6008, Accuracy: 66.18%\n",
      "Epoch [100/100], Loss: 0.5917, Val Loss: 0.6007, Accuracy: 66.60%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6161, Val Loss: 0.5968, Accuracy: 67.42%\n",
      "Epoch [20/100], Loss: 0.6038, Val Loss: 0.5903, Accuracy: 67.17%\n",
      "Epoch [30/100], Loss: 0.5994, Val Loss: 0.5890, Accuracy: 67.32%\n",
      "Epoch [40/100], Loss: 0.6012, Val Loss: 0.5892, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.5994, Val Loss: 0.5890, Accuracy: 67.11%\n",
      "Epoch [60/100], Loss: 0.5999, Val Loss: 0.5894, Accuracy: 67.73%\n",
      "Epoch [70/100], Loss: 0.5990, Val Loss: 0.5891, Accuracy: 67.48%\n",
      "Epoch [80/100], Loss: 0.6021, Val Loss: 0.5894, Accuracy: 67.22%\n",
      "Epoch [90/100], Loss: 0.5990, Val Loss: 0.5895, Accuracy: 67.79%\n",
      "Epoch [100/100], Loss: 0.5980, Val Loss: 0.5889, Accuracy: 67.11%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6092, Val Loss: 0.6131, Accuracy: 65.20%\n",
      "Epoch [20/100], Loss: 0.5969, Val Loss: 0.6104, Accuracy: 65.77%\n",
      "Epoch [30/100], Loss: 0.5937, Val Loss: 0.6096, Accuracy: 66.39%\n",
      "Epoch [40/100], Loss: 0.5917, Val Loss: 0.6104, Accuracy: 66.13%\n",
      "Epoch [50/100], Loss: 0.5873, Val Loss: 0.6104, Accuracy: 66.29%\n",
      "Epoch [60/100], Loss: 0.5920, Val Loss: 0.6095, Accuracy: 66.55%\n",
      "Epoch [70/100], Loss: 0.5894, Val Loss: 0.6104, Accuracy: 66.49%\n",
      "Epoch [80/100], Loss: 0.5916, Val Loss: 0.6103, Accuracy: 66.49%\n",
      "Epoch [90/100], Loss: 0.5925, Val Loss: 0.6096, Accuracy: 66.39%\n",
      "Epoch [100/100], Loss: 0.5890, Val Loss: 0.6095, Accuracy: 66.49%\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6042, Val Loss: 0.6186, Accuracy: 65.39%\n",
      "Epoch [20/100], Loss: 0.5971, Val Loss: 0.6107, Accuracy: 64.93%\n",
      "Epoch [30/100], Loss: 0.5955, Val Loss: 0.6127, Accuracy: 64.77%\n",
      "Epoch [40/100], Loss: 0.5906, Val Loss: 0.6136, Accuracy: 65.44%\n",
      "Epoch [50/100], Loss: 0.5883, Val Loss: 0.6114, Accuracy: 65.03%\n",
      "Epoch [60/100], Loss: 0.5882, Val Loss: 0.6122, Accuracy: 65.08%\n",
      "Epoch [70/100], Loss: 0.5885, Val Loss: 0.6120, Accuracy: 65.19%\n",
      "Epoch [80/100], Loss: 0.5894, Val Loss: 0.6125, Accuracy: 65.08%\n",
      "Epoch [90/100], Loss: 0.5879, Val Loss: 0.6117, Accuracy: 65.19%\n",
      "Epoch [100/100], Loss: 0.5889, Val Loss: 0.6118, Accuracy: 64.67%\n",
      "Best cross-validated accuracy: 68.92%\n",
      "Epoch [10/100], Loss: 0.6070, Val Loss: 0.6035, Accuracy: 67.45%\n",
      "Epoch [20/100], Loss: 0.6009, Val Loss: 0.6030, Accuracy: 67.12%\n",
      "Epoch [30/100], Loss: 0.5959, Val Loss: 0.6021, Accuracy: 67.45%\n",
      "Epoch [40/100], Loss: 0.5923, Val Loss: 0.6026, Accuracy: 67.29%\n",
      "Epoch [50/100], Loss: 0.5936, Val Loss: 0.6023, Accuracy: 67.66%\n",
      "Epoch [60/100], Loss: 0.5918, Val Loss: 0.6028, Accuracy: 67.53%\n",
      "Epoch [70/100], Loss: 0.5950, Val Loss: 0.6023, Accuracy: 67.37%\n",
      "Epoch [80/100], Loss: 0.5964, Val Loss: 0.6024, Accuracy: 67.45%\n",
      "Epoch [90/100], Loss: 0.5947, Val Loss: 0.6028, Accuracy: 67.41%\n",
      "Epoch [100/100], Loss: 0.5936, Val Loss: 0.6023, Accuracy: 67.37%\n",
      "Final model accuracy on test set: 67.82%\n",
      "Accuracy 67.82% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6084, Val Loss: 0.6021, Accuracy: 66.44%\n",
      "Epoch [20/100], Loss: 0.6031, Val Loss: 0.5986, Accuracy: 66.75%\n",
      "Epoch [30/100], Loss: 0.6006, Val Loss: 0.6031, Accuracy: 66.70%\n",
      "Epoch [40/100], Loss: 0.5940, Val Loss: 0.6001, Accuracy: 66.49%\n",
      "Epoch [50/100], Loss: 0.5906, Val Loss: 0.6001, Accuracy: 66.34%\n",
      "Epoch [60/100], Loss: 0.5956, Val Loss: 0.6003, Accuracy: 66.60%\n",
      "Epoch [70/100], Loss: 0.5955, Val Loss: 0.6004, Accuracy: 66.24%\n",
      "Epoch [80/100], Loss: 0.5951, Val Loss: 0.6004, Accuracy: 66.13%\n",
      "Epoch [90/100], Loss: 0.5937, Val Loss: 0.6002, Accuracy: 66.34%\n",
      "Epoch [100/100], Loss: 0.5915, Val Loss: 0.6001, Accuracy: 66.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6115, Val Loss: 0.5909, Accuracy: 67.32%\n",
      "Epoch [20/100], Loss: 0.6034, Val Loss: 0.5897, Accuracy: 67.42%\n",
      "Epoch [30/100], Loss: 0.5990, Val Loss: 0.5888, Accuracy: 67.42%\n",
      "Epoch [40/100], Loss: 0.6031, Val Loss: 0.5889, Accuracy: 67.68%\n",
      "Epoch [50/100], Loss: 0.5967, Val Loss: 0.5887, Accuracy: 67.37%\n",
      "Epoch [60/100], Loss: 0.5985, Val Loss: 0.5883, Accuracy: 67.63%\n",
      "Epoch [70/100], Loss: 0.5994, Val Loss: 0.5886, Accuracy: 67.63%\n",
      "Epoch [80/100], Loss: 0.5965, Val Loss: 0.5884, Accuracy: 67.89%\n",
      "Epoch [90/100], Loss: 0.5961, Val Loss: 0.5884, Accuracy: 67.79%\n",
      "Epoch [100/100], Loss: 0.5984, Val Loss: 0.5887, Accuracy: 67.63%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6053, Val Loss: 0.6135, Accuracy: 65.20%\n",
      "Epoch [20/100], Loss: 0.6009, Val Loss: 0.6115, Accuracy: 65.57%\n",
      "Epoch [30/100], Loss: 0.5962, Val Loss: 0.6082, Accuracy: 65.82%\n",
      "Epoch [40/100], Loss: 0.5928, Val Loss: 0.6098, Accuracy: 66.03%\n",
      "Epoch [50/100], Loss: 0.5891, Val Loss: 0.6076, Accuracy: 66.60%\n",
      "Epoch [60/100], Loss: 0.5854, Val Loss: 0.6066, Accuracy: 66.49%\n",
      "Epoch [70/100], Loss: 0.5853, Val Loss: 0.6070, Accuracy: 66.49%\n",
      "Epoch [80/100], Loss: 0.5867, Val Loss: 0.6072, Accuracy: 66.39%\n",
      "Epoch [90/100], Loss: 0.5860, Val Loss: 0.6070, Accuracy: 66.44%\n",
      "Epoch [100/100], Loss: 0.5869, Val Loss: 0.6072, Accuracy: 66.34%\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6049, Val Loss: 0.6116, Accuracy: 65.70%\n",
      "Epoch [20/100], Loss: 0.6005, Val Loss: 0.6094, Accuracy: 65.65%\n",
      "Epoch [30/100], Loss: 0.5953, Val Loss: 0.6065, Accuracy: 65.19%\n",
      "Epoch [40/100], Loss: 0.5915, Val Loss: 0.6096, Accuracy: 65.24%\n",
      "Epoch [50/100], Loss: 0.5859, Val Loss: 0.6089, Accuracy: 64.82%\n",
      "Epoch [60/100], Loss: 0.5883, Val Loss: 0.6088, Accuracy: 64.57%\n",
      "Epoch [70/100], Loss: 0.5881, Val Loss: 0.6090, Accuracy: 64.82%\n",
      "Epoch [80/100], Loss: 0.5896, Val Loss: 0.6088, Accuracy: 64.46%\n",
      "Epoch [90/100], Loss: 0.5844, Val Loss: 0.6087, Accuracy: 64.62%\n",
      "Epoch [100/100], Loss: 0.5883, Val Loss: 0.6096, Accuracy: 64.57%\n",
      "Best cross-validated accuracy: 69.18%\n",
      "Epoch [10/100], Loss: 0.6054, Val Loss: 0.6008, Accuracy: 67.53%\n",
      "Epoch [20/100], Loss: 0.6001, Val Loss: 0.5990, Accuracy: 68.07%\n",
      "Epoch [30/100], Loss: 0.5962, Val Loss: 0.5991, Accuracy: 67.95%\n",
      "Epoch [40/100], Loss: 0.5946, Val Loss: 0.5991, Accuracy: 67.66%\n",
      "Epoch [50/100], Loss: 0.5945, Val Loss: 0.5992, Accuracy: 67.99%\n",
      "Epoch [60/100], Loss: 0.5951, Val Loss: 0.5994, Accuracy: 67.58%\n",
      "Epoch [70/100], Loss: 0.5944, Val Loss: 0.5989, Accuracy: 67.70%\n",
      "Epoch [80/100], Loss: 0.5949, Val Loss: 0.5993, Accuracy: 67.95%\n",
      "Epoch [90/100], Loss: 0.5968, Val Loss: 0.5993, Accuracy: 67.91%\n",
      "Epoch [100/100], Loss: 0.5931, Val Loss: 0.5991, Accuracy: 67.66%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6111, Val Loss: 0.6013, Accuracy: 65.72%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m final_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m final_acc \u001b[38;5;241m<\u001b[39m target_accuracy:\n\u001b[0;32m---> 24\u001b[0m     best_acc, best_model_weights \u001b[38;5;241m=\u001b[39m cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, target_accuracy\u001b[38;5;241m=\u001b[39mtarget_accuracy)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validated accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Final training on full training data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[101], line 23\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(model_class, X, y, folds, epochs, batch_size, target_accuracy)\u001b[0m\n\u001b[1;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(input_dim\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m fold_acc, fold_weights \u001b[38;5;241m=\u001b[39m train_and_evaluate(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39mepochs, target_accuracy\u001b[38;5;241m=\u001b[39mtarget_accuracy)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fold_acc \u001b[38;5;241m>\u001b[39m best_acc:\n\u001b[1;32m     25\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m fold_acc\n",
      "Cell \u001b[0;32mIn[100], line 16\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, weight_decay, target_accuracy)\u001b[0m\n\u001b[1;32m     14\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 16\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 37\u001b[0m, in \u001b[0;36mOutcomeProbabilityV6.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_block1(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_block2(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_block3(x)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 11\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)))\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m residual\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    184\u001b[0m     bn_training,\n\u001b[1;32m    185\u001b[0m     exponential_average_factor,\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    187\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2510\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2511\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "# df = df[~(df == -20).any(axis=1)]\n",
    "# df = df[(df['surface_Hard'] == 1.0)]\n",
    "\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.689\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "    # Quick train break\n",
    "    # break\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        1.0   0.755534    1.11    6.73\n",
      "1        1.0   0.541769    1.67    2.15\n",
      "2        0.0   0.705679    1.51    2.48\n",
      "3        1.0   0.398880    2.70    1.43\n",
      "4        1.0   0.711706    1.71    2.10\n",
      "...      ...        ...     ...     ...\n",
      "3478     0.0   0.194122    2.61    1.47\n",
      "3479     0.0   0.464807    2.63    1.48\n",
      "3480     0.0   0.402206    3.26    1.32\n",
      "3481     0.0   0.412963    2.35    1.58\n",
      "3482     1.0   0.430298    1.36    3.05\n",
      "\n",
      "[3483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly Criterion\n",
    "def kelly_criterion(vegas_odds, calculated_probability):\n",
    "    # Calculate the Kelly fraction\n",
    "    kelly_fraction = (vegas_odds * calculated_probability - (1 - calculated_probability)) / vegas_odds\n",
    "    \n",
    "    # Ensure that the fraction is not negative\n",
    "    kelly_fraction = max(0, kelly_fraction)\n",
    "    \n",
    "    return kelly_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $10 bets: 189.00 on a total # bets: 158 from a total of 3483 games\n",
      "Amount of differing favorites 0.13149583692219352\n",
      "Amount of upset correct 0.4432314410480349 won $-67.79999999999995 on 458 bets\n",
      "Amount of incorrect bets : 0.47468354430379744\n",
      "Correct Bets: 0.5253164556962026\n",
      "Model % Correct : 0.6881998277347114 Vegas Correct % : 0.7025552684467413\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "upset_predict = 0\n",
    "upset_correct = 0\n",
    "upset_won = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .55\n",
    "confidence_top_pct = .6\n",
    "\n",
    "UNIT = 10\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct and row['Predicted'] > 1/row['A_Odds'] + .03:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['A_Odds']-1) * UNIT #(kelly_criterion(row['A_Odds'],row['Predicted']) * UNIT)\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT #(kelly_criterion(row['A_Odds'],row['Predicted']) * UNIT)\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct and 1-row['Predicted'] > 1/row['B_Odds']+ .03:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['B_Odds']-1) * UNIT #(kelly_criterion(row['B_Odds'],row['Predicted']) * UNIT)\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT #(kelly_criterion(row['B_Odds'],row['Predicted']) * UNIT)\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        upset_predict += 1\n",
    "        if round(row['Predicted']) == round(row['Actual']):\n",
    "            upset_correct += 1\n",
    "            if(row['Actual'] == 1):\n",
    "                upset_won += (row['A_Odds']-1) * UNIT\n",
    "            else:\n",
    "                upset_won += (row['B_Odds']-1) * UNIT\n",
    "        else:\n",
    "            upset_won -= UNIT\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on ${UNIT} bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of upset correct {upset_correct/upset_predict} won ${upset_won} on {upset_predict} bets\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
