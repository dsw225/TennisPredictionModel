{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36152\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('testcsvs/testout2.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_glicko_rd'] <= 150) & (df['b_glicko_rd'] <= 150)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff_high</th>\n",
       "      <th>glicko_rating_diff_low</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>point_glicko_rating_diff_high</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_high</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_low</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-312.688126</td>\n",
       "      <td>-40.429749</td>\n",
       "      <td>1663.238130</td>\n",
       "      <td>1839.797067</td>\n",
       "      <td>69.521886</td>\n",
       "      <td>66.607302</td>\n",
       "      <td>-146.037612</td>\n",
       "      <td>...</td>\n",
       "      <td>-142.430173</td>\n",
       "      <td>111.103350</td>\n",
       "      <td>1514.785695</td>\n",
       "      <td>1530.449107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-121.419605</td>\n",
       "      <td>147.756632</td>\n",
       "      <td>1704.915868</td>\n",
       "      <td>1691.747354</td>\n",
       "      <td>66.531031</td>\n",
       "      <td>68.057087</td>\n",
       "      <td>-124.530717</td>\n",
       "      <td>...</td>\n",
       "      <td>-125.068212</td>\n",
       "      <td>125.074139</td>\n",
       "      <td>1524.307970</td>\n",
       "      <td>1524.305006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-124.132627</td>\n",
       "      <td>182.766003</td>\n",
       "      <td>1599.867737</td>\n",
       "      <td>1570.551049</td>\n",
       "      <td>70.223143</td>\n",
       "      <td>83.226172</td>\n",
       "      <td>-148.102765</td>\n",
       "      <td>...</td>\n",
       "      <td>-130.959262</td>\n",
       "      <td>127.522140</td>\n",
       "      <td>1508.366657</td>\n",
       "      <td>1510.085217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-226.786525</td>\n",
       "      <td>45.492588</td>\n",
       "      <td>1640.524463</td>\n",
       "      <td>1731.171432</td>\n",
       "      <td>70.112236</td>\n",
       "      <td>66.027320</td>\n",
       "      <td>-131.094445</td>\n",
       "      <td>...</td>\n",
       "      <td>-127.146095</td>\n",
       "      <td>125.314148</td>\n",
       "      <td>1507.539322</td>\n",
       "      <td>1508.455296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-452.711077</td>\n",
       "      <td>-158.974000</td>\n",
       "      <td>1545.639101</td>\n",
       "      <td>1851.481640</td>\n",
       "      <td>79.730461</td>\n",
       "      <td>67.138078</td>\n",
       "      <td>-168.643719</td>\n",
       "      <td>...</td>\n",
       "      <td>-143.077859</td>\n",
       "      <td>109.514614</td>\n",
       "      <td>1486.558404</td>\n",
       "      <td>1503.340027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  a_player_rank  b_player_rank  glicko_rating_diff_high  \\\n",
       "5373      3.0           74.0           15.0              -312.688126   \n",
       "5375      3.0           46.0           65.0              -121.419605   \n",
       "5377      3.0           89.0           95.0              -124.132627   \n",
       "5378      3.0           83.0           48.0              -226.786525   \n",
       "5380      3.0           70.0           22.0              -452.711077   \n",
       "\n",
       "      glicko_rating_diff_low  a_glicko_rating  b_glicko_rating  a_glicko_rd  \\\n",
       "5373              -40.429749      1663.238130      1839.797067    69.521886   \n",
       "5375              147.756632      1704.915868      1691.747354    66.531031   \n",
       "5377              182.766003      1599.867737      1570.551049    70.223143   \n",
       "5378               45.492588      1640.524463      1731.171432    70.112236   \n",
       "5380             -158.974000      1545.639101      1851.481640    79.730461   \n",
       "\n",
       "      b_glicko_rd  point_glicko_rating_diff_high  ...  \\\n",
       "5373    66.607302                    -146.037612  ...   \n",
       "5375    68.057087                    -124.530717  ...   \n",
       "5377    83.226172                    -148.102765  ...   \n",
       "5378    66.027320                    -131.094445  ...   \n",
       "5380    67.138078                    -168.643719  ...   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_high  \\\n",
       "5373                                        -142.430173   \n",
       "5375                                        -125.068212   \n",
       "5377                                        -130.959262   \n",
       "5378                                        -127.146095   \n",
       "5380                                        -143.077859   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_low  \\\n",
       "5373                                        111.103350   \n",
       "5375                                        125.074139   \n",
       "5377                                        127.522140   \n",
       "5378                                        125.314148   \n",
       "5380                                        109.514614   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rating  \\\n",
       "5373                                1514.785695   \n",
       "5375                                1524.307970   \n",
       "5377                                1508.366657   \n",
       "5378                                1507.539322   \n",
       "5380                                1486.558404   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  a_b_win  a_odds  b_odds  \\\n",
       "5373                         1530.449107      0.0    3.59    1.28   \n",
       "5375                         1524.305006      0.0     NaN     NaN   \n",
       "5377                         1510.085217      0.0    2.29    1.59   \n",
       "5378                         1508.455296      1.0    1.54    2.40   \n",
       "5380                         1503.340027      0.0    4.44    1.19   \n",
       "\n",
       "      surface_Clay  surface_Grass  surface_Hard  \n",
       "5373           0.0            0.0           1.0  \n",
       "5375           0.0            0.0           1.0  \n",
       "5377           0.0            0.0           1.0  \n",
       "5378           0.0            0.0           1.0  \n",
       "5380           0.0            0.0           1.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6191, Val Loss: 0.6160, Accuracy: 64.50%\n",
      "Epoch [20/100], Loss: 0.6140, Val Loss: 0.6160, Accuracy: 64.93%\n",
      "Epoch [30/100], Loss: 0.6120, Val Loss: 0.6156, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6142, Accuracy: 65.15%\n",
      "Epoch [50/100], Loss: 0.6091, Val Loss: 0.6145, Accuracy: 65.47%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.6144, Accuracy: 65.18%\n",
      "Epoch [70/100], Loss: 0.6078, Val Loss: 0.6142, Accuracy: 65.36%\n",
      "Epoch [80/100], Loss: 0.6068, Val Loss: 0.6141, Accuracy: 65.18%\n",
      "Epoch [90/100], Loss: 0.6069, Val Loss: 0.6149, Accuracy: 65.25%\n",
      "Epoch [100/100], Loss: 0.6069, Val Loss: 0.6158, Accuracy: 64.68%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6286, Val Loss: 0.5959, Accuracy: 67.19%\n",
      "Epoch [20/100], Loss: 0.6172, Val Loss: 0.5943, Accuracy: 67.37%\n",
      "Epoch [30/100], Loss: 0.6179, Val Loss: 0.5977, Accuracy: 67.30%\n",
      "Epoch [40/100], Loss: 0.6169, Val Loss: 0.5929, Accuracy: 67.37%\n",
      "Epoch [50/100], Loss: 0.6135, Val Loss: 0.5905, Accuracy: 67.77%\n",
      "Epoch [60/100], Loss: 0.6129, Val Loss: 0.5907, Accuracy: 67.55%\n",
      "Epoch [70/100], Loss: 0.6125, Val Loss: 0.5904, Accuracy: 67.62%\n",
      "Epoch [80/100], Loss: 0.6136, Val Loss: 0.5936, Accuracy: 67.34%\n",
      "Epoch [90/100], Loss: 0.6151, Val Loss: 0.5927, Accuracy: 67.62%\n",
      "Epoch [100/100], Loss: 0.6161, Val Loss: 0.5908, Accuracy: 67.34%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6197, Val Loss: 0.6124, Accuracy: 64.39%\n",
      "Epoch [20/100], Loss: 0.6147, Val Loss: 0.6095, Accuracy: 65.43%\n",
      "Epoch [30/100], Loss: 0.6127, Val Loss: 0.6110, Accuracy: 65.69%\n",
      "Epoch [40/100], Loss: 0.6114, Val Loss: 0.6083, Accuracy: 65.29%\n",
      "Epoch [50/100], Loss: 0.6077, Val Loss: 0.6083, Accuracy: 65.76%\n",
      "Epoch [60/100], Loss: 0.6091, Val Loss: 0.6082, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6080, Val Loss: 0.6080, Accuracy: 65.69%\n",
      "Epoch [80/100], Loss: 0.6082, Val Loss: 0.6087, Accuracy: 65.33%\n",
      "Epoch [90/100], Loss: 0.6073, Val Loss: 0.6080, Accuracy: 65.65%\n",
      "Epoch [100/100], Loss: 0.6089, Val Loss: 0.6082, Accuracy: 65.58%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6170, Val Loss: 0.6187, Accuracy: 64.81%\n",
      "Epoch [20/100], Loss: 0.6123, Val Loss: 0.6189, Accuracy: 64.49%\n",
      "Epoch [30/100], Loss: 0.6087, Val Loss: 0.6196, Accuracy: 65.06%\n",
      "Epoch [40/100], Loss: 0.6095, Val Loss: 0.6183, Accuracy: 65.06%\n",
      "Epoch [50/100], Loss: 0.6091, Val Loss: 0.6200, Accuracy: 64.78%\n",
      "Epoch [60/100], Loss: 0.6076, Val Loss: 0.6189, Accuracy: 64.96%\n",
      "Epoch [70/100], Loss: 0.6074, Val Loss: 0.6189, Accuracy: 64.74%\n",
      "Epoch [80/100], Loss: 0.6079, Val Loss: 0.6190, Accuracy: 64.81%\n",
      "Epoch [90/100], Loss: 0.6089, Val Loss: 0.6191, Accuracy: 65.03%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6189, Accuracy: 64.99%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6209, Val Loss: 0.6071, Accuracy: 66.00%\n",
      "Epoch [20/100], Loss: 0.6162, Val Loss: 0.6056, Accuracy: 66.25%\n",
      "Epoch [30/100], Loss: 0.6141, Val Loss: 0.6043, Accuracy: 66.21%\n",
      "Epoch [40/100], Loss: 0.6088, Val Loss: 0.6055, Accuracy: 66.10%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6040, Accuracy: 66.18%\n",
      "Epoch [60/100], Loss: 0.6067, Val Loss: 0.6026, Accuracy: 66.57%\n",
      "Epoch [70/100], Loss: 0.6070, Val Loss: 0.6031, Accuracy: 66.39%\n",
      "Epoch [80/100], Loss: 0.6060, Val Loss: 0.6044, Accuracy: 66.36%\n",
      "Epoch [90/100], Loss: 0.6055, Val Loss: 0.6031, Accuracy: 66.75%\n",
      "Epoch [100/100], Loss: 0.6055, Val Loss: 0.6035, Accuracy: 66.57%\n",
      "Best cross-validated accuracy: 68.27%\n",
      "Epoch [10/100], Loss: 0.6112, Val Loss: 0.5975, Accuracy: 68.39%\n",
      "Epoch [20/100], Loss: 0.6086, Val Loss: 0.5977, Accuracy: 68.19%\n",
      "Epoch [30/100], Loss: 0.6083, Val Loss: 0.5972, Accuracy: 68.33%\n",
      "Epoch [40/100], Loss: 0.6084, Val Loss: 0.5976, Accuracy: 68.39%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.5976, Accuracy: 67.99%\n",
      "Epoch [60/100], Loss: 0.6090, Val Loss: 0.5975, Accuracy: 68.04%\n",
      "Epoch [70/100], Loss: 0.6074, Val Loss: 0.5975, Accuracy: 68.19%\n",
      "Epoch [80/100], Loss: 0.6085, Val Loss: 0.5975, Accuracy: 68.16%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.5974, Accuracy: 68.02%\n",
      "Epoch [100/100], Loss: 0.6075, Val Loss: 0.5975, Accuracy: 68.27%\n",
      "Final model accuracy on test set: 68.50%\n",
      "Accuracy 68.50% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6199, Val Loss: 0.6233, Accuracy: 63.71%\n",
      "Epoch [20/100], Loss: 0.6129, Val Loss: 0.6146, Accuracy: 64.93%\n",
      "Epoch [30/100], Loss: 0.6123, Val Loss: 0.6161, Accuracy: 65.11%\n",
      "Epoch [40/100], Loss: 0.6088, Val Loss: 0.6150, Accuracy: 65.11%\n",
      "Epoch [50/100], Loss: 0.6079, Val Loss: 0.6160, Accuracy: 64.93%\n",
      "Epoch [60/100], Loss: 0.6085, Val Loss: 0.6154, Accuracy: 64.90%\n",
      "Epoch [70/100], Loss: 0.6092, Val Loss: 0.6151, Accuracy: 64.93%\n",
      "Epoch [80/100], Loss: 0.6085, Val Loss: 0.6158, Accuracy: 65.00%\n",
      "Epoch [90/100], Loss: 0.6086, Val Loss: 0.6151, Accuracy: 65.25%\n",
      "Epoch [100/100], Loss: 0.6083, Val Loss: 0.6151, Accuracy: 65.11%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6275, Val Loss: 0.5953, Accuracy: 67.66%\n",
      "Epoch [20/100], Loss: 0.6226, Val Loss: 0.5939, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.6145, Val Loss: 0.5933, Accuracy: 68.02%\n",
      "Epoch [40/100], Loss: 0.6164, Val Loss: 0.5916, Accuracy: 67.70%\n",
      "Epoch [50/100], Loss: 0.6162, Val Loss: 0.5941, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.6158, Val Loss: 0.5922, Accuracy: 67.80%\n",
      "Epoch [70/100], Loss: 0.6170, Val Loss: 0.5924, Accuracy: 67.59%\n",
      "Epoch [80/100], Loss: 0.6153, Val Loss: 0.5930, Accuracy: 67.66%\n",
      "Epoch [90/100], Loss: 0.6168, Val Loss: 0.5916, Accuracy: 67.62%\n",
      "Epoch [100/100], Loss: 0.6167, Val Loss: 0.5926, Accuracy: 67.95%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6155, Val Loss: 0.6119, Accuracy: 64.90%\n",
      "Epoch [20/100], Loss: 0.6140, Val Loss: 0.6091, Accuracy: 65.72%\n",
      "Epoch [30/100], Loss: 0.6133, Val Loss: 0.6093, Accuracy: 65.40%\n",
      "Epoch [40/100], Loss: 0.6096, Val Loss: 0.6093, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6077, Val Loss: 0.6081, Accuracy: 65.33%\n",
      "Epoch [60/100], Loss: 0.6081, Val Loss: 0.6077, Accuracy: 65.51%\n",
      "Epoch [70/100], Loss: 0.6062, Val Loss: 0.6072, Accuracy: 65.65%\n",
      "Epoch [80/100], Loss: 0.6052, Val Loss: 0.6074, Accuracy: 65.65%\n",
      "Epoch [90/100], Loss: 0.6057, Val Loss: 0.6073, Accuracy: 65.61%\n",
      "Epoch [100/100], Loss: 0.6054, Val Loss: 0.6078, Accuracy: 65.40%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6183, Val Loss: 0.6204, Accuracy: 64.74%\n",
      "Epoch [20/100], Loss: 0.6118, Val Loss: 0.6195, Accuracy: 65.10%\n",
      "Epoch [30/100], Loss: 0.6089, Val Loss: 0.6198, Accuracy: 64.74%\n",
      "Epoch [40/100], Loss: 0.6060, Val Loss: 0.6220, Accuracy: 63.84%\n",
      "Epoch [50/100], Loss: 0.6071, Val Loss: 0.6189, Accuracy: 65.39%\n",
      "Epoch [60/100], Loss: 0.6098, Val Loss: 0.6209, Accuracy: 64.99%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.6207, Accuracy: 64.31%\n",
      "Epoch [80/100], Loss: 0.6052, Val Loss: 0.6185, Accuracy: 65.24%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.6202, Accuracy: 64.88%\n",
      "Epoch [100/100], Loss: 0.6071, Val Loss: 0.6193, Accuracy: 65.28%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6220, Val Loss: 0.6057, Accuracy: 66.71%\n",
      "Epoch [20/100], Loss: 0.6155, Val Loss: 0.6055, Accuracy: 65.96%\n",
      "Epoch [30/100], Loss: 0.6120, Val Loss: 0.6036, Accuracy: 66.07%\n",
      "Epoch [40/100], Loss: 0.6116, Val Loss: 0.6032, Accuracy: 66.32%\n",
      "Epoch [50/100], Loss: 0.6114, Val Loss: 0.6044, Accuracy: 65.71%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.6043, Accuracy: 66.39%\n",
      "Epoch [70/100], Loss: 0.6092, Val Loss: 0.6032, Accuracy: 65.96%\n",
      "Epoch [80/100], Loss: 0.6077, Val Loss: 0.6036, Accuracy: 66.03%\n",
      "Epoch [90/100], Loss: 0.6067, Val Loss: 0.6037, Accuracy: 66.25%\n",
      "Epoch [100/100], Loss: 0.6091, Val Loss: 0.6032, Accuracy: 66.18%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6119, Val Loss: 0.5986, Accuracy: 68.30%\n",
      "Epoch [20/100], Loss: 0.6107, Val Loss: 0.5996, Accuracy: 68.22%\n",
      "Epoch [30/100], Loss: 0.6069, Val Loss: 0.5976, Accuracy: 67.90%\n",
      "Epoch [40/100], Loss: 0.6044, Val Loss: 0.5980, Accuracy: 68.10%\n",
      "Epoch [50/100], Loss: 0.6068, Val Loss: 0.5978, Accuracy: 67.96%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.5978, Accuracy: 67.79%\n",
      "Epoch [70/100], Loss: 0.6075, Val Loss: 0.5976, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6061, Val Loss: 0.5977, Accuracy: 67.87%\n",
      "Epoch [90/100], Loss: 0.6070, Val Loss: 0.5978, Accuracy: 67.96%\n",
      "Epoch [100/100], Loss: 0.6074, Val Loss: 0.5980, Accuracy: 68.13%\n",
      "Final model accuracy on test set: 68.30%\n",
      "Accuracy 68.30% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6185, Val Loss: 0.6199, Accuracy: 65.04%\n",
      "Epoch [20/100], Loss: 0.6126, Val Loss: 0.6147, Accuracy: 65.04%\n",
      "Epoch [30/100], Loss: 0.6113, Val Loss: 0.6168, Accuracy: 65.22%\n",
      "Epoch [40/100], Loss: 0.6096, Val Loss: 0.6156, Accuracy: 65.22%\n",
      "Epoch [50/100], Loss: 0.6055, Val Loss: 0.6149, Accuracy: 65.15%\n",
      "Epoch [60/100], Loss: 0.6098, Val Loss: 0.6155, Accuracy: 65.43%\n",
      "Epoch [70/100], Loss: 0.6093, Val Loss: 0.6160, Accuracy: 65.36%\n",
      "Epoch [80/100], Loss: 0.6079, Val Loss: 0.6152, Accuracy: 65.15%\n",
      "Epoch [90/100], Loss: 0.6081, Val Loss: 0.6151, Accuracy: 65.58%\n",
      "Epoch [100/100], Loss: 0.6084, Val Loss: 0.6161, Accuracy: 65.22%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6255, Val Loss: 0.5960, Accuracy: 67.34%\n",
      "Epoch [20/100], Loss: 0.6203, Val Loss: 0.5925, Accuracy: 67.70%\n",
      "Epoch [30/100], Loss: 0.6160, Val Loss: 0.5940, Accuracy: 67.59%\n",
      "Epoch [40/100], Loss: 0.6134, Val Loss: 0.5944, Accuracy: 67.95%\n",
      "Epoch [50/100], Loss: 0.6145, Val Loss: 0.5959, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.6115, Val Loss: 0.5906, Accuracy: 68.31%\n",
      "Epoch [70/100], Loss: 0.6110, Val Loss: 0.5915, Accuracy: 67.95%\n",
      "Epoch [80/100], Loss: 0.6109, Val Loss: 0.5925, Accuracy: 68.34%\n",
      "Epoch [90/100], Loss: 0.6115, Val Loss: 0.5910, Accuracy: 68.41%\n",
      "Epoch [100/100], Loss: 0.6108, Val Loss: 0.5908, Accuracy: 67.98%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6215, Val Loss: 0.6147, Accuracy: 64.72%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6103, Accuracy: 65.36%\n",
      "Epoch [30/100], Loss: 0.6130, Val Loss: 0.6080, Accuracy: 66.01%\n",
      "Epoch [40/100], Loss: 0.6112, Val Loss: 0.6090, Accuracy: 65.94%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6084, Accuracy: 65.72%\n",
      "Epoch [60/100], Loss: 0.6082, Val Loss: 0.6069, Accuracy: 65.83%\n",
      "Epoch [70/100], Loss: 0.6079, Val Loss: 0.6069, Accuracy: 66.01%\n",
      "Epoch [80/100], Loss: 0.6075, Val Loss: 0.6078, Accuracy: 65.76%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.6079, Accuracy: 65.83%\n",
      "Epoch [100/100], Loss: 0.6069, Val Loss: 0.6080, Accuracy: 65.79%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6183, Val Loss: 0.6180, Accuracy: 64.81%\n",
      "Epoch [20/100], Loss: 0.6112, Val Loss: 0.6199, Accuracy: 64.38%\n",
      "Epoch [30/100], Loss: 0.6105, Val Loss: 0.6191, Accuracy: 65.35%\n",
      "Epoch [40/100], Loss: 0.6111, Val Loss: 0.6198, Accuracy: 64.96%\n",
      "Epoch [50/100], Loss: 0.6105, Val Loss: 0.6200, Accuracy: 64.96%\n",
      "Epoch [60/100], Loss: 0.6117, Val Loss: 0.6195, Accuracy: 64.67%\n",
      "Epoch [70/100], Loss: 0.6095, Val Loss: 0.6200, Accuracy: 64.81%\n",
      "Epoch [80/100], Loss: 0.6121, Val Loss: 0.6198, Accuracy: 65.17%\n",
      "Epoch [90/100], Loss: 0.6110, Val Loss: 0.6194, Accuracy: 65.06%\n",
      "Epoch [100/100], Loss: 0.6102, Val Loss: 0.6201, Accuracy: 64.96%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6067, Accuracy: 66.03%\n",
      "Epoch [20/100], Loss: 0.6151, Val Loss: 0.6048, Accuracy: 66.00%\n",
      "Epoch [30/100], Loss: 0.6142, Val Loss: 0.6046, Accuracy: 66.14%\n",
      "Epoch [40/100], Loss: 0.6103, Val Loss: 0.6048, Accuracy: 65.92%\n",
      "Epoch [50/100], Loss: 0.6098, Val Loss: 0.6034, Accuracy: 66.18%\n",
      "Epoch [60/100], Loss: 0.6074, Val Loss: 0.6034, Accuracy: 66.14%\n",
      "Epoch [70/100], Loss: 0.6096, Val Loss: 0.6046, Accuracy: 65.67%\n",
      "Epoch [80/100], Loss: 0.6092, Val Loss: 0.6032, Accuracy: 66.21%\n",
      "Epoch [90/100], Loss: 0.6083, Val Loss: 0.6034, Accuracy: 65.85%\n",
      "Epoch [100/100], Loss: 0.6078, Val Loss: 0.6059, Accuracy: 66.25%\n",
      "Best cross-validated accuracy: 68.41%\n",
      "Epoch [10/100], Loss: 0.6084, Val Loss: 0.5983, Accuracy: 67.61%\n",
      "Epoch [20/100], Loss: 0.6049, Val Loss: 0.5972, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.6060, Val Loss: 0.5980, Accuracy: 67.64%\n",
      "Epoch [40/100], Loss: 0.6043, Val Loss: 0.5976, Accuracy: 67.61%\n",
      "Epoch [50/100], Loss: 0.6064, Val Loss: 0.5977, Accuracy: 67.79%\n",
      "Epoch [60/100], Loss: 0.6055, Val Loss: 0.5977, Accuracy: 67.76%\n",
      "Epoch [70/100], Loss: 0.6052, Val Loss: 0.5979, Accuracy: 67.70%\n",
      "Epoch [80/100], Loss: 0.6048, Val Loss: 0.5977, Accuracy: 67.73%\n",
      "Epoch [90/100], Loss: 0.6040, Val Loss: 0.5979, Accuracy: 67.76%\n",
      "Epoch [100/100], Loss: 0.6042, Val Loss: 0.5977, Accuracy: 67.70%\n",
      "Final model accuracy on test set: 68.22%\n",
      "Accuracy 68.22% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6185, Val Loss: 0.6177, Accuracy: 64.75%\n",
      "Epoch [20/100], Loss: 0.6136, Val Loss: 0.6171, Accuracy: 64.64%\n",
      "Epoch [30/100], Loss: 0.6136, Val Loss: 0.6170, Accuracy: 65.15%\n",
      "Epoch [40/100], Loss: 0.6082, Val Loss: 0.6172, Accuracy: 64.97%\n",
      "Epoch [50/100], Loss: 0.6071, Val Loss: 0.6176, Accuracy: 65.29%\n",
      "Epoch [60/100], Loss: 0.6055, Val Loss: 0.6170, Accuracy: 65.08%\n",
      "Epoch [70/100], Loss: 0.6057, Val Loss: 0.6159, Accuracy: 64.93%\n",
      "Epoch [80/100], Loss: 0.6052, Val Loss: 0.6162, Accuracy: 64.72%\n",
      "Epoch [90/100], Loss: 0.6054, Val Loss: 0.6169, Accuracy: 65.04%\n",
      "Epoch [100/100], Loss: 0.6071, Val Loss: 0.6159, Accuracy: 65.04%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6237, Val Loss: 0.5940, Accuracy: 67.98%\n",
      "Epoch [20/100], Loss: 0.6161, Val Loss: 0.5929, Accuracy: 67.95%\n",
      "Epoch [30/100], Loss: 0.6144, Val Loss: 0.5909, Accuracy: 68.13%\n",
      "Epoch [40/100], Loss: 0.6149, Val Loss: 0.5926, Accuracy: 67.88%\n",
      "Epoch [50/100], Loss: 0.6138, Val Loss: 0.5910, Accuracy: 67.66%\n",
      "Epoch [60/100], Loss: 0.6122, Val Loss: 0.5924, Accuracy: 67.84%\n",
      "Epoch [70/100], Loss: 0.6128, Val Loss: 0.5911, Accuracy: 67.98%\n",
      "Epoch [80/100], Loss: 0.6122, Val Loss: 0.5914, Accuracy: 67.80%\n",
      "Epoch [90/100], Loss: 0.6133, Val Loss: 0.5905, Accuracy: 68.09%\n",
      "Epoch [100/100], Loss: 0.6106, Val Loss: 0.5916, Accuracy: 68.16%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6203, Val Loss: 0.6119, Accuracy: 65.40%\n",
      "Epoch [20/100], Loss: 0.6146, Val Loss: 0.6103, Accuracy: 65.69%\n",
      "Epoch [30/100], Loss: 0.6125, Val Loss: 0.6108, Accuracy: 65.40%\n",
      "Epoch [40/100], Loss: 0.6093, Val Loss: 0.6092, Accuracy: 65.43%\n",
      "Epoch [50/100], Loss: 0.6092, Val Loss: 0.6089, Accuracy: 65.58%\n",
      "Epoch [60/100], Loss: 0.6106, Val Loss: 0.6087, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6085, Val Loss: 0.6085, Accuracy: 65.61%\n",
      "Epoch [80/100], Loss: 0.6090, Val Loss: 0.6084, Accuracy: 65.61%\n",
      "Epoch [90/100], Loss: 0.6096, Val Loss: 0.6084, Accuracy: 65.47%\n",
      "Epoch [100/100], Loss: 0.6095, Val Loss: 0.6083, Accuracy: 65.54%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6184, Val Loss: 0.6192, Accuracy: 65.13%\n",
      "Epoch [20/100], Loss: 0.6126, Val Loss: 0.6189, Accuracy: 64.56%\n",
      "Epoch [30/100], Loss: 0.6115, Val Loss: 0.6192, Accuracy: 64.81%\n",
      "Epoch [40/100], Loss: 0.6083, Val Loss: 0.6200, Accuracy: 64.70%\n",
      "Epoch [50/100], Loss: 0.6098, Val Loss: 0.6196, Accuracy: 64.78%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6200, Accuracy: 64.63%\n",
      "Epoch [70/100], Loss: 0.6116, Val Loss: 0.6191, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6100, Val Loss: 0.6192, Accuracy: 64.78%\n",
      "Epoch [90/100], Loss: 0.6090, Val Loss: 0.6189, Accuracy: 64.52%\n",
      "Epoch [100/100], Loss: 0.6076, Val Loss: 0.6189, Accuracy: 64.70%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6216, Val Loss: 0.6084, Accuracy: 65.78%\n",
      "Epoch [20/100], Loss: 0.6151, Val Loss: 0.6054, Accuracy: 66.21%\n",
      "Epoch [30/100], Loss: 0.6128, Val Loss: 0.6037, Accuracy: 66.36%\n",
      "Epoch [40/100], Loss: 0.6128, Val Loss: 0.6048, Accuracy: 66.21%\n",
      "Epoch [50/100], Loss: 0.6076, Val Loss: 0.6041, Accuracy: 65.96%\n",
      "Epoch [60/100], Loss: 0.6095, Val Loss: 0.6033, Accuracy: 66.43%\n",
      "Epoch [70/100], Loss: 0.6083, Val Loss: 0.6050, Accuracy: 66.46%\n",
      "Epoch [80/100], Loss: 0.6095, Val Loss: 0.6029, Accuracy: 66.71%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.6039, Accuracy: 66.14%\n",
      "Epoch [100/100], Loss: 0.6095, Val Loss: 0.6042, Accuracy: 66.10%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6085, Val Loss: 0.5991, Accuracy: 68.04%\n",
      "Epoch [20/100], Loss: 0.6065, Val Loss: 0.5977, Accuracy: 68.16%\n",
      "Epoch [30/100], Loss: 0.6056, Val Loss: 0.5979, Accuracy: 68.13%\n",
      "Epoch [40/100], Loss: 0.6070, Val Loss: 0.5978, Accuracy: 68.07%\n",
      "Epoch [50/100], Loss: 0.6069, Val Loss: 0.5978, Accuracy: 67.93%\n",
      "Epoch [60/100], Loss: 0.6066, Val Loss: 0.5979, Accuracy: 68.19%\n",
      "Epoch [70/100], Loss: 0.6069, Val Loss: 0.5977, Accuracy: 68.19%\n",
      "Epoch [80/100], Loss: 0.6052, Val Loss: 0.5976, Accuracy: 68.13%\n",
      "Epoch [90/100], Loss: 0.6059, Val Loss: 0.5982, Accuracy: 67.82%\n",
      "Epoch [100/100], Loss: 0.6069, Val Loss: 0.5981, Accuracy: 67.87%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6144, Val Loss: 0.6170, Accuracy: 64.54%\n",
      "Epoch [20/100], Loss: 0.6112, Val Loss: 0.6153, Accuracy: 65.18%\n",
      "Epoch [30/100], Loss: 0.6103, Val Loss: 0.6150, Accuracy: 65.43%\n",
      "Epoch [40/100], Loss: 0.6087, Val Loss: 0.6175, Accuracy: 64.75%\n",
      "Epoch [50/100], Loss: 0.6059, Val Loss: 0.6139, Accuracy: 65.15%\n",
      "Epoch [60/100], Loss: 0.6044, Val Loss: 0.6162, Accuracy: 65.08%\n",
      "Epoch [70/100], Loss: 0.6045, Val Loss: 0.6142, Accuracy: 65.25%\n",
      "Epoch [80/100], Loss: 0.6042, Val Loss: 0.6149, Accuracy: 65.33%\n",
      "Epoch [90/100], Loss: 0.6057, Val Loss: 0.6159, Accuracy: 64.93%\n",
      "Epoch [100/100], Loss: 0.6057, Val Loss: 0.6150, Accuracy: 64.97%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6242, Val Loss: 0.5949, Accuracy: 67.30%\n",
      "Epoch [20/100], Loss: 0.6180, Val Loss: 0.5951, Accuracy: 67.52%\n",
      "Epoch [30/100], Loss: 0.6166, Val Loss: 0.5941, Accuracy: 67.88%\n",
      "Epoch [40/100], Loss: 0.6133, Val Loss: 0.5917, Accuracy: 67.91%\n",
      "Epoch [50/100], Loss: 0.6147, Val Loss: 0.5931, Accuracy: 67.55%\n",
      "Epoch [60/100], Loss: 0.6123, Val Loss: 0.5905, Accuracy: 68.16%\n",
      "Epoch [70/100], Loss: 0.6109, Val Loss: 0.5927, Accuracy: 68.02%\n",
      "Epoch [80/100], Loss: 0.6111, Val Loss: 0.5913, Accuracy: 67.70%\n",
      "Epoch [90/100], Loss: 0.6118, Val Loss: 0.5915, Accuracy: 67.62%\n",
      "Epoch [100/100], Loss: 0.6123, Val Loss: 0.5913, Accuracy: 67.88%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6202, Val Loss: 0.6136, Accuracy: 64.75%\n",
      "Epoch [20/100], Loss: 0.6151, Val Loss: 0.6100, Accuracy: 65.94%\n",
      "Epoch [30/100], Loss: 0.6114, Val Loss: 0.6078, Accuracy: 65.90%\n",
      "Epoch [40/100], Loss: 0.6118, Val Loss: 0.6077, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6089, Val Loss: 0.6081, Accuracy: 65.47%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.6084, Accuracy: 65.47%\n",
      "Epoch [70/100], Loss: 0.6073, Val Loss: 0.6074, Accuracy: 65.76%\n",
      "Epoch [80/100], Loss: 0.6080, Val Loss: 0.6076, Accuracy: 65.33%\n",
      "Epoch [90/100], Loss: 0.6100, Val Loss: 0.6076, Accuracy: 65.87%\n",
      "Epoch [100/100], Loss: 0.6091, Val Loss: 0.6076, Accuracy: 65.47%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6181, Val Loss: 0.6205, Accuracy: 64.70%\n",
      "Epoch [20/100], Loss: 0.6137, Val Loss: 0.6202, Accuracy: 64.85%\n",
      "Epoch [30/100], Loss: 0.6099, Val Loss: 0.6208, Accuracy: 64.78%\n",
      "Epoch [40/100], Loss: 0.6054, Val Loss: 0.6196, Accuracy: 64.60%\n",
      "Epoch [50/100], Loss: 0.6090, Val Loss: 0.6193, Accuracy: 64.99%\n",
      "Epoch [60/100], Loss: 0.6075, Val Loss: 0.6207, Accuracy: 65.06%\n",
      "Epoch [70/100], Loss: 0.6060, Val Loss: 0.6202, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6070, Val Loss: 0.6210, Accuracy: 64.63%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.6191, Accuracy: 64.88%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.6201, Accuracy: 64.34%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6226, Val Loss: 0.6106, Accuracy: 66.36%\n",
      "Epoch [20/100], Loss: 0.6182, Val Loss: 0.6098, Accuracy: 65.78%\n",
      "Epoch [30/100], Loss: 0.6121, Val Loss: 0.6055, Accuracy: 65.85%\n",
      "Epoch [40/100], Loss: 0.6114, Val Loss: 0.6052, Accuracy: 66.07%\n",
      "Epoch [50/100], Loss: 0.6109, Val Loss: 0.6050, Accuracy: 65.96%\n",
      "Epoch [60/100], Loss: 0.6106, Val Loss: 0.6050, Accuracy: 65.57%\n",
      "Epoch [70/100], Loss: 0.6116, Val Loss: 0.6047, Accuracy: 66.07%\n",
      "Epoch [80/100], Loss: 0.6126, Val Loss: 0.6051, Accuracy: 66.07%\n",
      "Epoch [90/100], Loss: 0.6110, Val Loss: 0.6053, Accuracy: 66.21%\n",
      "Epoch [100/100], Loss: 0.6117, Val Loss: 0.6048, Accuracy: 66.21%\n",
      "Best cross-validated accuracy: 68.38%\n",
      "Epoch [10/100], Loss: 0.6072, Val Loss: 0.6001, Accuracy: 67.67%\n",
      "Epoch [20/100], Loss: 0.6068, Val Loss: 0.5976, Accuracy: 68.07%\n",
      "Epoch [30/100], Loss: 0.6057, Val Loss: 0.5977, Accuracy: 67.82%\n",
      "Epoch [40/100], Loss: 0.6062, Val Loss: 0.5972, Accuracy: 67.90%\n",
      "Epoch [50/100], Loss: 0.6037, Val Loss: 0.5973, Accuracy: 68.04%\n",
      "Epoch [60/100], Loss: 0.6051, Val Loss: 0.5972, Accuracy: 68.10%\n",
      "Epoch [70/100], Loss: 0.6040, Val Loss: 0.5975, Accuracy: 67.93%\n",
      "Epoch [80/100], Loss: 0.6037, Val Loss: 0.5974, Accuracy: 68.16%\n",
      "Epoch [90/100], Loss: 0.6044, Val Loss: 0.5978, Accuracy: 67.90%\n",
      "Epoch [100/100], Loss: 0.6045, Val Loss: 0.5974, Accuracy: 67.84%\n",
      "Final model accuracy on test set: 68.25%\n",
      "Accuracy 68.25% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6169, Val Loss: 0.6188, Accuracy: 64.61%\n",
      "Epoch [20/100], Loss: 0.6143, Val Loss: 0.6191, Accuracy: 64.54%\n",
      "Epoch [30/100], Loss: 0.6108, Val Loss: 0.6159, Accuracy: 64.54%\n",
      "Epoch [40/100], Loss: 0.6106, Val Loss: 0.6162, Accuracy: 65.25%\n",
      "Epoch [50/100], Loss: 0.6109, Val Loss: 0.6159, Accuracy: 65.08%\n",
      "Epoch [60/100], Loss: 0.6102, Val Loss: 0.6171, Accuracy: 64.93%\n",
      "Epoch [70/100], Loss: 0.6107, Val Loss: 0.6161, Accuracy: 64.90%\n",
      "Epoch [80/100], Loss: 0.6096, Val Loss: 0.6163, Accuracy: 65.47%\n",
      "Epoch [90/100], Loss: 0.6091, Val Loss: 0.6153, Accuracy: 64.82%\n",
      "Epoch [100/100], Loss: 0.6104, Val Loss: 0.6161, Accuracy: 64.93%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6262, Val Loss: 0.5952, Accuracy: 67.09%\n",
      "Epoch [20/100], Loss: 0.6213, Val Loss: 0.5935, Accuracy: 68.09%\n",
      "Epoch [30/100], Loss: 0.6159, Val Loss: 0.5914, Accuracy: 67.80%\n",
      "Epoch [40/100], Loss: 0.6135, Val Loss: 0.5908, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.6115, Val Loss: 0.5899, Accuracy: 68.13%\n",
      "Epoch [60/100], Loss: 0.6127, Val Loss: 0.5921, Accuracy: 68.27%\n",
      "Epoch [70/100], Loss: 0.6130, Val Loss: 0.5905, Accuracy: 68.02%\n",
      "Epoch [80/100], Loss: 0.6126, Val Loss: 0.5905, Accuracy: 67.98%\n",
      "Epoch [90/100], Loss: 0.6113, Val Loss: 0.5923, Accuracy: 67.37%\n",
      "Epoch [100/100], Loss: 0.6116, Val Loss: 0.5901, Accuracy: 68.27%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6121, Accuracy: 65.58%\n",
      "Epoch [20/100], Loss: 0.6145, Val Loss: 0.6097, Accuracy: 65.22%\n",
      "Epoch [30/100], Loss: 0.6136, Val Loss: 0.6102, Accuracy: 64.97%\n",
      "Epoch [40/100], Loss: 0.6106, Val Loss: 0.6088, Accuracy: 66.08%\n",
      "Epoch [50/100], Loss: 0.6112, Val Loss: 0.6086, Accuracy: 65.61%\n",
      "Epoch [60/100], Loss: 0.6095, Val Loss: 0.6084, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6128, Val Loss: 0.6086, Accuracy: 66.22%\n",
      "Epoch [80/100], Loss: 0.6116, Val Loss: 0.6083, Accuracy: 65.83%\n",
      "Epoch [90/100], Loss: 0.6109, Val Loss: 0.6089, Accuracy: 65.58%\n",
      "Epoch [100/100], Loss: 0.6093, Val Loss: 0.6083, Accuracy: 65.72%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6198, Val Loss: 0.6175, Accuracy: 64.63%\n",
      "Epoch [20/100], Loss: 0.6131, Val Loss: 0.6206, Accuracy: 65.31%\n",
      "Epoch [30/100], Loss: 0.6110, Val Loss: 0.6199, Accuracy: 64.81%\n",
      "Epoch [40/100], Loss: 0.6114, Val Loss: 0.6216, Accuracy: 64.02%\n",
      "Epoch [50/100], Loss: 0.6119, Val Loss: 0.6209, Accuracy: 64.24%\n",
      "Epoch [60/100], Loss: 0.6120, Val Loss: 0.6190, Accuracy: 65.28%\n",
      "Epoch [70/100], Loss: 0.6111, Val Loss: 0.6200, Accuracy: 64.42%\n",
      "Epoch [80/100], Loss: 0.6093, Val Loss: 0.6200, Accuracy: 64.38%\n",
      "Epoch [90/100], Loss: 0.6073, Val Loss: 0.6192, Accuracy: 64.92%\n",
      "Epoch [100/100], Loss: 0.6106, Val Loss: 0.6202, Accuracy: 64.78%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6178, Val Loss: 0.6071, Accuracy: 65.96%\n",
      "Epoch [20/100], Loss: 0.6155, Val Loss: 0.6063, Accuracy: 65.92%\n",
      "Epoch [30/100], Loss: 0.6103, Val Loss: 0.6046, Accuracy: 66.03%\n",
      "Epoch [40/100], Loss: 0.6110, Val Loss: 0.6052, Accuracy: 65.60%\n",
      "Epoch [50/100], Loss: 0.6099, Val Loss: 0.6030, Accuracy: 66.14%\n",
      "Epoch [60/100], Loss: 0.6080, Val Loss: 0.6044, Accuracy: 65.96%\n",
      "Epoch [70/100], Loss: 0.6071, Val Loss: 0.6035, Accuracy: 66.18%\n",
      "Epoch [80/100], Loss: 0.6060, Val Loss: 0.6036, Accuracy: 66.14%\n",
      "Epoch [90/100], Loss: 0.6051, Val Loss: 0.6036, Accuracy: 66.03%\n",
      "Epoch [100/100], Loss: 0.6076, Val Loss: 0.6038, Accuracy: 65.96%\n",
      "Best cross-validated accuracy: 68.45%\n",
      "Epoch [10/100], Loss: 0.6103, Val Loss: 0.5985, Accuracy: 68.04%\n",
      "Epoch [20/100], Loss: 0.6086, Val Loss: 0.5987, Accuracy: 68.13%\n",
      "Epoch [30/100], Loss: 0.6078, Val Loss: 0.5987, Accuracy: 68.19%\n",
      "Epoch [40/100], Loss: 0.6090, Val Loss: 0.5981, Accuracy: 68.07%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.5982, Accuracy: 68.22%\n",
      "Epoch [60/100], Loss: 0.6056, Val Loss: 0.5983, Accuracy: 68.04%\n",
      "Epoch [70/100], Loss: 0.6080, Val Loss: 0.5982, Accuracy: 68.16%\n",
      "Epoch [80/100], Loss: 0.6085, Val Loss: 0.5981, Accuracy: 68.16%\n",
      "Epoch [90/100], Loss: 0.6074, Val Loss: 0.5981, Accuracy: 68.10%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.5984, Accuracy: 68.16%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6168, Val Loss: 0.6209, Accuracy: 64.72%\n",
      "Epoch [20/100], Loss: 0.6141, Val Loss: 0.6146, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6100, Val Loss: 0.6159, Accuracy: 64.75%\n",
      "Epoch [40/100], Loss: 0.6083, Val Loss: 0.6157, Accuracy: 64.64%\n",
      "Epoch [50/100], Loss: 0.6079, Val Loss: 0.6157, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6099, Val Loss: 0.6155, Accuracy: 65.08%\n",
      "Epoch [70/100], Loss: 0.6089, Val Loss: 0.6151, Accuracy: 65.33%\n",
      "Epoch [80/100], Loss: 0.6081, Val Loss: 0.6150, Accuracy: 65.18%\n",
      "Epoch [90/100], Loss: 0.6099, Val Loss: 0.6155, Accuracy: 64.75%\n",
      "Epoch [100/100], Loss: 0.6098, Val Loss: 0.6154, Accuracy: 65.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6231, Val Loss: 0.5949, Accuracy: 67.52%\n",
      "Epoch [20/100], Loss: 0.6211, Val Loss: 0.5941, Accuracy: 68.09%\n",
      "Epoch [30/100], Loss: 0.6153, Val Loss: 0.5911, Accuracy: 67.77%\n",
      "Epoch [40/100], Loss: 0.6156, Val Loss: 0.5924, Accuracy: 68.13%\n",
      "Epoch [50/100], Loss: 0.6138, Val Loss: 0.5909, Accuracy: 67.95%\n",
      "Epoch [60/100], Loss: 0.6104, Val Loss: 0.5909, Accuracy: 67.59%\n",
      "Epoch [70/100], Loss: 0.6103, Val Loss: 0.5904, Accuracy: 67.98%\n",
      "Epoch [80/100], Loss: 0.6105, Val Loss: 0.5908, Accuracy: 67.84%\n",
      "Epoch [90/100], Loss: 0.6111, Val Loss: 0.5915, Accuracy: 67.80%\n",
      "Epoch [100/100], Loss: 0.6102, Val Loss: 0.5909, Accuracy: 67.77%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6120, Accuracy: 65.08%\n",
      "Epoch [20/100], Loss: 0.6164, Val Loss: 0.6110, Accuracy: 65.00%\n",
      "Epoch [30/100], Loss: 0.6097, Val Loss: 0.6094, Accuracy: 65.33%\n",
      "Epoch [40/100], Loss: 0.6104, Val Loss: 0.6080, Accuracy: 65.97%\n",
      "Epoch [50/100], Loss: 0.6085, Val Loss: 0.6077, Accuracy: 65.58%\n",
      "Epoch [60/100], Loss: 0.6078, Val Loss: 0.6078, Accuracy: 65.87%\n",
      "Epoch [70/100], Loss: 0.6075, Val Loss: 0.6076, Accuracy: 65.69%\n",
      "Epoch [80/100], Loss: 0.6084, Val Loss: 0.6076, Accuracy: 65.83%\n",
      "Epoch [90/100], Loss: 0.6084, Val Loss: 0.6076, Accuracy: 65.87%\n",
      "Epoch [100/100], Loss: 0.6070, Val Loss: 0.6074, Accuracy: 65.79%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6188, Val Loss: 0.6182, Accuracy: 65.17%\n",
      "Epoch [20/100], Loss: 0.6133, Val Loss: 0.6183, Accuracy: 64.70%\n",
      "Epoch [30/100], Loss: 0.6099, Val Loss: 0.6185, Accuracy: 65.21%\n",
      "Epoch [40/100], Loss: 0.6085, Val Loss: 0.6193, Accuracy: 65.10%\n",
      "Epoch [50/100], Loss: 0.6092, Val Loss: 0.6194, Accuracy: 64.96%\n",
      "Epoch [60/100], Loss: 0.6047, Val Loss: 0.6186, Accuracy: 65.06%\n",
      "Epoch [70/100], Loss: 0.6088, Val Loss: 0.6189, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6067, Val Loss: 0.6183, Accuracy: 64.67%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.6184, Accuracy: 64.96%\n",
      "Epoch [100/100], Loss: 0.6101, Val Loss: 0.6187, Accuracy: 64.67%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6214, Val Loss: 0.6072, Accuracy: 66.57%\n",
      "Epoch [20/100], Loss: 0.6160, Val Loss: 0.6050, Accuracy: 65.89%\n",
      "Epoch [30/100], Loss: 0.6145, Val Loss: 0.6037, Accuracy: 66.75%\n",
      "Epoch [40/100], Loss: 0.6137, Val Loss: 0.6039, Accuracy: 66.50%\n",
      "Epoch [50/100], Loss: 0.6118, Val Loss: 0.6050, Accuracy: 66.14%\n",
      "Epoch [60/100], Loss: 0.6122, Val Loss: 0.6036, Accuracy: 66.61%\n",
      "Epoch [70/100], Loss: 0.6125, Val Loss: 0.6039, Accuracy: 66.32%\n",
      "Epoch [80/100], Loss: 0.6112, Val Loss: 0.6055, Accuracy: 66.71%\n",
      "Epoch [90/100], Loss: 0.6105, Val Loss: 0.6038, Accuracy: 66.46%\n",
      "Epoch [100/100], Loss: 0.6116, Val Loss: 0.6033, Accuracy: 66.57%\n",
      "Best cross-validated accuracy: 68.27%\n",
      "Epoch [10/100], Loss: 0.6102, Val Loss: 0.5982, Accuracy: 68.30%\n",
      "Epoch [20/100], Loss: 0.6085, Val Loss: 0.5978, Accuracy: 68.39%\n",
      "Epoch [30/100], Loss: 0.6084, Val Loss: 0.5972, Accuracy: 68.13%\n",
      "Epoch [40/100], Loss: 0.6060, Val Loss: 0.5972, Accuracy: 68.36%\n",
      "Epoch [50/100], Loss: 0.6057, Val Loss: 0.5971, Accuracy: 67.90%\n",
      "Epoch [60/100], Loss: 0.6070, Val Loss: 0.5971, Accuracy: 67.93%\n",
      "Epoch [70/100], Loss: 0.6061, Val Loss: 0.5974, Accuracy: 68.27%\n",
      "Epoch [80/100], Loss: 0.6064, Val Loss: 0.5971, Accuracy: 68.36%\n",
      "Epoch [90/100], Loss: 0.6073, Val Loss: 0.5973, Accuracy: 68.04%\n",
      "Epoch [100/100], Loss: 0.6082, Val Loss: 0.5972, Accuracy: 68.22%\n",
      "Final model accuracy on test set: 68.48%\n",
      "Accuracy 68.48% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6203, Val Loss: 0.6182, Accuracy: 65.08%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6174, Accuracy: 64.68%\n",
      "Epoch [30/100], Loss: 0.6095, Val Loss: 0.6157, Accuracy: 65.11%\n",
      "Epoch [40/100], Loss: 0.6094, Val Loss: 0.6174, Accuracy: 65.08%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.6157, Accuracy: 65.11%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6160, Accuracy: 65.11%\n",
      "Epoch [70/100], Loss: 0.6078, Val Loss: 0.6163, Accuracy: 65.15%\n",
      "Epoch [80/100], Loss: 0.6100, Val Loss: 0.6172, Accuracy: 65.08%\n",
      "Epoch [90/100], Loss: 0.6088, Val Loss: 0.6177, Accuracy: 64.82%\n",
      "Epoch [100/100], Loss: 0.6082, Val Loss: 0.6166, Accuracy: 65.11%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6220, Val Loss: 0.5959, Accuracy: 66.91%\n",
      "Epoch [20/100], Loss: 0.6186, Val Loss: 0.5924, Accuracy: 67.80%\n",
      "Epoch [30/100], Loss: 0.6163, Val Loss: 0.5907, Accuracy: 67.84%\n",
      "Epoch [40/100], Loss: 0.6158, Val Loss: 0.5944, Accuracy: 67.59%\n",
      "Epoch [50/100], Loss: 0.6124, Val Loss: 0.5909, Accuracy: 67.52%\n",
      "Epoch [60/100], Loss: 0.6150, Val Loss: 0.5919, Accuracy: 68.16%\n",
      "Epoch [70/100], Loss: 0.6124, Val Loss: 0.5928, Accuracy: 67.77%\n",
      "Epoch [80/100], Loss: 0.6112, Val Loss: 0.5930, Accuracy: 68.02%\n",
      "Epoch [90/100], Loss: 0.6136, Val Loss: 0.5910, Accuracy: 68.09%\n",
      "Epoch [100/100], Loss: 0.6116, Val Loss: 0.5925, Accuracy: 67.91%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6216, Val Loss: 0.6134, Accuracy: 65.08%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6102, Accuracy: 65.54%\n",
      "Epoch [30/100], Loss: 0.6119, Val Loss: 0.6088, Accuracy: 65.43%\n",
      "Epoch [40/100], Loss: 0.6100, Val Loss: 0.6080, Accuracy: 65.43%\n",
      "Epoch [50/100], Loss: 0.6083, Val Loss: 0.6084, Accuracy: 65.65%\n",
      "Epoch [60/100], Loss: 0.6083, Val Loss: 0.6085, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6098, Val Loss: 0.6084, Accuracy: 65.54%\n",
      "Epoch [80/100], Loss: 0.6091, Val Loss: 0.6080, Accuracy: 65.43%\n",
      "Epoch [90/100], Loss: 0.6086, Val Loss: 0.6081, Accuracy: 65.33%\n",
      "Epoch [100/100], Loss: 0.6087, Val Loss: 0.6084, Accuracy: 65.47%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6163, Val Loss: 0.6193, Accuracy: 65.21%\n",
      "Epoch [20/100], Loss: 0.6100, Val Loss: 0.6194, Accuracy: 64.49%\n",
      "Epoch [30/100], Loss: 0.6099, Val Loss: 0.6188, Accuracy: 64.63%\n",
      "Epoch [40/100], Loss: 0.6088, Val Loss: 0.6182, Accuracy: 64.60%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6182, Accuracy: 64.67%\n",
      "Epoch [60/100], Loss: 0.6101, Val Loss: 0.6185, Accuracy: 64.78%\n",
      "Epoch [70/100], Loss: 0.6102, Val Loss: 0.6182, Accuracy: 64.70%\n",
      "Epoch [80/100], Loss: 0.6102, Val Loss: 0.6189, Accuracy: 64.74%\n",
      "Epoch [90/100], Loss: 0.6087, Val Loss: 0.6186, Accuracy: 64.52%\n",
      "Epoch [100/100], Loss: 0.6105, Val Loss: 0.6182, Accuracy: 64.60%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6084, Accuracy: 65.71%\n",
      "Epoch [20/100], Loss: 0.6141, Val Loss: 0.6063, Accuracy: 66.36%\n",
      "Epoch [30/100], Loss: 0.6153, Val Loss: 0.6056, Accuracy: 66.25%\n",
      "Epoch [40/100], Loss: 0.6106, Val Loss: 0.6047, Accuracy: 66.43%\n",
      "Epoch [50/100], Loss: 0.6109, Val Loss: 0.6044, Accuracy: 66.32%\n",
      "Epoch [60/100], Loss: 0.6095, Val Loss: 0.6039, Accuracy: 66.57%\n",
      "Epoch [70/100], Loss: 0.6087, Val Loss: 0.6042, Accuracy: 66.36%\n",
      "Epoch [80/100], Loss: 0.6102, Val Loss: 0.6049, Accuracy: 66.25%\n",
      "Epoch [90/100], Loss: 0.6082, Val Loss: 0.6040, Accuracy: 66.25%\n",
      "Epoch [100/100], Loss: 0.6095, Val Loss: 0.6037, Accuracy: 66.39%\n",
      "Best cross-validated accuracy: 68.20%\n",
      "Epoch [10/100], Loss: 0.6087, Val Loss: 0.5979, Accuracy: 68.42%\n",
      "Epoch [20/100], Loss: 0.6081, Val Loss: 0.5976, Accuracy: 68.04%\n",
      "Epoch [30/100], Loss: 0.6062, Val Loss: 0.5975, Accuracy: 68.25%\n",
      "Epoch [40/100], Loss: 0.6058, Val Loss: 0.5974, Accuracy: 68.04%\n",
      "Epoch [50/100], Loss: 0.6051, Val Loss: 0.5975, Accuracy: 68.10%\n",
      "Epoch [60/100], Loss: 0.6070, Val Loss: 0.5977, Accuracy: 68.16%\n",
      "Epoch [70/100], Loss: 0.6057, Val Loss: 0.5975, Accuracy: 68.04%\n",
      "Epoch [80/100], Loss: 0.6083, Val Loss: 0.5975, Accuracy: 68.07%\n",
      "Epoch [90/100], Loss: 0.6067, Val Loss: 0.5977, Accuracy: 68.02%\n",
      "Epoch [100/100], Loss: 0.6068, Val Loss: 0.5979, Accuracy: 68.13%\n",
      "Final model accuracy on test set: 68.45%\n",
      "Accuracy 68.45% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6187, Val Loss: 0.6237, Accuracy: 63.78%\n",
      "Epoch [20/100], Loss: 0.6150, Val Loss: 0.6179, Accuracy: 64.47%\n",
      "Epoch [30/100], Loss: 0.6111, Val Loss: 0.6165, Accuracy: 65.22%\n",
      "Epoch [40/100], Loss: 0.6085, Val Loss: 0.6157, Accuracy: 65.18%\n",
      "Epoch [50/100], Loss: 0.6080, Val Loss: 0.6149, Accuracy: 65.29%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6149, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6059, Val Loss: 0.6152, Accuracy: 65.25%\n",
      "Epoch [80/100], Loss: 0.6060, Val Loss: 0.6144, Accuracy: 65.22%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6148, Accuracy: 65.18%\n",
      "Epoch [100/100], Loss: 0.6056, Val Loss: 0.6147, Accuracy: 65.22%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6249, Val Loss: 0.5961, Accuracy: 66.87%\n",
      "Epoch [20/100], Loss: 0.6183, Val Loss: 0.5960, Accuracy: 67.34%\n",
      "Epoch [30/100], Loss: 0.6164, Val Loss: 0.5940, Accuracy: 67.62%\n",
      "Epoch [40/100], Loss: 0.6141, Val Loss: 0.5917, Accuracy: 67.62%\n",
      "Epoch [50/100], Loss: 0.6148, Val Loss: 0.5946, Accuracy: 67.59%\n",
      "Epoch [60/100], Loss: 0.6115, Val Loss: 0.5934, Accuracy: 67.52%\n",
      "Epoch [70/100], Loss: 0.6109, Val Loss: 0.5925, Accuracy: 67.37%\n",
      "Epoch [80/100], Loss: 0.6103, Val Loss: 0.5912, Accuracy: 67.66%\n",
      "Epoch [90/100], Loss: 0.6125, Val Loss: 0.5915, Accuracy: 67.41%\n",
      "Epoch [100/100], Loss: 0.6112, Val Loss: 0.5908, Accuracy: 67.44%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6193, Val Loss: 0.6121, Accuracy: 64.72%\n",
      "Epoch [20/100], Loss: 0.6118, Val Loss: 0.6099, Accuracy: 65.79%\n",
      "Epoch [30/100], Loss: 0.6123, Val Loss: 0.6095, Accuracy: 65.33%\n",
      "Epoch [40/100], Loss: 0.6094, Val Loss: 0.6081, Accuracy: 65.94%\n",
      "Epoch [50/100], Loss: 0.6090, Val Loss: 0.6087, Accuracy: 65.43%\n",
      "Epoch [60/100], Loss: 0.6058, Val Loss: 0.6080, Accuracy: 65.76%\n",
      "Epoch [70/100], Loss: 0.6073, Val Loss: 0.6087, Accuracy: 65.65%\n",
      "Epoch [80/100], Loss: 0.6056, Val Loss: 0.6079, Accuracy: 65.61%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6085, Accuracy: 65.51%\n",
      "Epoch [100/100], Loss: 0.6071, Val Loss: 0.6082, Accuracy: 65.83%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6184, Val Loss: 0.6198, Accuracy: 65.24%\n",
      "Epoch [20/100], Loss: 0.6120, Val Loss: 0.6196, Accuracy: 64.85%\n",
      "Epoch [30/100], Loss: 0.6079, Val Loss: 0.6197, Accuracy: 64.63%\n",
      "Epoch [40/100], Loss: 0.6060, Val Loss: 0.6199, Accuracy: 64.24%\n",
      "Epoch [50/100], Loss: 0.6063, Val Loss: 0.6187, Accuracy: 64.52%\n",
      "Epoch [60/100], Loss: 0.6080, Val Loss: 0.6194, Accuracy: 64.70%\n",
      "Epoch [70/100], Loss: 0.6078, Val Loss: 0.6196, Accuracy: 64.85%\n",
      "Epoch [80/100], Loss: 0.6079, Val Loss: 0.6188, Accuracy: 64.99%\n",
      "Epoch [90/100], Loss: 0.6097, Val Loss: 0.6191, Accuracy: 65.24%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6194, Accuracy: 64.67%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6196, Val Loss: 0.6128, Accuracy: 65.64%\n",
      "Epoch [20/100], Loss: 0.6158, Val Loss: 0.6051, Accuracy: 66.54%\n",
      "Epoch [30/100], Loss: 0.6112, Val Loss: 0.6049, Accuracy: 65.82%\n",
      "Epoch [40/100], Loss: 0.6123, Val Loss: 0.6045, Accuracy: 65.96%\n",
      "Epoch [50/100], Loss: 0.6095, Val Loss: 0.6041, Accuracy: 66.97%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6025, Accuracy: 66.46%\n",
      "Epoch [70/100], Loss: 0.6079, Val Loss: 0.6029, Accuracy: 67.15%\n",
      "Epoch [80/100], Loss: 0.6071, Val Loss: 0.6024, Accuracy: 66.36%\n",
      "Epoch [90/100], Loss: 0.6065, Val Loss: 0.6029, Accuracy: 67.04%\n",
      "Epoch [100/100], Loss: 0.6065, Val Loss: 0.6027, Accuracy: 66.97%\n",
      "Best cross-validated accuracy: 68.05%\n",
      "Epoch [10/100], Loss: 0.6088, Val Loss: 0.5994, Accuracy: 68.36%\n",
      "Epoch [20/100], Loss: 0.6064, Val Loss: 0.5976, Accuracy: 68.27%\n",
      "Epoch [30/100], Loss: 0.6069, Val Loss: 0.5973, Accuracy: 67.96%\n",
      "Epoch [40/100], Loss: 0.6063, Val Loss: 0.5982, Accuracy: 68.42%\n",
      "Epoch [50/100], Loss: 0.6066, Val Loss: 0.5977, Accuracy: 68.02%\n",
      "Epoch [60/100], Loss: 0.6068, Val Loss: 0.5976, Accuracy: 68.22%\n",
      "Epoch [70/100], Loss: 0.6056, Val Loss: 0.5976, Accuracy: 68.07%\n",
      "Epoch [80/100], Loss: 0.6068, Val Loss: 0.5977, Accuracy: 68.25%\n",
      "Epoch [90/100], Loss: 0.6069, Val Loss: 0.5977, Accuracy: 68.27%\n",
      "Epoch [100/100], Loss: 0.6068, Val Loss: 0.5977, Accuracy: 68.27%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6159, Val Loss: 0.6172, Accuracy: 64.68%\n",
      "Epoch [20/100], Loss: 0.6128, Val Loss: 0.6146, Accuracy: 64.93%\n",
      "Epoch [30/100], Loss: 0.6082, Val Loss: 0.6163, Accuracy: 65.04%\n",
      "Epoch [40/100], Loss: 0.6075, Val Loss: 0.6160, Accuracy: 64.61%\n",
      "Epoch [50/100], Loss: 0.6077, Val Loss: 0.6158, Accuracy: 64.47%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6155, Accuracy: 64.61%\n",
      "Epoch [70/100], Loss: 0.6077, Val Loss: 0.6155, Accuracy: 64.47%\n",
      "Epoch [80/100], Loss: 0.6114, Val Loss: 0.6161, Accuracy: 64.97%\n",
      "Epoch [90/100], Loss: 0.6114, Val Loss: 0.6157, Accuracy: 65.08%\n",
      "Epoch [100/100], Loss: 0.6088, Val Loss: 0.6159, Accuracy: 64.79%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6263, Val Loss: 0.5980, Accuracy: 67.30%\n",
      "Epoch [20/100], Loss: 0.6199, Val Loss: 0.5926, Accuracy: 67.30%\n",
      "Epoch [30/100], Loss: 0.6161, Val Loss: 0.5925, Accuracy: 68.02%\n",
      "Epoch [40/100], Loss: 0.6119, Val Loss: 0.5897, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6139, Val Loss: 0.5913, Accuracy: 67.88%\n",
      "Epoch [60/100], Loss: 0.6122, Val Loss: 0.5910, Accuracy: 67.77%\n",
      "Epoch [70/100], Loss: 0.6105, Val Loss: 0.5916, Accuracy: 67.88%\n",
      "Epoch [80/100], Loss: 0.6110, Val Loss: 0.5921, Accuracy: 68.20%\n",
      "Epoch [90/100], Loss: 0.6118, Val Loss: 0.5907, Accuracy: 67.84%\n",
      "Epoch [100/100], Loss: 0.6094, Val Loss: 0.5905, Accuracy: 68.05%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6114, Accuracy: 65.25%\n",
      "Epoch [20/100], Loss: 0.6127, Val Loss: 0.6095, Accuracy: 65.36%\n",
      "Epoch [30/100], Loss: 0.6125, Val Loss: 0.6095, Accuracy: 65.36%\n",
      "Epoch [40/100], Loss: 0.6112, Val Loss: 0.6094, Accuracy: 65.58%\n",
      "Epoch [50/100], Loss: 0.6095, Val Loss: 0.6079, Accuracy: 65.54%\n",
      "Epoch [60/100], Loss: 0.6078, Val Loss: 0.6074, Accuracy: 65.47%\n",
      "Epoch [70/100], Loss: 0.6051, Val Loss: 0.6070, Accuracy: 65.69%\n",
      "Epoch [80/100], Loss: 0.6075, Val Loss: 0.6067, Accuracy: 65.83%\n",
      "Epoch [90/100], Loss: 0.6060, Val Loss: 0.6074, Accuracy: 65.76%\n",
      "Epoch [100/100], Loss: 0.6049, Val Loss: 0.6077, Accuracy: 65.61%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6188, Val Loss: 0.6200, Accuracy: 64.96%\n",
      "Epoch [20/100], Loss: 0.6114, Val Loss: 0.6208, Accuracy: 64.52%\n",
      "Epoch [30/100], Loss: 0.6062, Val Loss: 0.6206, Accuracy: 64.78%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6209, Accuracy: 64.67%\n",
      "Epoch [50/100], Loss: 0.6109, Val Loss: 0.6202, Accuracy: 65.31%\n",
      "Epoch [60/100], Loss: 0.6112, Val Loss: 0.6194, Accuracy: 65.42%\n",
      "Epoch [70/100], Loss: 0.6103, Val Loss: 0.6194, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6090, Val Loss: 0.6199, Accuracy: 64.99%\n",
      "Epoch [90/100], Loss: 0.6109, Val Loss: 0.6204, Accuracy: 64.78%\n",
      "Epoch [100/100], Loss: 0.6100, Val Loss: 0.6208, Accuracy: 65.28%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6077, Accuracy: 65.71%\n",
      "Epoch [20/100], Loss: 0.6157, Val Loss: 0.6072, Accuracy: 66.10%\n",
      "Epoch [30/100], Loss: 0.6150, Val Loss: 0.6056, Accuracy: 65.71%\n",
      "Epoch [40/100], Loss: 0.6112, Val Loss: 0.6051, Accuracy: 66.39%\n",
      "Epoch [50/100], Loss: 0.6092, Val Loss: 0.6044, Accuracy: 66.39%\n",
      "Epoch [60/100], Loss: 0.6085, Val Loss: 0.6050, Accuracy: 66.50%\n",
      "Epoch [70/100], Loss: 0.6098, Val Loss: 0.6041, Accuracy: 66.39%\n",
      "Epoch [80/100], Loss: 0.6094, Val Loss: 0.6046, Accuracy: 66.21%\n",
      "Epoch [90/100], Loss: 0.6084, Val Loss: 0.6053, Accuracy: 66.10%\n",
      "Epoch [100/100], Loss: 0.6088, Val Loss: 0.6049, Accuracy: 66.07%\n",
      "Best cross-validated accuracy: 68.41%\n",
      "Epoch [10/100], Loss: 0.6095, Val Loss: 0.5993, Accuracy: 68.02%\n",
      "Epoch [20/100], Loss: 0.6083, Val Loss: 0.5990, Accuracy: 68.19%\n",
      "Epoch [30/100], Loss: 0.6039, Val Loss: 0.5983, Accuracy: 67.79%\n",
      "Epoch [40/100], Loss: 0.6059, Val Loss: 0.5983, Accuracy: 67.76%\n",
      "Epoch [50/100], Loss: 0.6060, Val Loss: 0.5981, Accuracy: 67.79%\n",
      "Epoch [60/100], Loss: 0.6072, Val Loss: 0.5980, Accuracy: 67.82%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.5981, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6055, Val Loss: 0.5988, Accuracy: 67.82%\n",
      "Epoch [90/100], Loss: 0.6067, Val Loss: 0.5984, Accuracy: 67.84%\n",
      "Epoch [100/100], Loss: 0.6067, Val Loss: 0.5986, Accuracy: 67.73%\n",
      "Final model accuracy on test set: 68.39%\n",
      "Accuracy 68.39% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6207, Val Loss: 0.6190, Accuracy: 64.93%\n",
      "Epoch [20/100], Loss: 0.6135, Val Loss: 0.6168, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6114, Val Loss: 0.6146, Accuracy: 65.40%\n",
      "Epoch [40/100], Loss: 0.6066, Val Loss: 0.6142, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6078, Val Loss: 0.6159, Accuracy: 64.86%\n",
      "Epoch [60/100], Loss: 0.6075, Val Loss: 0.6141, Accuracy: 65.72%\n",
      "Epoch [70/100], Loss: 0.6069, Val Loss: 0.6153, Accuracy: 65.22%\n",
      "Epoch [80/100], Loss: 0.6057, Val Loss: 0.6142, Accuracy: 65.87%\n",
      "Epoch [90/100], Loss: 0.6070, Val Loss: 0.6138, Accuracy: 65.51%\n",
      "Epoch [100/100], Loss: 0.6066, Val Loss: 0.6138, Accuracy: 65.33%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6221, Val Loss: 0.5942, Accuracy: 67.66%\n",
      "Epoch [20/100], Loss: 0.6170, Val Loss: 0.5919, Accuracy: 68.31%\n",
      "Epoch [30/100], Loss: 0.6137, Val Loss: 0.5915, Accuracy: 67.80%\n",
      "Epoch [40/100], Loss: 0.6134, Val Loss: 0.5923, Accuracy: 68.13%\n",
      "Epoch [50/100], Loss: 0.6115, Val Loss: 0.5900, Accuracy: 67.91%\n",
      "Epoch [60/100], Loss: 0.6105, Val Loss: 0.5904, Accuracy: 67.80%\n",
      "Epoch [70/100], Loss: 0.6116, Val Loss: 0.5912, Accuracy: 68.49%\n",
      "Epoch [80/100], Loss: 0.6118, Val Loss: 0.5920, Accuracy: 68.05%\n",
      "Epoch [90/100], Loss: 0.6117, Val Loss: 0.5910, Accuracy: 67.91%\n",
      "Epoch [100/100], Loss: 0.6128, Val Loss: 0.5926, Accuracy: 67.66%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6206, Val Loss: 0.6126, Accuracy: 64.68%\n",
      "Epoch [20/100], Loss: 0.6160, Val Loss: 0.6092, Accuracy: 65.94%\n",
      "Epoch [30/100], Loss: 0.6122, Val Loss: 0.6085, Accuracy: 65.69%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6084, Accuracy: 65.76%\n",
      "Epoch [50/100], Loss: 0.6115, Val Loss: 0.6081, Accuracy: 65.65%\n",
      "Epoch [60/100], Loss: 0.6112, Val Loss: 0.6082, Accuracy: 65.51%\n",
      "Epoch [70/100], Loss: 0.6097, Val Loss: 0.6083, Accuracy: 65.33%\n",
      "Epoch [80/100], Loss: 0.6103, Val Loss: 0.6082, Accuracy: 65.72%\n",
      "Epoch [90/100], Loss: 0.6101, Val Loss: 0.6084, Accuracy: 65.54%\n",
      "Epoch [100/100], Loss: 0.6097, Val Loss: 0.6078, Accuracy: 65.79%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6163, Val Loss: 0.6199, Accuracy: 64.92%\n",
      "Epoch [20/100], Loss: 0.6116, Val Loss: 0.6193, Accuracy: 64.78%\n",
      "Epoch [30/100], Loss: 0.6092, Val Loss: 0.6195, Accuracy: 64.34%\n",
      "Epoch [40/100], Loss: 0.6101, Val Loss: 0.6198, Accuracy: 64.56%\n",
      "Epoch [50/100], Loss: 0.6114, Val Loss: 0.6191, Accuracy: 64.81%\n",
      "Epoch [60/100], Loss: 0.6123, Val Loss: 0.6205, Accuracy: 64.92%\n",
      "Epoch [70/100], Loss: 0.6116, Val Loss: 0.6198, Accuracy: 64.81%\n",
      "Epoch [80/100], Loss: 0.6114, Val Loss: 0.6192, Accuracy: 65.03%\n",
      "Epoch [90/100], Loss: 0.6112, Val Loss: 0.6193, Accuracy: 64.99%\n",
      "Epoch [100/100], Loss: 0.6122, Val Loss: 0.6211, Accuracy: 64.34%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6262, Val Loss: 0.6082, Accuracy: 65.89%\n",
      "Epoch [20/100], Loss: 0.6146, Val Loss: 0.6053, Accuracy: 65.85%\n",
      "Epoch [30/100], Loss: 0.6122, Val Loss: 0.6058, Accuracy: 65.82%\n",
      "Epoch [40/100], Loss: 0.6115, Val Loss: 0.6040, Accuracy: 65.85%\n",
      "Epoch [50/100], Loss: 0.6089, Val Loss: 0.6042, Accuracy: 65.64%\n",
      "Epoch [60/100], Loss: 0.6097, Val Loss: 0.6042, Accuracy: 66.28%\n",
      "Epoch [70/100], Loss: 0.6068, Val Loss: 0.6037, Accuracy: 65.78%\n",
      "Epoch [80/100], Loss: 0.6080, Val Loss: 0.6039, Accuracy: 66.39%\n",
      "Epoch [90/100], Loss: 0.6080, Val Loss: 0.6037, Accuracy: 66.32%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.6038, Accuracy: 66.03%\n",
      "Best cross-validated accuracy: 68.49%\n",
      "Epoch [10/100], Loss: 0.6093, Val Loss: 0.5977, Accuracy: 68.16%\n",
      "Epoch [20/100], Loss: 0.6064, Val Loss: 0.5972, Accuracy: 67.96%\n",
      "Epoch [30/100], Loss: 0.6063, Val Loss: 0.5975, Accuracy: 67.82%\n",
      "Epoch [40/100], Loss: 0.6068, Val Loss: 0.5976, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.6068, Val Loss: 0.5979, Accuracy: 67.99%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.5978, Accuracy: 67.90%\n",
      "Epoch [70/100], Loss: 0.6067, Val Loss: 0.5977, Accuracy: 68.02%\n",
      "Epoch [80/100], Loss: 0.6065, Val Loss: 0.5976, Accuracy: 67.76%\n",
      "Epoch [90/100], Loss: 0.6065, Val Loss: 0.5979, Accuracy: 68.04%\n",
      "Epoch [100/100], Loss: 0.6057, Val Loss: 0.5975, Accuracy: 67.79%\n",
      "Final model accuracy on test set: 68.50%\n",
      "Accuracy 68.50% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6189, Val Loss: 0.6172, Accuracy: 64.64%\n",
      "Epoch [20/100], Loss: 0.6148, Val Loss: 0.6143, Accuracy: 65.22%\n",
      "Epoch [30/100], Loss: 0.6116, Val Loss: 0.6138, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6069, Val Loss: 0.6144, Accuracy: 65.51%\n",
      "Epoch [50/100], Loss: 0.6065, Val Loss: 0.6156, Accuracy: 64.97%\n",
      "Epoch [60/100], Loss: 0.6062, Val Loss: 0.6140, Accuracy: 65.00%\n",
      "Epoch [70/100], Loss: 0.6050, Val Loss: 0.6148, Accuracy: 64.97%\n",
      "Epoch [80/100], Loss: 0.6077, Val Loss: 0.6147, Accuracy: 64.75%\n",
      "Epoch [90/100], Loss: 0.6061, Val Loss: 0.6149, Accuracy: 65.11%\n",
      "Epoch [100/100], Loss: 0.6063, Val Loss: 0.6143, Accuracy: 65.08%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6228, Val Loss: 0.5934, Accuracy: 67.98%\n",
      "Epoch [20/100], Loss: 0.6189, Val Loss: 0.5937, Accuracy: 67.91%\n",
      "Epoch [30/100], Loss: 0.6165, Val Loss: 0.5936, Accuracy: 67.09%\n",
      "Epoch [40/100], Loss: 0.6139, Val Loss: 0.5926, Accuracy: 68.02%\n",
      "Epoch [50/100], Loss: 0.6115, Val Loss: 0.5900, Accuracy: 68.49%\n",
      "Epoch [60/100], Loss: 0.6118, Val Loss: 0.5914, Accuracy: 68.31%\n",
      "Epoch [70/100], Loss: 0.6112, Val Loss: 0.5910, Accuracy: 68.23%\n",
      "Epoch [80/100], Loss: 0.6120, Val Loss: 0.5906, Accuracy: 68.02%\n",
      "Epoch [90/100], Loss: 0.6123, Val Loss: 0.5911, Accuracy: 68.49%\n",
      "Epoch [100/100], Loss: 0.6128, Val Loss: 0.5908, Accuracy: 68.27%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6196, Val Loss: 0.6128, Accuracy: 65.29%\n",
      "Epoch [20/100], Loss: 0.6144, Val Loss: 0.6122, Accuracy: 65.00%\n",
      "Epoch [30/100], Loss: 0.6120, Val Loss: 0.6097, Accuracy: 65.87%\n",
      "Epoch [40/100], Loss: 0.6111, Val Loss: 0.6098, Accuracy: 65.36%\n",
      "Epoch [50/100], Loss: 0.6099, Val Loss: 0.6086, Accuracy: 65.72%\n",
      "Epoch [60/100], Loss: 0.6075, Val Loss: 0.6080, Accuracy: 65.47%\n",
      "Epoch [70/100], Loss: 0.6083, Val Loss: 0.6079, Accuracy: 65.65%\n",
      "Epoch [80/100], Loss: 0.6074, Val Loss: 0.6080, Accuracy: 65.51%\n",
      "Epoch [90/100], Loss: 0.6082, Val Loss: 0.6080, Accuracy: 65.61%\n",
      "Epoch [100/100], Loss: 0.6073, Val Loss: 0.6080, Accuracy: 65.51%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6169, Val Loss: 0.6179, Accuracy: 64.88%\n",
      "Epoch [20/100], Loss: 0.6122, Val Loss: 0.6216, Accuracy: 64.42%\n",
      "Epoch [30/100], Loss: 0.6097, Val Loss: 0.6193, Accuracy: 64.88%\n",
      "Epoch [40/100], Loss: 0.6108, Val Loss: 0.6197, Accuracy: 64.67%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.6201, Accuracy: 64.74%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6207, Accuracy: 64.45%\n",
      "Epoch [70/100], Loss: 0.6099, Val Loss: 0.6201, Accuracy: 64.70%\n",
      "Epoch [80/100], Loss: 0.6080, Val Loss: 0.6199, Accuracy: 64.60%\n",
      "Epoch [90/100], Loss: 0.6100, Val Loss: 0.6198, Accuracy: 64.56%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.6194, Accuracy: 64.70%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6214, Val Loss: 0.6090, Accuracy: 65.71%\n",
      "Epoch [20/100], Loss: 0.6169, Val Loss: 0.6067, Accuracy: 65.78%\n",
      "Epoch [30/100], Loss: 0.6118, Val Loss: 0.6058, Accuracy: 65.67%\n",
      "Epoch [40/100], Loss: 0.6109, Val Loss: 0.6052, Accuracy: 66.10%\n",
      "Epoch [50/100], Loss: 0.6108, Val Loss: 0.6041, Accuracy: 66.28%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6038, Accuracy: 66.00%\n",
      "Epoch [70/100], Loss: 0.6086, Val Loss: 0.6042, Accuracy: 66.03%\n",
      "Epoch [80/100], Loss: 0.6099, Val Loss: 0.6048, Accuracy: 66.00%\n",
      "Epoch [90/100], Loss: 0.6081, Val Loss: 0.6046, Accuracy: 65.89%\n",
      "Epoch [100/100], Loss: 0.6095, Val Loss: 0.6039, Accuracy: 65.96%\n",
      "Best cross-validated accuracy: 68.66%\n",
      "Epoch [10/100], Loss: 0.6073, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [20/100], Loss: 0.6075, Val Loss: 0.5970, Accuracy: 68.30%\n",
      "Epoch [30/100], Loss: 0.6046, Val Loss: 0.5973, Accuracy: 68.16%\n",
      "Epoch [40/100], Loss: 0.6062, Val Loss: 0.5974, Accuracy: 68.25%\n",
      "Epoch [50/100], Loss: 0.6035, Val Loss: 0.5974, Accuracy: 68.27%\n",
      "Epoch [60/100], Loss: 0.6052, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [70/100], Loss: 0.6057, Val Loss: 0.5977, Accuracy: 68.16%\n",
      "Epoch [80/100], Loss: 0.6057, Val Loss: 0.5973, Accuracy: 67.96%\n",
      "Epoch [90/100], Loss: 0.6055, Val Loss: 0.5971, Accuracy: 68.02%\n",
      "Epoch [100/100], Loss: 0.6051, Val Loss: 0.5973, Accuracy: 68.27%\n",
      "Final model accuracy on test set: 68.59%\n",
      "Accuracy 68.59% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6224, Val Loss: 0.6215, Accuracy: 64.39%\n",
      "Epoch [20/100], Loss: 0.6166, Val Loss: 0.6172, Accuracy: 64.72%\n",
      "Epoch [30/100], Loss: 0.6126, Val Loss: 0.6152, Accuracy: 65.43%\n",
      "Epoch [40/100], Loss: 0.6096, Val Loss: 0.6157, Accuracy: 64.93%\n",
      "Epoch [50/100], Loss: 0.6115, Val Loss: 0.6164, Accuracy: 64.79%\n",
      "Epoch [60/100], Loss: 0.6056, Val Loss: 0.6151, Accuracy: 65.15%\n",
      "Epoch [70/100], Loss: 0.6087, Val Loss: 0.6154, Accuracy: 65.04%\n",
      "Epoch [80/100], Loss: 0.6059, Val Loss: 0.6147, Accuracy: 65.18%\n",
      "Epoch [90/100], Loss: 0.6069, Val Loss: 0.6145, Accuracy: 65.00%\n",
      "Epoch [100/100], Loss: 0.6065, Val Loss: 0.6152, Accuracy: 65.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6227, Val Loss: 0.5980, Accuracy: 67.30%\n",
      "Epoch [20/100], Loss: 0.6185, Val Loss: 0.5957, Accuracy: 67.84%\n",
      "Epoch [30/100], Loss: 0.6153, Val Loss: 0.5931, Accuracy: 67.95%\n",
      "Epoch [40/100], Loss: 0.6131, Val Loss: 0.5922, Accuracy: 67.91%\n",
      "Epoch [50/100], Loss: 0.6144, Val Loss: 0.5923, Accuracy: 67.91%\n",
      "Epoch [60/100], Loss: 0.6161, Val Loss: 0.5917, Accuracy: 68.20%\n",
      "Epoch [70/100], Loss: 0.6136, Val Loss: 0.5929, Accuracy: 67.62%\n",
      "Epoch [80/100], Loss: 0.6167, Val Loss: 0.5922, Accuracy: 68.02%\n",
      "Epoch [90/100], Loss: 0.6131, Val Loss: 0.5922, Accuracy: 67.95%\n",
      "Epoch [100/100], Loss: 0.6149, Val Loss: 0.5924, Accuracy: 67.91%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6121, Accuracy: 64.86%\n",
      "Epoch [20/100], Loss: 0.6169, Val Loss: 0.6093, Accuracy: 65.51%\n",
      "Epoch [30/100], Loss: 0.6154, Val Loss: 0.6087, Accuracy: 65.76%\n",
      "Epoch [40/100], Loss: 0.6110, Val Loss: 0.6071, Accuracy: 66.01%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6072, Accuracy: 66.15%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.6080, Accuracy: 65.43%\n",
      "Epoch [70/100], Loss: 0.6099, Val Loss: 0.6075, Accuracy: 65.69%\n",
      "Epoch [80/100], Loss: 0.6083, Val Loss: 0.6075, Accuracy: 65.94%\n",
      "Epoch [90/100], Loss: 0.6061, Val Loss: 0.6073, Accuracy: 65.79%\n",
      "Epoch [100/100], Loss: 0.6085, Val Loss: 0.6078, Accuracy: 65.69%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6174, Val Loss: 0.6214, Accuracy: 65.21%\n",
      "Epoch [20/100], Loss: 0.6102, Val Loss: 0.6209, Accuracy: 64.49%\n",
      "Epoch [30/100], Loss: 0.6101, Val Loss: 0.6206, Accuracy: 64.78%\n",
      "Epoch [40/100], Loss: 0.6078, Val Loss: 0.6205, Accuracy: 64.92%\n",
      "Epoch [50/100], Loss: 0.6081, Val Loss: 0.6221, Accuracy: 64.92%\n",
      "Epoch [60/100], Loss: 0.6056, Val Loss: 0.6206, Accuracy: 65.03%\n",
      "Epoch [70/100], Loss: 0.6081, Val Loss: 0.6195, Accuracy: 64.99%\n",
      "Epoch [80/100], Loss: 0.6082, Val Loss: 0.6208, Accuracy: 64.45%\n",
      "Epoch [90/100], Loss: 0.6097, Val Loss: 0.6202, Accuracy: 64.92%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.6204, Accuracy: 64.88%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6223, Val Loss: 0.6086, Accuracy: 65.71%\n",
      "Epoch [20/100], Loss: 0.6157, Val Loss: 0.6061, Accuracy: 66.14%\n",
      "Epoch [30/100], Loss: 0.6116, Val Loss: 0.6039, Accuracy: 66.43%\n",
      "Epoch [40/100], Loss: 0.6117, Val Loss: 0.6053, Accuracy: 66.25%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6066, Accuracy: 66.39%\n",
      "Epoch [60/100], Loss: 0.6114, Val Loss: 0.6048, Accuracy: 66.57%\n",
      "Epoch [70/100], Loss: 0.6076, Val Loss: 0.6057, Accuracy: 65.71%\n",
      "Epoch [80/100], Loss: 0.6110, Val Loss: 0.6055, Accuracy: 66.64%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.6047, Accuracy: 66.39%\n",
      "Epoch [100/100], Loss: 0.6107, Val Loss: 0.6047, Accuracy: 66.50%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6113, Val Loss: 0.5975, Accuracy: 67.99%\n",
      "Epoch [20/100], Loss: 0.6077, Val Loss: 0.5984, Accuracy: 67.82%\n",
      "Epoch [30/100], Loss: 0.6045, Val Loss: 0.5978, Accuracy: 68.25%\n",
      "Epoch [40/100], Loss: 0.6085, Val Loss: 0.5979, Accuracy: 68.16%\n",
      "Epoch [50/100], Loss: 0.6067, Val Loss: 0.5982, Accuracy: 67.82%\n",
      "Epoch [60/100], Loss: 0.6062, Val Loss: 0.5981, Accuracy: 67.96%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.5981, Accuracy: 68.22%\n",
      "Epoch [80/100], Loss: 0.6066, Val Loss: 0.5983, Accuracy: 68.10%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.5980, Accuracy: 68.25%\n",
      "Epoch [100/100], Loss: 0.6072, Val Loss: 0.5979, Accuracy: 68.10%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6188, Val Loss: 0.6171, Accuracy: 65.33%\n",
      "Epoch [20/100], Loss: 0.6116, Val Loss: 0.6160, Accuracy: 65.43%\n",
      "Epoch [30/100], Loss: 0.6101, Val Loss: 0.6158, Accuracy: 65.43%\n",
      "Epoch [40/100], Loss: 0.6080, Val Loss: 0.6145, Accuracy: 64.82%\n",
      "Epoch [50/100], Loss: 0.6082, Val Loss: 0.6150, Accuracy: 65.18%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6151, Accuracy: 65.04%\n",
      "Epoch [70/100], Loss: 0.6075, Val Loss: 0.6157, Accuracy: 65.00%\n",
      "Epoch [80/100], Loss: 0.6076, Val Loss: 0.6154, Accuracy: 65.15%\n",
      "Epoch [90/100], Loss: 0.6076, Val Loss: 0.6143, Accuracy: 65.25%\n",
      "Epoch [100/100], Loss: 0.6062, Val Loss: 0.6140, Accuracy: 65.25%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6241, Val Loss: 0.5947, Accuracy: 67.59%\n",
      "Epoch [20/100], Loss: 0.6188, Val Loss: 0.5915, Accuracy: 67.09%\n",
      "Epoch [30/100], Loss: 0.6169, Val Loss: 0.5920, Accuracy: 68.16%\n",
      "Epoch [40/100], Loss: 0.6144, Val Loss: 0.5914, Accuracy: 68.20%\n",
      "Epoch [50/100], Loss: 0.6125, Val Loss: 0.5906, Accuracy: 67.73%\n",
      "Epoch [60/100], Loss: 0.6130, Val Loss: 0.5918, Accuracy: 67.91%\n",
      "Epoch [70/100], Loss: 0.6095, Val Loss: 0.5907, Accuracy: 67.91%\n",
      "Epoch [80/100], Loss: 0.6111, Val Loss: 0.5908, Accuracy: 67.62%\n",
      "Epoch [90/100], Loss: 0.6111, Val Loss: 0.5915, Accuracy: 67.80%\n",
      "Epoch [100/100], Loss: 0.6122, Val Loss: 0.5902, Accuracy: 67.98%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6209, Val Loss: 0.6125, Accuracy: 65.15%\n",
      "Epoch [20/100], Loss: 0.6156, Val Loss: 0.6108, Accuracy: 65.33%\n",
      "Epoch [30/100], Loss: 0.6117, Val Loss: 0.6107, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6115, Val Loss: 0.6086, Accuracy: 65.83%\n",
      "Epoch [50/100], Loss: 0.6096, Val Loss: 0.6093, Accuracy: 65.18%\n",
      "Epoch [60/100], Loss: 0.6075, Val Loss: 0.6084, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6087, Val Loss: 0.6085, Accuracy: 65.79%\n",
      "Epoch [80/100], Loss: 0.6070, Val Loss: 0.6088, Accuracy: 65.61%\n",
      "Epoch [90/100], Loss: 0.6078, Val Loss: 0.6087, Accuracy: 65.29%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6086, Accuracy: 65.51%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6165, Val Loss: 0.6177, Accuracy: 65.03%\n",
      "Epoch [20/100], Loss: 0.6103, Val Loss: 0.6191, Accuracy: 65.13%\n",
      "Epoch [30/100], Loss: 0.6088, Val Loss: 0.6203, Accuracy: 64.34%\n",
      "Epoch [40/100], Loss: 0.6102, Val Loss: 0.6196, Accuracy: 64.85%\n",
      "Epoch [50/100], Loss: 0.6113, Val Loss: 0.6196, Accuracy: 64.96%\n",
      "Epoch [60/100], Loss: 0.6101, Val Loss: 0.6191, Accuracy: 65.64%\n",
      "Epoch [70/100], Loss: 0.6108, Val Loss: 0.6196, Accuracy: 64.45%\n",
      "Epoch [80/100], Loss: 0.6111, Val Loss: 0.6190, Accuracy: 65.28%\n",
      "Epoch [90/100], Loss: 0.6104, Val Loss: 0.6203, Accuracy: 64.34%\n",
      "Epoch [100/100], Loss: 0.6100, Val Loss: 0.6187, Accuracy: 64.96%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6197, Val Loss: 0.6087, Accuracy: 65.78%\n",
      "Epoch [20/100], Loss: 0.6164, Val Loss: 0.6082, Accuracy: 65.60%\n",
      "Epoch [30/100], Loss: 0.6117, Val Loss: 0.6058, Accuracy: 65.89%\n",
      "Epoch [40/100], Loss: 0.6108, Val Loss: 0.6041, Accuracy: 66.21%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6056, Accuracy: 66.18%\n",
      "Epoch [60/100], Loss: 0.6082, Val Loss: 0.6045, Accuracy: 66.07%\n",
      "Epoch [70/100], Loss: 0.6054, Val Loss: 0.6049, Accuracy: 66.32%\n",
      "Epoch [80/100], Loss: 0.6073, Val Loss: 0.6043, Accuracy: 66.10%\n",
      "Epoch [90/100], Loss: 0.6089, Val Loss: 0.6042, Accuracy: 66.50%\n",
      "Epoch [100/100], Loss: 0.6072, Val Loss: 0.6045, Accuracy: 66.21%\n",
      "Best cross-validated accuracy: 68.38%\n",
      "Epoch [10/100], Loss: 0.6078, Val Loss: 0.5985, Accuracy: 67.76%\n",
      "Epoch [20/100], Loss: 0.6061, Val Loss: 0.5979, Accuracy: 67.76%\n",
      "Epoch [30/100], Loss: 0.6056, Val Loss: 0.5977, Accuracy: 68.07%\n",
      "Epoch [40/100], Loss: 0.6036, Val Loss: 0.5975, Accuracy: 68.16%\n",
      "Epoch [50/100], Loss: 0.6030, Val Loss: 0.5975, Accuracy: 68.27%\n",
      "Epoch [60/100], Loss: 0.6042, Val Loss: 0.5974, Accuracy: 68.30%\n",
      "Epoch [70/100], Loss: 0.6021, Val Loss: 0.5976, Accuracy: 68.42%\n",
      "Epoch [80/100], Loss: 0.6024, Val Loss: 0.5975, Accuracy: 68.42%\n",
      "Epoch [90/100], Loss: 0.6036, Val Loss: 0.5973, Accuracy: 68.27%\n",
      "Epoch [100/100], Loss: 0.6026, Val Loss: 0.5972, Accuracy: 68.13%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6195, Val Loss: 0.6159, Accuracy: 65.51%\n",
      "Epoch [20/100], Loss: 0.6124, Val Loss: 0.6164, Accuracy: 65.08%\n",
      "Epoch [30/100], Loss: 0.6096, Val Loss: 0.6142, Accuracy: 64.97%\n",
      "Epoch [40/100], Loss: 0.6077, Val Loss: 0.6149, Accuracy: 65.00%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.6153, Accuracy: 65.18%\n",
      "Epoch [60/100], Loss: 0.6068, Val Loss: 0.6147, Accuracy: 65.00%\n",
      "Epoch [70/100], Loss: 0.6073, Val Loss: 0.6146, Accuracy: 64.90%\n",
      "Epoch [80/100], Loss: 0.6077, Val Loss: 0.6138, Accuracy: 65.25%\n",
      "Epoch [90/100], Loss: 0.6099, Val Loss: 0.6139, Accuracy: 65.18%\n",
      "Epoch [100/100], Loss: 0.6076, Val Loss: 0.6142, Accuracy: 65.36%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6245, Val Loss: 0.5977, Accuracy: 67.44%\n",
      "Epoch [20/100], Loss: 0.6192, Val Loss: 0.5917, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.6183, Val Loss: 0.5945, Accuracy: 67.84%\n",
      "Epoch [40/100], Loss: 0.6165, Val Loss: 0.5931, Accuracy: 67.77%\n",
      "Epoch [50/100], Loss: 0.6141, Val Loss: 0.5930, Accuracy: 67.26%\n",
      "Epoch [60/100], Loss: 0.6135, Val Loss: 0.5935, Accuracy: 67.30%\n",
      "Epoch [70/100], Loss: 0.6143, Val Loss: 0.5944, Accuracy: 67.26%\n",
      "Epoch [80/100], Loss: 0.6140, Val Loss: 0.5924, Accuracy: 68.20%\n",
      "Epoch [90/100], Loss: 0.6147, Val Loss: 0.5928, Accuracy: 67.55%\n",
      "Epoch [100/100], Loss: 0.6140, Val Loss: 0.5934, Accuracy: 67.80%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6214, Val Loss: 0.6118, Accuracy: 65.15%\n",
      "Epoch [20/100], Loss: 0.6154, Val Loss: 0.6114, Accuracy: 65.11%\n",
      "Epoch [30/100], Loss: 0.6110, Val Loss: 0.6089, Accuracy: 65.15%\n",
      "Epoch [40/100], Loss: 0.6110, Val Loss: 0.6079, Accuracy: 65.18%\n",
      "Epoch [50/100], Loss: 0.6101, Val Loss: 0.6077, Accuracy: 65.36%\n",
      "Epoch [60/100], Loss: 0.6097, Val Loss: 0.6075, Accuracy: 65.61%\n",
      "Epoch [70/100], Loss: 0.6076, Val Loss: 0.6075, Accuracy: 65.61%\n",
      "Epoch [80/100], Loss: 0.6073, Val Loss: 0.6074, Accuracy: 65.40%\n",
      "Epoch [90/100], Loss: 0.6065, Val Loss: 0.6069, Accuracy: 65.61%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6074, Accuracy: 65.51%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6183, Val Loss: 0.6176, Accuracy: 64.78%\n",
      "Epoch [20/100], Loss: 0.6126, Val Loss: 0.6186, Accuracy: 64.88%\n",
      "Epoch [30/100], Loss: 0.6091, Val Loss: 0.6187, Accuracy: 65.10%\n",
      "Epoch [40/100], Loss: 0.6101, Val Loss: 0.6188, Accuracy: 64.60%\n",
      "Epoch [50/100], Loss: 0.6116, Val Loss: 0.6180, Accuracy: 65.31%\n",
      "Epoch [60/100], Loss: 0.6135, Val Loss: 0.6188, Accuracy: 65.17%\n",
      "Epoch [70/100], Loss: 0.6086, Val Loss: 0.6202, Accuracy: 63.91%\n",
      "Epoch [80/100], Loss: 0.6099, Val Loss: 0.6182, Accuracy: 64.67%\n",
      "Epoch [90/100], Loss: 0.6104, Val Loss: 0.6189, Accuracy: 65.03%\n",
      "Epoch [100/100], Loss: 0.6103, Val Loss: 0.6182, Accuracy: 65.03%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6089, Accuracy: 65.60%\n",
      "Epoch [20/100], Loss: 0.6167, Val Loss: 0.6057, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.6140, Val Loss: 0.6047, Accuracy: 66.39%\n",
      "Epoch [40/100], Loss: 0.6121, Val Loss: 0.6040, Accuracy: 66.46%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6037, Accuracy: 66.36%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.6040, Accuracy: 66.46%\n",
      "Epoch [70/100], Loss: 0.6091, Val Loss: 0.6033, Accuracy: 66.54%\n",
      "Epoch [80/100], Loss: 0.6087, Val Loss: 0.6034, Accuracy: 66.39%\n",
      "Epoch [90/100], Loss: 0.6080, Val Loss: 0.6038, Accuracy: 66.03%\n",
      "Epoch [100/100], Loss: 0.6080, Val Loss: 0.6035, Accuracy: 66.50%\n",
      "Best cross-validated accuracy: 68.45%\n",
      "Epoch [10/100], Loss: 0.6105, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [20/100], Loss: 0.6083, Val Loss: 0.5994, Accuracy: 68.25%\n",
      "Epoch [30/100], Loss: 0.6075, Val Loss: 0.5975, Accuracy: 68.02%\n",
      "Epoch [40/100], Loss: 0.6076, Val Loss: 0.5976, Accuracy: 67.99%\n",
      "Epoch [50/100], Loss: 0.6056, Val Loss: 0.5977, Accuracy: 67.79%\n",
      "Epoch [60/100], Loss: 0.6071, Val Loss: 0.5976, Accuracy: 67.96%\n",
      "Epoch [70/100], Loss: 0.6062, Val Loss: 0.5978, Accuracy: 67.96%\n",
      "Epoch [80/100], Loss: 0.6069, Val Loss: 0.5976, Accuracy: 67.99%\n",
      "Epoch [90/100], Loss: 0.6048, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [100/100], Loss: 0.6048, Val Loss: 0.5976, Accuracy: 67.96%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6183, Val Loss: 0.6160, Accuracy: 65.04%\n",
      "Epoch [20/100], Loss: 0.6135, Val Loss: 0.6169, Accuracy: 64.97%\n",
      "Epoch [30/100], Loss: 0.6098, Val Loss: 0.6148, Accuracy: 64.97%\n",
      "Epoch [40/100], Loss: 0.6090, Val Loss: 0.6137, Accuracy: 65.08%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6156, Accuracy: 65.00%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6142, Accuracy: 65.15%\n",
      "Epoch [70/100], Loss: 0.6079, Val Loss: 0.6143, Accuracy: 64.97%\n",
      "Epoch [80/100], Loss: 0.6099, Val Loss: 0.6139, Accuracy: 65.29%\n",
      "Epoch [90/100], Loss: 0.6078, Val Loss: 0.6141, Accuracy: 65.04%\n",
      "Epoch [100/100], Loss: 0.6092, Val Loss: 0.6151, Accuracy: 64.75%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6243, Val Loss: 0.5969, Accuracy: 67.37%\n",
      "Epoch [20/100], Loss: 0.6185, Val Loss: 0.5937, Accuracy: 67.52%\n",
      "Epoch [30/100], Loss: 0.6158, Val Loss: 0.5926, Accuracy: 67.55%\n",
      "Epoch [40/100], Loss: 0.6158, Val Loss: 0.5940, Accuracy: 67.59%\n",
      "Epoch [50/100], Loss: 0.6147, Val Loss: 0.5924, Accuracy: 67.77%\n",
      "Epoch [60/100], Loss: 0.6147, Val Loss: 0.5926, Accuracy: 67.73%\n",
      "Epoch [70/100], Loss: 0.6149, Val Loss: 0.5927, Accuracy: 67.91%\n",
      "Epoch [80/100], Loss: 0.6161, Val Loss: 0.5922, Accuracy: 67.80%\n",
      "Epoch [90/100], Loss: 0.6159, Val Loss: 0.5920, Accuracy: 67.66%\n",
      "Epoch [100/100], Loss: 0.6142, Val Loss: 0.5941, Accuracy: 67.88%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6242, Val Loss: 0.6116, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6153, Val Loss: 0.6103, Accuracy: 65.47%\n",
      "Epoch [30/100], Loss: 0.6131, Val Loss: 0.6095, Accuracy: 65.33%\n",
      "Epoch [40/100], Loss: 0.6096, Val Loss: 0.6088, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6070, Val Loss: 0.6079, Accuracy: 66.26%\n",
      "Epoch [60/100], Loss: 0.6070, Val Loss: 0.6074, Accuracy: 65.87%\n",
      "Epoch [70/100], Loss: 0.6066, Val Loss: 0.6076, Accuracy: 65.61%\n",
      "Epoch [80/100], Loss: 0.6042, Val Loss: 0.6069, Accuracy: 65.65%\n",
      "Epoch [90/100], Loss: 0.6078, Val Loss: 0.6072, Accuracy: 65.76%\n",
      "Epoch [100/100], Loss: 0.6074, Val Loss: 0.6073, Accuracy: 66.12%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6200, Val Loss: 0.6204, Accuracy: 64.13%\n",
      "Epoch [20/100], Loss: 0.6110, Val Loss: 0.6194, Accuracy: 64.49%\n",
      "Epoch [30/100], Loss: 0.6122, Val Loss: 0.6198, Accuracy: 64.85%\n",
      "Epoch [40/100], Loss: 0.6128, Val Loss: 0.6194, Accuracy: 64.67%\n",
      "Epoch [50/100], Loss: 0.6106, Val Loss: 0.6194, Accuracy: 64.67%\n",
      "Epoch [60/100], Loss: 0.6105, Val Loss: 0.6198, Accuracy: 64.27%\n",
      "Epoch [70/100], Loss: 0.6125, Val Loss: 0.6197, Accuracy: 64.42%\n",
      "Epoch [80/100], Loss: 0.6100, Val Loss: 0.6197, Accuracy: 64.24%\n",
      "Epoch [90/100], Loss: 0.6107, Val Loss: 0.6190, Accuracy: 64.70%\n",
      "Epoch [100/100], Loss: 0.6127, Val Loss: 0.6206, Accuracy: 64.60%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6242, Val Loss: 0.6082, Accuracy: 65.60%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6054, Accuracy: 65.71%\n",
      "Epoch [30/100], Loss: 0.6116, Val Loss: 0.6067, Accuracy: 65.46%\n",
      "Epoch [40/100], Loss: 0.6100, Val Loss: 0.6049, Accuracy: 66.07%\n",
      "Epoch [50/100], Loss: 0.6102, Val Loss: 0.6059, Accuracy: 65.78%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6057, Accuracy: 66.36%\n",
      "Epoch [70/100], Loss: 0.6101, Val Loss: 0.6052, Accuracy: 65.67%\n",
      "Epoch [80/100], Loss: 0.6069, Val Loss: 0.6034, Accuracy: 65.96%\n",
      "Epoch [90/100], Loss: 0.6064, Val Loss: 0.6042, Accuracy: 66.50%\n",
      "Epoch [100/100], Loss: 0.6034, Val Loss: 0.6039, Accuracy: 66.03%\n",
      "Best cross-validated accuracy: 68.09%\n",
      "Epoch [10/100], Loss: 0.6079, Val Loss: 0.5998, Accuracy: 68.33%\n",
      "Epoch [20/100], Loss: 0.6095, Val Loss: 0.5987, Accuracy: 68.22%\n",
      "Epoch [30/100], Loss: 0.6086, Val Loss: 0.5977, Accuracy: 67.84%\n",
      "Epoch [40/100], Loss: 0.6060, Val Loss: 0.5978, Accuracy: 67.87%\n",
      "Epoch [50/100], Loss: 0.6072, Val Loss: 0.5978, Accuracy: 67.84%\n",
      "Epoch [60/100], Loss: 0.6067, Val Loss: 0.5977, Accuracy: 67.79%\n",
      "Epoch [70/100], Loss: 0.6069, Val Loss: 0.5977, Accuracy: 67.87%\n",
      "Epoch [80/100], Loss: 0.6072, Val Loss: 0.5977, Accuracy: 67.93%\n",
      "Epoch [90/100], Loss: 0.6066, Val Loss: 0.5979, Accuracy: 67.87%\n",
      "Epoch [100/100], Loss: 0.6073, Val Loss: 0.5978, Accuracy: 67.93%\n",
      "Final model accuracy on test set: 68.39%\n",
      "Accuracy 68.39% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6187, Val Loss: 0.6188, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6128, Val Loss: 0.6166, Accuracy: 64.97%\n",
      "Epoch [30/100], Loss: 0.6094, Val Loss: 0.6149, Accuracy: 65.22%\n",
      "Epoch [40/100], Loss: 0.6098, Val Loss: 0.6149, Accuracy: 65.36%\n",
      "Epoch [50/100], Loss: 0.6097, Val Loss: 0.6152, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6092, Val Loss: 0.6157, Accuracy: 65.29%\n",
      "Epoch [70/100], Loss: 0.6066, Val Loss: 0.6164, Accuracy: 64.90%\n",
      "Epoch [80/100], Loss: 0.6081, Val Loss: 0.6150, Accuracy: 65.36%\n",
      "Epoch [90/100], Loss: 0.6092, Val Loss: 0.6142, Accuracy: 65.29%\n",
      "Epoch [100/100], Loss: 0.6079, Val Loss: 0.6153, Accuracy: 65.43%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6205, Val Loss: 0.5966, Accuracy: 67.37%\n",
      "Epoch [20/100], Loss: 0.6188, Val Loss: 0.5978, Accuracy: 67.55%\n",
      "Epoch [30/100], Loss: 0.6179, Val Loss: 0.5937, Accuracy: 67.91%\n",
      "Epoch [40/100], Loss: 0.6138, Val Loss: 0.5915, Accuracy: 67.44%\n",
      "Epoch [50/100], Loss: 0.6116, Val Loss: 0.5925, Accuracy: 68.02%\n",
      "Epoch [60/100], Loss: 0.6133, Val Loss: 0.5921, Accuracy: 67.52%\n",
      "Epoch [70/100], Loss: 0.6140, Val Loss: 0.5914, Accuracy: 67.66%\n",
      "Epoch [80/100], Loss: 0.6125, Val Loss: 0.5926, Accuracy: 67.84%\n",
      "Epoch [90/100], Loss: 0.6121, Val Loss: 0.5920, Accuracy: 68.13%\n",
      "Epoch [100/100], Loss: 0.6129, Val Loss: 0.5937, Accuracy: 68.05%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6202, Val Loss: 0.6118, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6164, Val Loss: 0.6103, Accuracy: 65.33%\n",
      "Epoch [30/100], Loss: 0.6124, Val Loss: 0.6098, Accuracy: 66.01%\n",
      "Epoch [40/100], Loss: 0.6118, Val Loss: 0.6089, Accuracy: 64.97%\n",
      "Epoch [50/100], Loss: 0.6083, Val Loss: 0.6081, Accuracy: 65.79%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.6094, Accuracy: 64.97%\n",
      "Epoch [70/100], Loss: 0.6089, Val Loss: 0.6090, Accuracy: 65.18%\n",
      "Epoch [80/100], Loss: 0.6068, Val Loss: 0.6081, Accuracy: 65.61%\n",
      "Epoch [90/100], Loss: 0.6083, Val Loss: 0.6092, Accuracy: 65.33%\n",
      "Epoch [100/100], Loss: 0.6078, Val Loss: 0.6082, Accuracy: 65.18%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6161, Val Loss: 0.6189, Accuracy: 65.28%\n",
      "Epoch [20/100], Loss: 0.6114, Val Loss: 0.6187, Accuracy: 64.67%\n",
      "Epoch [30/100], Loss: 0.6090, Val Loss: 0.6194, Accuracy: 64.81%\n",
      "Epoch [40/100], Loss: 0.6049, Val Loss: 0.6190, Accuracy: 64.81%\n",
      "Epoch [50/100], Loss: 0.6076, Val Loss: 0.6197, Accuracy: 64.92%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.6203, Accuracy: 65.03%\n",
      "Epoch [70/100], Loss: 0.6077, Val Loss: 0.6198, Accuracy: 64.60%\n",
      "Epoch [80/100], Loss: 0.6072, Val Loss: 0.6198, Accuracy: 64.60%\n",
      "Epoch [90/100], Loss: 0.6092, Val Loss: 0.6196, Accuracy: 64.63%\n",
      "Epoch [100/100], Loss: 0.6063, Val Loss: 0.6201, Accuracy: 64.88%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6200, Val Loss: 0.6060, Accuracy: 66.07%\n",
      "Epoch [20/100], Loss: 0.6138, Val Loss: 0.6045, Accuracy: 66.43%\n",
      "Epoch [30/100], Loss: 0.6126, Val Loss: 0.6042, Accuracy: 66.25%\n",
      "Epoch [40/100], Loss: 0.6124, Val Loss: 0.6043, Accuracy: 66.21%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6041, Accuracy: 66.36%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6050, Accuracy: 66.10%\n",
      "Epoch [70/100], Loss: 0.6098, Val Loss: 0.6039, Accuracy: 66.03%\n",
      "Epoch [80/100], Loss: 0.6113, Val Loss: 0.6036, Accuracy: 66.28%\n",
      "Epoch [90/100], Loss: 0.6114, Val Loss: 0.6038, Accuracy: 66.36%\n",
      "Epoch [100/100], Loss: 0.6112, Val Loss: 0.6034, Accuracy: 66.75%\n",
      "Best cross-validated accuracy: 68.20%\n",
      "Epoch [10/100], Loss: 0.6103, Val Loss: 0.5982, Accuracy: 68.25%\n",
      "Final model accuracy on test set: 68.73%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.687 # 75%\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        0.0   0.190110    6.73    1.11\n",
      "1        0.0   0.425913    2.15    1.67\n",
      "2        0.0   0.690066    1.51    2.48\n",
      "3        1.0   0.352727    2.70    1.43\n",
      "4        1.0   0.707952    1.71    2.10\n",
      "...      ...        ...     ...     ...\n",
      "3478     0.0   0.256129    2.61    1.47\n",
      "3479     1.0   0.496119    1.48    2.63\n",
      "3480     0.0   0.354244    3.26    1.32\n",
      "3481     0.0   0.404170    2.35    1.58\n",
      "3482     0.0   0.615572    3.05    1.36\n",
      "\n",
      "[3483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $10 bets: 204.00 on a total # bets: 268 from a total of 3483 games\n",
      "Amount of differing favorites 0.14786103933390754\n",
      "Amount of upset correct 0.4679611650485437 won $204.7 on 515 bets\n",
      "Amount of incorrect bets : 0.5410447761194029\n",
      "Correct Bets: 0.458955223880597\n",
      "Model % Correct : 0.6873385012919897 Vegas Correct % : 0.7025552684467413\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "upset_predict = 0\n",
    "upset_correct = 0\n",
    "upset_won = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .5\n",
    "confidence_top_pct = .55\n",
    "\n",
    "UNIT = 10\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct and row['Predicted'] > 1/row['A_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['A_Odds']-1) * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['B_Odds']-1) * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        upset_predict += 1\n",
    "        if round(row['Predicted']) == round(row['Actual']):\n",
    "            upset_correct += 1\n",
    "            if(row['Actual'] == 1):\n",
    "                upset_won += (row['A_Odds']-1) * UNIT\n",
    "            else:\n",
    "                upset_won += (row['B_Odds']-1) * UNIT\n",
    "        else:\n",
    "            upset_won -= UNIT\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on ${UNIT} bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of upset correct {upset_correct/upset_predict} won ${upset_won} on {upset_predict} bets\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
