{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36152\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('testcsvs/testout2.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_glicko_rd'] <= 150) & (df['b_glicko_rd'] <= 150)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>match_num</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff_high</th>\n",
       "      <th>glicko_rating_diff_low</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_high</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_low</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-312.688126</td>\n",
       "      <td>-40.429749</td>\n",
       "      <td>1663.238130</td>\n",
       "      <td>1839.797067</td>\n",
       "      <td>69.521886</td>\n",
       "      <td>66.607302</td>\n",
       "      <td>...</td>\n",
       "      <td>-142.430173</td>\n",
       "      <td>111.103350</td>\n",
       "      <td>1514.785695</td>\n",
       "      <td>1530.449107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-121.419605</td>\n",
       "      <td>147.756632</td>\n",
       "      <td>1704.915868</td>\n",
       "      <td>1691.747354</td>\n",
       "      <td>66.531031</td>\n",
       "      <td>68.057087</td>\n",
       "      <td>...</td>\n",
       "      <td>-125.068212</td>\n",
       "      <td>125.074139</td>\n",
       "      <td>1524.307970</td>\n",
       "      <td>1524.305006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-124.132627</td>\n",
       "      <td>182.766003</td>\n",
       "      <td>1599.867737</td>\n",
       "      <td>1570.551049</td>\n",
       "      <td>70.223143</td>\n",
       "      <td>83.226172</td>\n",
       "      <td>...</td>\n",
       "      <td>-130.959262</td>\n",
       "      <td>127.522140</td>\n",
       "      <td>1508.366657</td>\n",
       "      <td>1510.085217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-226.786525</td>\n",
       "      <td>45.492588</td>\n",
       "      <td>1640.524463</td>\n",
       "      <td>1731.171432</td>\n",
       "      <td>70.112236</td>\n",
       "      <td>66.027320</td>\n",
       "      <td>...</td>\n",
       "      <td>-127.146095</td>\n",
       "      <td>125.314148</td>\n",
       "      <td>1507.539322</td>\n",
       "      <td>1508.455296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-452.711077</td>\n",
       "      <td>-158.974000</td>\n",
       "      <td>1545.639101</td>\n",
       "      <td>1851.481640</td>\n",
       "      <td>79.730461</td>\n",
       "      <td>67.138078</td>\n",
       "      <td>...</td>\n",
       "      <td>-143.077859</td>\n",
       "      <td>109.514614</td>\n",
       "      <td>1486.558404</td>\n",
       "      <td>1503.340027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  match_num  a_player_rank  b_player_rank  \\\n",
       "5373      3.0       12.0           74.0           15.0   \n",
       "5375      3.0       14.0           46.0           65.0   \n",
       "5377      3.0        8.0           89.0           95.0   \n",
       "5378      3.0        7.0           83.0           48.0   \n",
       "5380      3.0        8.0           70.0           22.0   \n",
       "\n",
       "      glicko_rating_diff_high  glicko_rating_diff_low  a_glicko_rating  \\\n",
       "5373              -312.688126              -40.429749      1663.238130   \n",
       "5375              -121.419605              147.756632      1704.915868   \n",
       "5377              -124.132627              182.766003      1599.867737   \n",
       "5378              -226.786525               45.492588      1640.524463   \n",
       "5380              -452.711077             -158.974000      1545.639101   \n",
       "\n",
       "      b_glicko_rating  a_glicko_rd  b_glicko_rd  ...  \\\n",
       "5373      1839.797067    69.521886    66.607302  ...   \n",
       "5375      1691.747354    66.531031    68.057087  ...   \n",
       "5377      1570.551049    70.223143    83.226172  ...   \n",
       "5378      1731.171432    70.112236    66.027320  ...   \n",
       "5380      1851.481640    79.730461    67.138078  ...   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_high  \\\n",
       "5373                                        -142.430173   \n",
       "5375                                        -125.068212   \n",
       "5377                                        -130.959262   \n",
       "5378                                        -127.146095   \n",
       "5380                                        -143.077859   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_low  \\\n",
       "5373                                        111.103350   \n",
       "5375                                        125.074139   \n",
       "5377                                        127.522140   \n",
       "5378                                        125.314148   \n",
       "5380                                        109.514614   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rating  \\\n",
       "5373                                1514.785695   \n",
       "5375                                1524.307970   \n",
       "5377                                1508.366657   \n",
       "5378                                1507.539322   \n",
       "5380                                1486.558404   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  a_b_win  a_odds  b_odds  \\\n",
       "5373                         1530.449107      0.0    3.59    1.28   \n",
       "5375                         1524.305006      0.0     NaN     NaN   \n",
       "5377                         1510.085217      0.0    2.29    1.59   \n",
       "5378                         1508.455296      1.0    1.54    2.40   \n",
       "5380                         1503.340027      0.0    4.44    1.19   \n",
       "\n",
       "      surface_Clay  surface_Grass  surface_Hard  \n",
       "5373           0.0            0.0           1.0  \n",
       "5375           0.0            0.0           1.0  \n",
       "5377           0.0            0.0           1.0  \n",
       "5378           0.0            0.0           1.0  \n",
       "5380           0.0            0.0           1.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6194, Val Loss: 0.6168, Accuracy: 65.04%\n",
      "Epoch [20/100], Loss: 0.6153, Val Loss: 0.6157, Accuracy: 64.86%\n",
      "Epoch [30/100], Loss: 0.6115, Val Loss: 0.6144, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6164, Accuracy: 65.22%\n",
      "Epoch [50/100], Loss: 0.6054, Val Loss: 0.6164, Accuracy: 65.22%\n",
      "Epoch [60/100], Loss: 0.6055, Val Loss: 0.6170, Accuracy: 65.04%\n",
      "Epoch [70/100], Loss: 0.6063, Val Loss: 0.6154, Accuracy: 65.18%\n",
      "Epoch [80/100], Loss: 0.6067, Val Loss: 0.6162, Accuracy: 65.18%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6155, Accuracy: 65.11%\n",
      "Epoch [100/100], Loss: 0.6067, Val Loss: 0.6158, Accuracy: 64.86%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.5956, Accuracy: 67.01%\n",
      "Epoch [20/100], Loss: 0.6173, Val Loss: 0.5915, Accuracy: 67.30%\n",
      "Epoch [30/100], Loss: 0.6150, Val Loss: 0.5910, Accuracy: 67.44%\n",
      "Epoch [40/100], Loss: 0.6139, Val Loss: 0.5935, Accuracy: 68.13%\n",
      "Epoch [50/100], Loss: 0.6118, Val Loss: 0.5913, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.6136, Val Loss: 0.5920, Accuracy: 68.23%\n",
      "Epoch [70/100], Loss: 0.6137, Val Loss: 0.5912, Accuracy: 67.73%\n",
      "Epoch [80/100], Loss: 0.6128, Val Loss: 0.5914, Accuracy: 67.66%\n",
      "Epoch [90/100], Loss: 0.6131, Val Loss: 0.5918, Accuracy: 67.80%\n",
      "Epoch [100/100], Loss: 0.6143, Val Loss: 0.5936, Accuracy: 67.77%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6175, Val Loss: 0.6102, Accuracy: 65.47%\n",
      "Epoch [20/100], Loss: 0.6168, Val Loss: 0.6099, Accuracy: 65.61%\n",
      "Epoch [30/100], Loss: 0.6112, Val Loss: 0.6077, Accuracy: 65.90%\n",
      "Epoch [40/100], Loss: 0.6119, Val Loss: 0.6081, Accuracy: 65.94%\n",
      "Epoch [50/100], Loss: 0.6135, Val Loss: 0.6078, Accuracy: 65.65%\n",
      "Epoch [60/100], Loss: 0.6066, Val Loss: 0.6076, Accuracy: 65.33%\n",
      "Epoch [70/100], Loss: 0.6063, Val Loss: 0.6079, Accuracy: 65.51%\n",
      "Epoch [80/100], Loss: 0.6072, Val Loss: 0.6071, Accuracy: 65.33%\n",
      "Epoch [90/100], Loss: 0.6068, Val Loss: 0.6071, Accuracy: 65.22%\n",
      "Epoch [100/100], Loss: 0.6060, Val Loss: 0.6070, Accuracy: 65.43%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6185, Val Loss: 0.6177, Accuracy: 64.99%\n",
      "Epoch [20/100], Loss: 0.6113, Val Loss: 0.6188, Accuracy: 64.42%\n",
      "Epoch [30/100], Loss: 0.6076, Val Loss: 0.6202, Accuracy: 64.70%\n",
      "Epoch [40/100], Loss: 0.6099, Val Loss: 0.6194, Accuracy: 64.78%\n",
      "Epoch [50/100], Loss: 0.6070, Val Loss: 0.6189, Accuracy: 64.67%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6193, Accuracy: 64.70%\n",
      "Epoch [70/100], Loss: 0.6089, Val Loss: 0.6189, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6086, Val Loss: 0.6200, Accuracy: 64.49%\n",
      "Epoch [90/100], Loss: 0.6093, Val Loss: 0.6194, Accuracy: 64.74%\n",
      "Epoch [100/100], Loss: 0.6097, Val Loss: 0.6213, Accuracy: 64.13%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6233, Val Loss: 0.6094, Accuracy: 65.17%\n",
      "Epoch [20/100], Loss: 0.6182, Val Loss: 0.6048, Accuracy: 66.43%\n",
      "Epoch [30/100], Loss: 0.6121, Val Loss: 0.6058, Accuracy: 66.03%\n",
      "Epoch [40/100], Loss: 0.6125, Val Loss: 0.6034, Accuracy: 66.18%\n",
      "Epoch [50/100], Loss: 0.6096, Val Loss: 0.6039, Accuracy: 66.10%\n",
      "Epoch [60/100], Loss: 0.6103, Val Loss: 0.6030, Accuracy: 66.50%\n",
      "Epoch [70/100], Loss: 0.6104, Val Loss: 0.6049, Accuracy: 66.39%\n",
      "Epoch [80/100], Loss: 0.6092, Val Loss: 0.6038, Accuracy: 66.07%\n",
      "Epoch [90/100], Loss: 0.6074, Val Loss: 0.6031, Accuracy: 66.39%\n",
      "Epoch [100/100], Loss: 0.6092, Val Loss: 0.6034, Accuracy: 66.07%\n",
      "Best cross-validated accuracy: 68.23%\n",
      "Epoch [10/100], Loss: 0.6102, Val Loss: 0.5977, Accuracy: 67.73%\n",
      "Epoch [20/100], Loss: 0.6080, Val Loss: 0.5966, Accuracy: 67.96%\n",
      "Epoch [30/100], Loss: 0.6065, Val Loss: 0.5968, Accuracy: 67.96%\n",
      "Epoch [40/100], Loss: 0.6062, Val Loss: 0.5969, Accuracy: 67.90%\n",
      "Epoch [50/100], Loss: 0.6072, Val Loss: 0.5968, Accuracy: 68.02%\n",
      "Epoch [60/100], Loss: 0.6066, Val Loss: 0.5969, Accuracy: 67.93%\n",
      "Epoch [70/100], Loss: 0.6072, Val Loss: 0.5968, Accuracy: 67.96%\n",
      "Epoch [80/100], Loss: 0.6064, Val Loss: 0.5970, Accuracy: 68.04%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.5966, Accuracy: 68.02%\n",
      "Epoch [100/100], Loss: 0.6066, Val Loss: 0.5969, Accuracy: 68.02%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6189, Val Loss: 0.6157, Accuracy: 64.86%\n",
      "Epoch [20/100], Loss: 0.6145, Val Loss: 0.6154, Accuracy: 64.79%\n",
      "Epoch [30/100], Loss: 0.6090, Val Loss: 0.6140, Accuracy: 65.08%\n",
      "Epoch [40/100], Loss: 0.6073, Val Loss: 0.6180, Accuracy: 64.97%\n",
      "Epoch [50/100], Loss: 0.6080, Val Loss: 0.6145, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6071, Val Loss: 0.6150, Accuracy: 65.33%\n",
      "Epoch [70/100], Loss: 0.6056, Val Loss: 0.6145, Accuracy: 64.43%\n",
      "Epoch [80/100], Loss: 0.6054, Val Loss: 0.6149, Accuracy: 65.15%\n",
      "Epoch [90/100], Loss: 0.6035, Val Loss: 0.6161, Accuracy: 64.64%\n",
      "Epoch [100/100], Loss: 0.6061, Val Loss: 0.6138, Accuracy: 65.33%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6227, Val Loss: 0.5928, Accuracy: 67.34%\n",
      "Epoch [20/100], Loss: 0.6198, Val Loss: 0.5915, Accuracy: 67.77%\n",
      "Epoch [30/100], Loss: 0.6165, Val Loss: 0.5926, Accuracy: 68.13%\n",
      "Epoch [40/100], Loss: 0.6127, Val Loss: 0.5918, Accuracy: 67.52%\n",
      "Epoch [50/100], Loss: 0.6121, Val Loss: 0.5934, Accuracy: 68.02%\n",
      "Epoch [60/100], Loss: 0.6116, Val Loss: 0.5920, Accuracy: 67.62%\n",
      "Epoch [70/100], Loss: 0.6121, Val Loss: 0.5905, Accuracy: 67.77%\n",
      "Epoch [80/100], Loss: 0.6130, Val Loss: 0.5920, Accuracy: 67.77%\n",
      "Epoch [90/100], Loss: 0.6142, Val Loss: 0.5913, Accuracy: 67.66%\n",
      "Epoch [100/100], Loss: 0.6118, Val Loss: 0.5921, Accuracy: 68.49%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.6117, Accuracy: 65.15%\n",
      "Epoch [20/100], Loss: 0.6144, Val Loss: 0.6086, Accuracy: 65.51%\n",
      "Epoch [30/100], Loss: 0.6113, Val Loss: 0.6095, Accuracy: 65.94%\n",
      "Epoch [40/100], Loss: 0.6097, Val Loss: 0.6083, Accuracy: 65.47%\n",
      "Epoch [50/100], Loss: 0.6109, Val Loss: 0.6084, Accuracy: 65.76%\n",
      "Epoch [60/100], Loss: 0.6085, Val Loss: 0.6079, Accuracy: 65.87%\n",
      "Epoch [70/100], Loss: 0.6091, Val Loss: 0.6077, Accuracy: 65.90%\n",
      "Epoch [80/100], Loss: 0.6080, Val Loss: 0.6077, Accuracy: 65.83%\n",
      "Epoch [90/100], Loss: 0.6071, Val Loss: 0.6076, Accuracy: 65.51%\n",
      "Epoch [100/100], Loss: 0.6080, Val Loss: 0.6078, Accuracy: 65.69%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6156, Val Loss: 0.6214, Accuracy: 64.70%\n",
      "Epoch [20/100], Loss: 0.6134, Val Loss: 0.6193, Accuracy: 64.88%\n",
      "Epoch [30/100], Loss: 0.6107, Val Loss: 0.6185, Accuracy: 64.85%\n",
      "Epoch [40/100], Loss: 0.6100, Val Loss: 0.6189, Accuracy: 64.09%\n",
      "Epoch [50/100], Loss: 0.6103, Val Loss: 0.6204, Accuracy: 64.09%\n",
      "Epoch [60/100], Loss: 0.6092, Val Loss: 0.6194, Accuracy: 64.70%\n",
      "Epoch [70/100], Loss: 0.6109, Val Loss: 0.6196, Accuracy: 64.34%\n",
      "Epoch [80/100], Loss: 0.6089, Val Loss: 0.6186, Accuracy: 64.92%\n",
      "Epoch [90/100], Loss: 0.6104, Val Loss: 0.6185, Accuracy: 64.27%\n",
      "Epoch [100/100], Loss: 0.6115, Val Loss: 0.6203, Accuracy: 63.88%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6179, Val Loss: 0.6057, Accuracy: 66.25%\n",
      "Epoch [20/100], Loss: 0.6138, Val Loss: 0.6042, Accuracy: 66.36%\n",
      "Epoch [30/100], Loss: 0.6114, Val Loss: 0.6047, Accuracy: 66.50%\n",
      "Epoch [40/100], Loss: 0.6103, Val Loss: 0.6041, Accuracy: 66.46%\n",
      "Epoch [50/100], Loss: 0.6092, Val Loss: 0.6035, Accuracy: 66.36%\n",
      "Epoch [60/100], Loss: 0.6096, Val Loss: 0.6043, Accuracy: 65.89%\n",
      "Epoch [70/100], Loss: 0.6123, Val Loss: 0.6036, Accuracy: 66.82%\n",
      "Epoch [80/100], Loss: 0.6109, Val Loss: 0.6047, Accuracy: 65.96%\n",
      "Epoch [90/100], Loss: 0.6118, Val Loss: 0.6041, Accuracy: 66.54%\n",
      "Epoch [100/100], Loss: 0.6125, Val Loss: 0.6036, Accuracy: 66.54%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6074, Val Loss: 0.5972, Accuracy: 67.84%\n",
      "Epoch [20/100], Loss: 0.6078, Val Loss: 0.5982, Accuracy: 68.07%\n",
      "Epoch [30/100], Loss: 0.6048, Val Loss: 0.5968, Accuracy: 67.56%\n",
      "Epoch [40/100], Loss: 0.6057, Val Loss: 0.5969, Accuracy: 67.70%\n",
      "Epoch [50/100], Loss: 0.6035, Val Loss: 0.5968, Accuracy: 67.76%\n",
      "Epoch [60/100], Loss: 0.6042, Val Loss: 0.5971, Accuracy: 67.84%\n",
      "Epoch [70/100], Loss: 0.6052, Val Loss: 0.5970, Accuracy: 67.73%\n",
      "Epoch [80/100], Loss: 0.6031, Val Loss: 0.5969, Accuracy: 67.67%\n",
      "Epoch [90/100], Loss: 0.6034, Val Loss: 0.5972, Accuracy: 67.93%\n",
      "Epoch [100/100], Loss: 0.6041, Val Loss: 0.5970, Accuracy: 67.84%\n",
      "Final model accuracy on test set: 68.22%\n",
      "Accuracy 68.22% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6208, Val Loss: 0.6168, Accuracy: 65.36%\n",
      "Epoch [20/100], Loss: 0.6148, Val Loss: 0.6140, Accuracy: 64.90%\n",
      "Epoch [30/100], Loss: 0.6106, Val Loss: 0.6162, Accuracy: 64.72%\n",
      "Epoch [40/100], Loss: 0.6087, Val Loss: 0.6156, Accuracy: 65.25%\n",
      "Epoch [50/100], Loss: 0.6085, Val Loss: 0.6152, Accuracy: 65.00%\n",
      "Epoch [60/100], Loss: 0.6101, Val Loss: 0.6153, Accuracy: 65.25%\n",
      "Epoch [70/100], Loss: 0.6092, Val Loss: 0.6147, Accuracy: 65.47%\n",
      "Epoch [80/100], Loss: 0.6088, Val Loss: 0.6149, Accuracy: 65.29%\n",
      "Epoch [90/100], Loss: 0.6080, Val Loss: 0.6147, Accuracy: 65.51%\n",
      "Epoch [100/100], Loss: 0.6098, Val Loss: 0.6151, Accuracy: 65.00%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6236, Val Loss: 0.5976, Accuracy: 66.94%\n",
      "Epoch [20/100], Loss: 0.6174, Val Loss: 0.5962, Accuracy: 67.05%\n",
      "Epoch [30/100], Loss: 0.6153, Val Loss: 0.5906, Accuracy: 67.70%\n",
      "Epoch [40/100], Loss: 0.6151, Val Loss: 0.5933, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.6126, Val Loss: 0.5912, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.6113, Val Loss: 0.5943, Accuracy: 67.84%\n",
      "Epoch [70/100], Loss: 0.6141, Val Loss: 0.5915, Accuracy: 68.05%\n",
      "Epoch [80/100], Loss: 0.6112, Val Loss: 0.5907, Accuracy: 67.91%\n",
      "Epoch [90/100], Loss: 0.6114, Val Loss: 0.5916, Accuracy: 67.73%\n",
      "Epoch [100/100], Loss: 0.6137, Val Loss: 0.5924, Accuracy: 68.05%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6218, Val Loss: 0.6134, Accuracy: 64.57%\n",
      "Epoch [20/100], Loss: 0.6128, Val Loss: 0.6100, Accuracy: 65.18%\n",
      "Epoch [30/100], Loss: 0.6130, Val Loss: 0.6084, Accuracy: 65.79%\n",
      "Epoch [40/100], Loss: 0.6097, Val Loss: 0.6094, Accuracy: 66.12%\n",
      "Epoch [50/100], Loss: 0.6083, Val Loss: 0.6079, Accuracy: 65.36%\n",
      "Epoch [60/100], Loss: 0.6096, Val Loss: 0.6078, Accuracy: 65.40%\n",
      "Epoch [70/100], Loss: 0.6083, Val Loss: 0.6080, Accuracy: 65.90%\n",
      "Epoch [80/100], Loss: 0.6079, Val Loss: 0.6077, Accuracy: 65.47%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6077, Accuracy: 65.58%\n",
      "Epoch [100/100], Loss: 0.6093, Val Loss: 0.6076, Accuracy: 65.51%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6187, Val Loss: 0.6201, Accuracy: 64.81%\n",
      "Epoch [20/100], Loss: 0.6118, Val Loss: 0.6193, Accuracy: 64.17%\n",
      "Epoch [30/100], Loss: 0.6072, Val Loss: 0.6203, Accuracy: 64.49%\n",
      "Epoch [40/100], Loss: 0.6102, Val Loss: 0.6190, Accuracy: 64.96%\n",
      "Epoch [50/100], Loss: 0.6096, Val Loss: 0.6202, Accuracy: 64.60%\n",
      "Epoch [60/100], Loss: 0.6091, Val Loss: 0.6193, Accuracy: 64.81%\n",
      "Epoch [70/100], Loss: 0.6108, Val Loss: 0.6203, Accuracy: 64.70%\n",
      "Epoch [80/100], Loss: 0.6086, Val Loss: 0.6196, Accuracy: 65.03%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.6202, Accuracy: 64.56%\n",
      "Epoch [100/100], Loss: 0.6097, Val Loss: 0.6196, Accuracy: 64.49%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6071, Accuracy: 66.28%\n",
      "Epoch [20/100], Loss: 0.6139, Val Loss: 0.6061, Accuracy: 66.25%\n",
      "Epoch [30/100], Loss: 0.6123, Val Loss: 0.6035, Accuracy: 66.18%\n",
      "Epoch [40/100], Loss: 0.6119, Val Loss: 0.6043, Accuracy: 66.21%\n",
      "Epoch [50/100], Loss: 0.6077, Val Loss: 0.6034, Accuracy: 66.43%\n",
      "Epoch [60/100], Loss: 0.6093, Val Loss: 0.6035, Accuracy: 66.03%\n",
      "Epoch [70/100], Loss: 0.6081, Val Loss: 0.6038, Accuracy: 66.50%\n",
      "Epoch [80/100], Loss: 0.6095, Val Loss: 0.6037, Accuracy: 66.36%\n",
      "Epoch [90/100], Loss: 0.6090, Val Loss: 0.6042, Accuracy: 65.49%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.6036, Accuracy: 66.36%\n",
      "Best cross-validated accuracy: 68.20%\n",
      "Epoch [10/100], Loss: 0.6082, Val Loss: 0.5964, Accuracy: 67.67%\n",
      "Epoch [20/100], Loss: 0.6068, Val Loss: 0.5971, Accuracy: 68.19%\n",
      "Epoch [30/100], Loss: 0.6068, Val Loss: 0.5972, Accuracy: 68.16%\n",
      "Epoch [40/100], Loss: 0.6069, Val Loss: 0.5968, Accuracy: 67.93%\n",
      "Epoch [50/100], Loss: 0.6072, Val Loss: 0.5969, Accuracy: 67.84%\n",
      "Epoch [60/100], Loss: 0.6073, Val Loss: 0.5968, Accuracy: 67.90%\n",
      "Epoch [70/100], Loss: 0.6061, Val Loss: 0.5970, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6076, Val Loss: 0.5972, Accuracy: 68.27%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.5969, Accuracy: 67.93%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.5968, Accuracy: 67.87%\n",
      "Final model accuracy on test set: 68.45%\n",
      "Accuracy 68.45% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6155, Val Loss: 0.6171, Accuracy: 64.90%\n",
      "Epoch [20/100], Loss: 0.6151, Val Loss: 0.6151, Accuracy: 65.36%\n",
      "Epoch [30/100], Loss: 0.6094, Val Loss: 0.6185, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6072, Val Loss: 0.6153, Accuracy: 65.51%\n",
      "Epoch [50/100], Loss: 0.6085, Val Loss: 0.6142, Accuracy: 65.43%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.6142, Accuracy: 65.69%\n",
      "Epoch [70/100], Loss: 0.6055, Val Loss: 0.6147, Accuracy: 65.29%\n",
      "Epoch [80/100], Loss: 0.6073, Val Loss: 0.6147, Accuracy: 65.36%\n",
      "Epoch [90/100], Loss: 0.6086, Val Loss: 0.6144, Accuracy: 65.47%\n",
      "Epoch [100/100], Loss: 0.6066, Val Loss: 0.6148, Accuracy: 65.54%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6217, Val Loss: 0.5939, Accuracy: 67.23%\n",
      "Epoch [20/100], Loss: 0.6218, Val Loss: 0.5932, Accuracy: 68.02%\n",
      "Epoch [30/100], Loss: 0.6167, Val Loss: 0.5937, Accuracy: 67.95%\n",
      "Epoch [40/100], Loss: 0.6190, Val Loss: 0.5924, Accuracy: 68.16%\n",
      "Epoch [50/100], Loss: 0.6153, Val Loss: 0.5910, Accuracy: 67.77%\n",
      "Epoch [60/100], Loss: 0.6154, Val Loss: 0.5946, Accuracy: 68.23%\n",
      "Epoch [70/100], Loss: 0.6161, Val Loss: 0.5923, Accuracy: 67.73%\n",
      "Epoch [80/100], Loss: 0.6163, Val Loss: 0.5915, Accuracy: 67.73%\n",
      "Epoch [90/100], Loss: 0.6151, Val Loss: 0.5932, Accuracy: 68.34%\n",
      "Epoch [100/100], Loss: 0.6160, Val Loss: 0.5918, Accuracy: 67.80%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6187, Val Loss: 0.6113, Accuracy: 65.72%\n",
      "Epoch [20/100], Loss: 0.6161, Val Loss: 0.6088, Accuracy: 65.29%\n",
      "Epoch [30/100], Loss: 0.6141, Val Loss: 0.6081, Accuracy: 65.40%\n",
      "Epoch [40/100], Loss: 0.6109, Val Loss: 0.6089, Accuracy: 66.04%\n",
      "Epoch [50/100], Loss: 0.6110, Val Loss: 0.6078, Accuracy: 65.47%\n",
      "Epoch [60/100], Loss: 0.6080, Val Loss: 0.6084, Accuracy: 65.72%\n",
      "Epoch [70/100], Loss: 0.6087, Val Loss: 0.6079, Accuracy: 65.43%\n",
      "Epoch [80/100], Loss: 0.6095, Val Loss: 0.6084, Accuracy: 65.58%\n",
      "Epoch [90/100], Loss: 0.6089, Val Loss: 0.6080, Accuracy: 65.25%\n",
      "Epoch [100/100], Loss: 0.6086, Val Loss: 0.6084, Accuracy: 65.61%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6170, Val Loss: 0.6173, Accuracy: 64.99%\n",
      "Epoch [20/100], Loss: 0.6090, Val Loss: 0.6204, Accuracy: 64.52%\n",
      "Epoch [30/100], Loss: 0.6106, Val Loss: 0.6190, Accuracy: 64.81%\n",
      "Epoch [40/100], Loss: 0.6100, Val Loss: 0.6185, Accuracy: 64.81%\n",
      "Epoch [50/100], Loss: 0.6078, Val Loss: 0.6187, Accuracy: 64.81%\n",
      "Epoch [60/100], Loss: 0.6085, Val Loss: 0.6187, Accuracy: 64.81%\n",
      "Epoch [70/100], Loss: 0.6090, Val Loss: 0.6190, Accuracy: 64.96%\n",
      "Epoch [80/100], Loss: 0.6103, Val Loss: 0.6190, Accuracy: 64.56%\n",
      "Epoch [90/100], Loss: 0.6110, Val Loss: 0.6190, Accuracy: 64.88%\n",
      "Epoch [100/100], Loss: 0.6072, Val Loss: 0.6182, Accuracy: 65.17%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6192, Val Loss: 0.6072, Accuracy: 66.14%\n",
      "Epoch [20/100], Loss: 0.6168, Val Loss: 0.6051, Accuracy: 66.07%\n",
      "Epoch [30/100], Loss: 0.6130, Val Loss: 0.6052, Accuracy: 66.28%\n",
      "Epoch [40/100], Loss: 0.6095, Val Loss: 0.6037, Accuracy: 66.50%\n",
      "Epoch [50/100], Loss: 0.6103, Val Loss: 0.6048, Accuracy: 66.00%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.6045, Accuracy: 66.61%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.6037, Accuracy: 66.39%\n",
      "Epoch [80/100], Loss: 0.6066, Val Loss: 0.6041, Accuracy: 66.43%\n",
      "Epoch [90/100], Loss: 0.6080, Val Loss: 0.6035, Accuracy: 66.61%\n",
      "Epoch [100/100], Loss: 0.6054, Val Loss: 0.6046, Accuracy: 66.39%\n",
      "Best cross-validated accuracy: 68.34%\n",
      "Epoch [10/100], Loss: 0.6100, Val Loss: 0.5988, Accuracy: 67.59%\n",
      "Epoch [20/100], Loss: 0.6104, Val Loss: 0.5974, Accuracy: 67.79%\n",
      "Epoch [30/100], Loss: 0.6092, Val Loss: 0.5975, Accuracy: 68.02%\n",
      "Epoch [40/100], Loss: 0.6063, Val Loss: 0.5976, Accuracy: 67.93%\n",
      "Epoch [50/100], Loss: 0.6088, Val Loss: 0.5981, Accuracy: 67.79%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.5977, Accuracy: 67.84%\n",
      "Epoch [70/100], Loss: 0.6078, Val Loss: 0.5976, Accuracy: 67.90%\n",
      "Epoch [80/100], Loss: 0.6063, Val Loss: 0.5975, Accuracy: 67.90%\n",
      "Epoch [90/100], Loss: 0.6092, Val Loss: 0.5974, Accuracy: 67.79%\n",
      "Epoch [100/100], Loss: 0.6092, Val Loss: 0.5976, Accuracy: 67.67%\n",
      "Final model accuracy on test set: 68.19%\n",
      "Accuracy 68.19% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6196, Val Loss: 0.6194, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6133, Val Loss: 0.6174, Accuracy: 65.08%\n",
      "Epoch [30/100], Loss: 0.6093, Val Loss: 0.6159, Accuracy: 64.61%\n",
      "Epoch [40/100], Loss: 0.6078, Val Loss: 0.6153, Accuracy: 64.93%\n",
      "Epoch [50/100], Loss: 0.6051, Val Loss: 0.6159, Accuracy: 65.54%\n",
      "Epoch [60/100], Loss: 0.6066, Val Loss: 0.6142, Accuracy: 65.36%\n",
      "Epoch [70/100], Loss: 0.6074, Val Loss: 0.6139, Accuracy: 65.00%\n",
      "Epoch [80/100], Loss: 0.6078, Val Loss: 0.6139, Accuracy: 65.15%\n",
      "Epoch [90/100], Loss: 0.6074, Val Loss: 0.6137, Accuracy: 65.33%\n",
      "Epoch [100/100], Loss: 0.6055, Val Loss: 0.6145, Accuracy: 64.57%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6251, Val Loss: 0.5975, Accuracy: 67.73%\n",
      "Epoch [20/100], Loss: 0.6214, Val Loss: 0.5943, Accuracy: 67.70%\n",
      "Epoch [30/100], Loss: 0.6161, Val Loss: 0.5927, Accuracy: 67.88%\n",
      "Epoch [40/100], Loss: 0.6140, Val Loss: 0.5938, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6140, Val Loss: 0.5912, Accuracy: 67.62%\n",
      "Epoch [60/100], Loss: 0.6126, Val Loss: 0.5915, Accuracy: 67.77%\n",
      "Epoch [70/100], Loss: 0.6091, Val Loss: 0.5899, Accuracy: 67.98%\n",
      "Epoch [80/100], Loss: 0.6092, Val Loss: 0.5897, Accuracy: 67.73%\n",
      "Epoch [90/100], Loss: 0.6108, Val Loss: 0.5902, Accuracy: 68.49%\n",
      "Epoch [100/100], Loss: 0.6112, Val Loss: 0.5906, Accuracy: 68.02%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6218, Val Loss: 0.6104, Accuracy: 65.54%\n",
      "Epoch [20/100], Loss: 0.6146, Val Loss: 0.6083, Accuracy: 65.43%\n",
      "Epoch [30/100], Loss: 0.6134, Val Loss: 0.6082, Accuracy: 65.54%\n",
      "Epoch [40/100], Loss: 0.6097, Val Loss: 0.6077, Accuracy: 65.43%\n",
      "Epoch [50/100], Loss: 0.6086, Val Loss: 0.6080, Accuracy: 65.43%\n",
      "Epoch [60/100], Loss: 0.6088, Val Loss: 0.6072, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6080, Val Loss: 0.6069, Accuracy: 65.43%\n",
      "Epoch [80/100], Loss: 0.6080, Val Loss: 0.6070, Accuracy: 65.51%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6071, Accuracy: 65.43%\n",
      "Epoch [100/100], Loss: 0.6082, Val Loss: 0.6067, Accuracy: 65.40%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6167, Val Loss: 0.6205, Accuracy: 64.88%\n",
      "Epoch [20/100], Loss: 0.6101, Val Loss: 0.6226, Accuracy: 64.52%\n",
      "Epoch [30/100], Loss: 0.6083, Val Loss: 0.6200, Accuracy: 65.17%\n",
      "Epoch [40/100], Loss: 0.6079, Val Loss: 0.6211, Accuracy: 64.31%\n",
      "Epoch [50/100], Loss: 0.6079, Val Loss: 0.6199, Accuracy: 64.60%\n",
      "Epoch [60/100], Loss: 0.6090, Val Loss: 0.6203, Accuracy: 64.49%\n",
      "Epoch [70/100], Loss: 0.6094, Val Loss: 0.6198, Accuracy: 64.85%\n",
      "Epoch [80/100], Loss: 0.6070, Val Loss: 0.6201, Accuracy: 64.31%\n",
      "Epoch [90/100], Loss: 0.6048, Val Loss: 0.6200, Accuracy: 64.88%\n",
      "Epoch [100/100], Loss: 0.6102, Val Loss: 0.6203, Accuracy: 64.49%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6201, Val Loss: 0.6086, Accuracy: 65.71%\n",
      "Epoch [20/100], Loss: 0.6167, Val Loss: 0.6060, Accuracy: 66.14%\n",
      "Epoch [30/100], Loss: 0.6128, Val Loss: 0.6059, Accuracy: 65.96%\n",
      "Epoch [40/100], Loss: 0.6137, Val Loss: 0.6043, Accuracy: 66.50%\n",
      "Epoch [50/100], Loss: 0.6102, Val Loss: 0.6033, Accuracy: 66.68%\n",
      "Epoch [60/100], Loss: 0.6083, Val Loss: 0.6042, Accuracy: 66.43%\n",
      "Epoch [70/100], Loss: 0.6044, Val Loss: 0.6038, Accuracy: 66.54%\n",
      "Epoch [80/100], Loss: 0.6069, Val Loss: 0.6032, Accuracy: 66.54%\n",
      "Epoch [90/100], Loss: 0.6053, Val Loss: 0.6031, Accuracy: 66.54%\n",
      "Epoch [100/100], Loss: 0.6056, Val Loss: 0.6030, Accuracy: 66.50%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6081, Val Loss: 0.5984, Accuracy: 68.16%\n",
      "Epoch [20/100], Loss: 0.6054, Val Loss: 0.5968, Accuracy: 68.02%\n",
      "Epoch [30/100], Loss: 0.6053, Val Loss: 0.5968, Accuracy: 67.87%\n",
      "Epoch [40/100], Loss: 0.6044, Val Loss: 0.5966, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.6059, Val Loss: 0.5972, Accuracy: 68.10%\n",
      "Epoch [60/100], Loss: 0.6046, Val Loss: 0.5968, Accuracy: 68.07%\n",
      "Epoch [70/100], Loss: 0.6048, Val Loss: 0.5967, Accuracy: 68.02%\n",
      "Epoch [80/100], Loss: 0.6050, Val Loss: 0.5967, Accuracy: 68.02%\n",
      "Epoch [90/100], Loss: 0.6040, Val Loss: 0.5966, Accuracy: 68.10%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.5967, Accuracy: 68.07%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6198, Val Loss: 0.6176, Accuracy: 65.08%\n",
      "Epoch [20/100], Loss: 0.6122, Val Loss: 0.6157, Accuracy: 65.04%\n",
      "Epoch [30/100], Loss: 0.6118, Val Loss: 0.6149, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6085, Val Loss: 0.6151, Accuracy: 65.33%\n",
      "Epoch [50/100], Loss: 0.6065, Val Loss: 0.6151, Accuracy: 65.18%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.6137, Accuracy: 65.51%\n",
      "Epoch [70/100], Loss: 0.6061, Val Loss: 0.6142, Accuracy: 65.15%\n",
      "Epoch [80/100], Loss: 0.6055, Val Loss: 0.6145, Accuracy: 65.36%\n",
      "Epoch [90/100], Loss: 0.6069, Val Loss: 0.6152, Accuracy: 65.11%\n",
      "Epoch [100/100], Loss: 0.6093, Val Loss: 0.6148, Accuracy: 65.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6210, Val Loss: 0.5944, Accuracy: 67.62%\n",
      "Epoch [20/100], Loss: 0.6184, Val Loss: 0.5934, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.6146, Val Loss: 0.5920, Accuracy: 68.16%\n",
      "Epoch [40/100], Loss: 0.6135, Val Loss: 0.5904, Accuracy: 68.34%\n",
      "Epoch [50/100], Loss: 0.6125, Val Loss: 0.5920, Accuracy: 67.98%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6194, Val Loss: 0.6124, Accuracy: 65.11%\n",
      "Epoch [20/100], Loss: 0.6178, Val Loss: 0.6093, Accuracy: 65.36%\n",
      "Epoch [30/100], Loss: 0.6128, Val Loss: 0.6101, Accuracy: 64.82%\n",
      "Epoch [40/100], Loss: 0.6111, Val Loss: 0.6077, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6091, Val Loss: 0.6083, Accuracy: 65.94%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6072, Accuracy: 66.15%\n",
      "Epoch [70/100], Loss: 0.6053, Val Loss: 0.6071, Accuracy: 65.43%\n",
      "Epoch [80/100], Loss: 0.6078, Val Loss: 0.6074, Accuracy: 65.54%\n",
      "Epoch [90/100], Loss: 0.6064, Val Loss: 0.6080, Accuracy: 65.61%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.6074, Accuracy: 65.47%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6186, Val Loss: 0.6176, Accuracy: 64.78%\n",
      "Epoch [20/100], Loss: 0.6138, Val Loss: 0.6186, Accuracy: 64.63%\n",
      "Epoch [30/100], Loss: 0.6087, Val Loss: 0.6183, Accuracy: 64.70%\n",
      "Epoch [40/100], Loss: 0.6087, Val Loss: 0.6184, Accuracy: 64.67%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6186, Accuracy: 65.10%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6179, Accuracy: 65.17%\n",
      "Epoch [70/100], Loss: 0.6096, Val Loss: 0.6187, Accuracy: 64.88%\n",
      "Epoch [80/100], Loss: 0.6088, Val Loss: 0.6184, Accuracy: 65.03%\n",
      "Epoch [90/100], Loss: 0.6111, Val Loss: 0.6177, Accuracy: 64.92%\n",
      "Epoch [100/100], Loss: 0.6090, Val Loss: 0.6183, Accuracy: 64.74%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6211, Val Loss: 0.6075, Accuracy: 65.67%\n",
      "Epoch [20/100], Loss: 0.6161, Val Loss: 0.6055, Accuracy: 66.00%\n",
      "Epoch [30/100], Loss: 0.6119, Val Loss: 0.6045, Accuracy: 66.39%\n",
      "Epoch [40/100], Loss: 0.6108, Val Loss: 0.6036, Accuracy: 66.46%\n",
      "Epoch [50/100], Loss: 0.6113, Val Loss: 0.6035, Accuracy: 66.57%\n",
      "Epoch [60/100], Loss: 0.6082, Val Loss: 0.6044, Accuracy: 66.28%\n",
      "Epoch [70/100], Loss: 0.6066, Val Loss: 0.6040, Accuracy: 66.36%\n",
      "Epoch [80/100], Loss: 0.6059, Val Loss: 0.6038, Accuracy: 66.39%\n",
      "Epoch [90/100], Loss: 0.6095, Val Loss: 0.6040, Accuracy: 66.43%\n",
      "Epoch [100/100], Loss: 0.6065, Val Loss: 0.6039, Accuracy: 66.54%\n",
      "Best cross-validated accuracy: 68.63%\n",
      "Epoch [10/100], Loss: 0.6087, Val Loss: 0.5984, Accuracy: 68.19%\n",
      "Epoch [20/100], Loss: 0.6068, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [30/100], Loss: 0.6055, Val Loss: 0.5974, Accuracy: 68.16%\n",
      "Epoch [40/100], Loss: 0.6061, Val Loss: 0.5972, Accuracy: 68.10%\n",
      "Epoch [50/100], Loss: 0.6044, Val Loss: 0.5977, Accuracy: 68.13%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.5974, Accuracy: 68.19%\n",
      "Epoch [70/100], Loss: 0.6050, Val Loss: 0.5974, Accuracy: 68.22%\n",
      "Epoch [80/100], Loss: 0.6058, Val Loss: 0.5976, Accuracy: 68.04%\n",
      "Epoch [90/100], Loss: 0.6057, Val Loss: 0.5973, Accuracy: 68.04%\n",
      "Epoch [100/100], Loss: 0.6050, Val Loss: 0.5976, Accuracy: 68.10%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6174, Val Loss: 0.6164, Accuracy: 65.36%\n",
      "Epoch [20/100], Loss: 0.6153, Val Loss: 0.6149, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6124, Val Loss: 0.6139, Accuracy: 65.69%\n",
      "Epoch [40/100], Loss: 0.6083, Val Loss: 0.6174, Accuracy: 65.00%\n",
      "Epoch [50/100], Loss: 0.6069, Val Loss: 0.6141, Accuracy: 65.51%\n",
      "Epoch [60/100], Loss: 0.6050, Val Loss: 0.6155, Accuracy: 65.40%\n",
      "Epoch [70/100], Loss: 0.6059, Val Loss: 0.6147, Accuracy: 65.61%\n",
      "Epoch [80/100], Loss: 0.6038, Val Loss: 0.6137, Accuracy: 65.47%\n",
      "Epoch [90/100], Loss: 0.6050, Val Loss: 0.6157, Accuracy: 64.75%\n",
      "Epoch [100/100], Loss: 0.6056, Val Loss: 0.6157, Accuracy: 65.43%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6252, Val Loss: 0.5944, Accuracy: 67.44%\n",
      "Epoch [20/100], Loss: 0.6179, Val Loss: 0.5940, Accuracy: 68.02%\n",
      "Epoch [30/100], Loss: 0.6161, Val Loss: 0.5922, Accuracy: 67.66%\n",
      "Epoch [40/100], Loss: 0.6130, Val Loss: 0.5914, Accuracy: 67.95%\n",
      "Epoch [50/100], Loss: 0.6129, Val Loss: 0.5915, Accuracy: 68.16%\n",
      "Epoch [60/100], Loss: 0.6158, Val Loss: 0.5902, Accuracy: 67.95%\n",
      "Epoch [70/100], Loss: 0.6126, Val Loss: 0.5909, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6133, Val Loss: 0.5915, Accuracy: 68.23%\n",
      "Epoch [90/100], Loss: 0.6149, Val Loss: 0.5915, Accuracy: 68.16%\n",
      "Epoch [100/100], Loss: 0.6155, Val Loss: 0.5914, Accuracy: 67.91%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6200, Val Loss: 0.6120, Accuracy: 64.61%\n",
      "Epoch [20/100], Loss: 0.6132, Val Loss: 0.6098, Accuracy: 65.33%\n",
      "Epoch [30/100], Loss: 0.6112, Val Loss: 0.6085, Accuracy: 65.69%\n",
      "Epoch [40/100], Loss: 0.6096, Val Loss: 0.6086, Accuracy: 65.65%\n",
      "Epoch [50/100], Loss: 0.6080, Val Loss: 0.6076, Accuracy: 65.51%\n",
      "Epoch [60/100], Loss: 0.6087, Val Loss: 0.6074, Accuracy: 65.40%\n",
      "Epoch [70/100], Loss: 0.6086, Val Loss: 0.6082, Accuracy: 65.43%\n",
      "Epoch [80/100], Loss: 0.6083, Val Loss: 0.6077, Accuracy: 65.40%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.6075, Accuracy: 65.54%\n",
      "Epoch [100/100], Loss: 0.6083, Val Loss: 0.6078, Accuracy: 65.25%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6166, Val Loss: 0.6174, Accuracy: 64.31%\n",
      "Epoch [20/100], Loss: 0.6106, Val Loss: 0.6201, Accuracy: 64.20%\n",
      "Epoch [30/100], Loss: 0.6121, Val Loss: 0.6181, Accuracy: 64.38%\n",
      "Epoch [40/100], Loss: 0.6118, Val Loss: 0.6194, Accuracy: 64.81%\n",
      "Epoch [50/100], Loss: 0.6117, Val Loss: 0.6184, Accuracy: 64.78%\n",
      "Epoch [60/100], Loss: 0.6104, Val Loss: 0.6185, Accuracy: 64.67%\n",
      "Epoch [70/100], Loss: 0.6115, Val Loss: 0.6190, Accuracy: 64.63%\n",
      "Epoch [80/100], Loss: 0.6111, Val Loss: 0.6183, Accuracy: 64.17%\n",
      "Epoch [90/100], Loss: 0.6125, Val Loss: 0.6192, Accuracy: 64.92%\n",
      "Epoch [100/100], Loss: 0.6105, Val Loss: 0.6193, Accuracy: 64.67%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6203, Val Loss: 0.6069, Accuracy: 65.89%\n",
      "Epoch [20/100], Loss: 0.6147, Val Loss: 0.6074, Accuracy: 65.78%\n",
      "Epoch [30/100], Loss: 0.6137, Val Loss: 0.6063, Accuracy: 65.85%\n",
      "Epoch [40/100], Loss: 0.6102, Val Loss: 0.6044, Accuracy: 66.28%\n",
      "Epoch [50/100], Loss: 0.6083, Val Loss: 0.6040, Accuracy: 66.18%\n",
      "Epoch [60/100], Loss: 0.6074, Val Loss: 0.6047, Accuracy: 66.57%\n",
      "Epoch [70/100], Loss: 0.6081, Val Loss: 0.6042, Accuracy: 66.21%\n",
      "Epoch [80/100], Loss: 0.6089, Val Loss: 0.6035, Accuracy: 66.43%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.6041, Accuracy: 66.36%\n",
      "Epoch [100/100], Loss: 0.6094, Val Loss: 0.6041, Accuracy: 66.14%\n",
      "Best cross-validated accuracy: 68.49%\n",
      "Epoch [10/100], Loss: 0.6131, Val Loss: 0.5982, Accuracy: 68.19%\n",
      "Epoch [20/100], Loss: 0.6092, Val Loss: 0.5975, Accuracy: 68.50%\n",
      "Epoch [30/100], Loss: 0.6074, Val Loss: 0.5971, Accuracy: 68.30%\n",
      "Epoch [40/100], Loss: 0.6088, Val Loss: 0.5969, Accuracy: 67.84%\n",
      "Final model accuracy on test set: 68.62%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.686 # 75%\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        0.0   0.216807    6.73    1.11\n",
      "1        0.0   0.451096    2.15    1.67\n",
      "2        0.0   0.696201    1.51    2.48\n",
      "3        1.0   0.373522    2.70    1.43\n",
      "4        1.0   0.737495    1.71    2.10\n",
      "...      ...        ...     ...     ...\n",
      "3478     0.0   0.278078    2.61    1.47\n",
      "3479     1.0   0.525191    1.48    2.63\n",
      "3480     0.0   0.389085    3.26    1.32\n",
      "3481     0.0   0.408328    2.35    1.58\n",
      "3482     0.0   0.588944    3.05    1.36\n",
      "\n",
      "[3483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $1 bets: 1.77 on a total # bets: 171 from a total of 3483 games\n",
      "Amount of differing favorites 0.13637668676428366\n",
      "Amount of incorrect bets : 0.22807017543859648\n",
      "Correct Bets: 0.7719298245614035\n",
      "Model % Correct : 0.6861900660350273 Vegas Correct % : 0.7025552684467413\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .75\n",
    "confidence_top_pct = 1\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct and row['Predicted'] > 1/row['A_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            total_won += row['A_Odds']-1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= 1\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            total_won += row['B_Odds']-1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= 1\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on $1 bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
