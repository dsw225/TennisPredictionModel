{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36152\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('testout2.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_glicko_rd'] <= 150) & (df['b_glicko_rd'] <= 150)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>match_num</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff_high</th>\n",
       "      <th>glicko_rating_diff_low</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_high</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_low</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-312.688126</td>\n",
       "      <td>-40.429749</td>\n",
       "      <td>1663.238130</td>\n",
       "      <td>1839.797067</td>\n",
       "      <td>69.521886</td>\n",
       "      <td>66.607302</td>\n",
       "      <td>...</td>\n",
       "      <td>-142.430173</td>\n",
       "      <td>111.103350</td>\n",
       "      <td>1514.785695</td>\n",
       "      <td>1530.449107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-121.419605</td>\n",
       "      <td>147.756632</td>\n",
       "      <td>1704.915868</td>\n",
       "      <td>1691.747354</td>\n",
       "      <td>66.531031</td>\n",
       "      <td>68.057087</td>\n",
       "      <td>...</td>\n",
       "      <td>-125.068212</td>\n",
       "      <td>125.074139</td>\n",
       "      <td>1524.307970</td>\n",
       "      <td>1524.305006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-124.132627</td>\n",
       "      <td>182.766003</td>\n",
       "      <td>1599.867737</td>\n",
       "      <td>1570.551049</td>\n",
       "      <td>70.223143</td>\n",
       "      <td>83.226172</td>\n",
       "      <td>...</td>\n",
       "      <td>-130.959262</td>\n",
       "      <td>127.522140</td>\n",
       "      <td>1508.366657</td>\n",
       "      <td>1510.085217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-226.786525</td>\n",
       "      <td>45.492588</td>\n",
       "      <td>1640.524463</td>\n",
       "      <td>1731.171432</td>\n",
       "      <td>70.112236</td>\n",
       "      <td>66.027320</td>\n",
       "      <td>...</td>\n",
       "      <td>-127.146095</td>\n",
       "      <td>125.314148</td>\n",
       "      <td>1507.539322</td>\n",
       "      <td>1508.455296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-452.711077</td>\n",
       "      <td>-158.974000</td>\n",
       "      <td>1545.639101</td>\n",
       "      <td>1851.481640</td>\n",
       "      <td>79.730461</td>\n",
       "      <td>67.138078</td>\n",
       "      <td>...</td>\n",
       "      <td>-143.077859</td>\n",
       "      <td>109.514614</td>\n",
       "      <td>1486.558404</td>\n",
       "      <td>1503.340027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  match_num  a_player_rank  b_player_rank  \\\n",
       "5373      3.0       12.0           74.0           15.0   \n",
       "5375      3.0       14.0           46.0           65.0   \n",
       "5377      3.0        8.0           89.0           95.0   \n",
       "5378      3.0        7.0           83.0           48.0   \n",
       "5380      3.0        8.0           70.0           22.0   \n",
       "\n",
       "      glicko_rating_diff_high  glicko_rating_diff_low  a_glicko_rating  \\\n",
       "5373              -312.688126              -40.429749      1663.238130   \n",
       "5375              -121.419605              147.756632      1704.915868   \n",
       "5377              -124.132627              182.766003      1599.867737   \n",
       "5378              -226.786525               45.492588      1640.524463   \n",
       "5380              -452.711077             -158.974000      1545.639101   \n",
       "\n",
       "      b_glicko_rating  a_glicko_rd  b_glicko_rd  ...  \\\n",
       "5373      1839.797067    69.521886    66.607302  ...   \n",
       "5375      1691.747354    66.531031    68.057087  ...   \n",
       "5377      1570.551049    70.223143    83.226172  ...   \n",
       "5378      1731.171432    70.112236    66.027320  ...   \n",
       "5380      1851.481640    79.730461    67.138078  ...   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_high  \\\n",
       "5373                                        -142.430173   \n",
       "5375                                        -125.068212   \n",
       "5377                                        -130.959262   \n",
       "5378                                        -127.146095   \n",
       "5380                                        -143.077859   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_low  \\\n",
       "5373                                        111.103350   \n",
       "5375                                        125.074139   \n",
       "5377                                        127.522140   \n",
       "5378                                        125.314148   \n",
       "5380                                        109.514614   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rating  \\\n",
       "5373                                1514.785695   \n",
       "5375                                1524.307970   \n",
       "5377                                1508.366657   \n",
       "5378                                1507.539322   \n",
       "5380                                1486.558404   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  a_b_win  a_odds  b_odds  \\\n",
       "5373                         1530.449107      0.0    3.59    1.28   \n",
       "5375                         1524.305006      0.0     NaN     NaN   \n",
       "5377                         1510.085217      0.0    2.29    1.59   \n",
       "5378                         1508.455296      1.0    1.54    2.40   \n",
       "5380                         1503.340027      0.0    4.44    1.19   \n",
       "\n",
       "      surface_Clay  surface_Grass  surface_Hard  \n",
       "5373           0.0            0.0           1.0  \n",
       "5375           0.0            0.0           1.0  \n",
       "5377           0.0            0.0           1.0  \n",
       "5378           0.0            0.0           1.0  \n",
       "5380           0.0            0.0           1.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6164, Val Loss: 0.6212, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6144, Val Loss: 0.6170, Accuracy: 65.00%\n",
      "Epoch [30/100], Loss: 0.6093, Val Loss: 0.6157, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6143, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6063, Val Loss: 0.6153, Accuracy: 65.36%\n",
      "Epoch [60/100], Loss: 0.6024, Val Loss: 0.6146, Accuracy: 65.65%\n",
      "Epoch [70/100], Loss: 0.6048, Val Loss: 0.6150, Accuracy: 65.36%\n",
      "Epoch [80/100], Loss: 0.6038, Val Loss: 0.6140, Accuracy: 65.25%\n",
      "Epoch [90/100], Loss: 0.6047, Val Loss: 0.6150, Accuracy: 65.36%\n",
      "Epoch [100/100], Loss: 0.6067, Val Loss: 0.6153, Accuracy: 65.00%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6216, Val Loss: 0.5925, Accuracy: 67.52%\n",
      "Epoch [20/100], Loss: 0.6172, Val Loss: 0.5928, Accuracy: 68.34%\n",
      "Epoch [30/100], Loss: 0.6165, Val Loss: 0.5914, Accuracy: 68.31%\n",
      "Epoch [40/100], Loss: 0.6148, Val Loss: 0.5911, Accuracy: 68.38%\n",
      "Epoch [50/100], Loss: 0.6155, Val Loss: 0.5910, Accuracy: 67.91%\n",
      "Epoch [60/100], Loss: 0.6129, Val Loss: 0.5906, Accuracy: 68.13%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6187, Val Loss: 0.6149, Accuracy: 65.15%\n",
      "Epoch [20/100], Loss: 0.6118, Val Loss: 0.6098, Accuracy: 65.08%\n",
      "Epoch [30/100], Loss: 0.6117, Val Loss: 0.6079, Accuracy: 65.58%\n",
      "Epoch [40/100], Loss: 0.6105, Val Loss: 0.6075, Accuracy: 65.51%\n",
      "Epoch [50/100], Loss: 0.6090, Val Loss: 0.6071, Accuracy: 65.76%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6083, Accuracy: 65.79%\n",
      "Epoch [70/100], Loss: 0.6052, Val Loss: 0.6071, Accuracy: 65.58%\n",
      "Epoch [80/100], Loss: 0.6058, Val Loss: 0.6072, Accuracy: 65.83%\n",
      "Epoch [90/100], Loss: 0.6065, Val Loss: 0.6068, Accuracy: 65.72%\n",
      "Epoch [100/100], Loss: 0.6086, Val Loss: 0.6071, Accuracy: 65.69%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6175, Val Loss: 0.6191, Accuracy: 64.78%\n",
      "Epoch [20/100], Loss: 0.6120, Val Loss: 0.6198, Accuracy: 64.49%\n",
      "Epoch [30/100], Loss: 0.6103, Val Loss: 0.6192, Accuracy: 64.96%\n",
      "Epoch [40/100], Loss: 0.6128, Val Loss: 0.6194, Accuracy: 64.06%\n",
      "Epoch [50/100], Loss: 0.6099, Val Loss: 0.6199, Accuracy: 64.78%\n",
      "Epoch [60/100], Loss: 0.6123, Val Loss: 0.6220, Accuracy: 63.99%\n",
      "Epoch [70/100], Loss: 0.6099, Val Loss: 0.6211, Accuracy: 64.17%\n",
      "Epoch [80/100], Loss: 0.6114, Val Loss: 0.6194, Accuracy: 64.70%\n",
      "Epoch [90/100], Loss: 0.6101, Val Loss: 0.6200, Accuracy: 64.67%\n",
      "Epoch [100/100], Loss: 0.6111, Val Loss: 0.6193, Accuracy: 64.49%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6215, Val Loss: 0.6093, Accuracy: 65.46%\n",
      "Epoch [20/100], Loss: 0.6148, Val Loss: 0.6063, Accuracy: 66.21%\n",
      "Epoch [30/100], Loss: 0.6129, Val Loss: 0.6046, Accuracy: 66.28%\n",
      "Epoch [40/100], Loss: 0.6120, Val Loss: 0.6047, Accuracy: 65.85%\n",
      "Epoch [50/100], Loss: 0.6102, Val Loss: 0.6058, Accuracy: 66.54%\n",
      "Epoch [60/100], Loss: 0.6076, Val Loss: 0.6030, Accuracy: 66.07%\n",
      "Epoch [70/100], Loss: 0.6041, Val Loss: 0.6028, Accuracy: 66.36%\n",
      "Epoch [80/100], Loss: 0.6063, Val Loss: 0.6029, Accuracy: 66.61%\n",
      "Epoch [90/100], Loss: 0.6061, Val Loss: 0.6028, Accuracy: 66.50%\n",
      "Epoch [100/100], Loss: 0.6054, Val Loss: 0.6026, Accuracy: 66.32%\n",
      "Best cross-validated accuracy: 68.52%\n",
      "Epoch [10/100], Loss: 0.6099, Val Loss: 0.5999, Accuracy: 68.25%\n",
      "Epoch [20/100], Loss: 0.6093, Val Loss: 0.5978, Accuracy: 67.93%\n",
      "Epoch [30/100], Loss: 0.6093, Val Loss: 0.5975, Accuracy: 67.99%\n",
      "Epoch [40/100], Loss: 0.6077, Val Loss: 0.5974, Accuracy: 67.96%\n",
      "Epoch [50/100], Loss: 0.6063, Val Loss: 0.5977, Accuracy: 68.13%\n",
      "Epoch [60/100], Loss: 0.6058, Val Loss: 0.5976, Accuracy: 67.93%\n",
      "Epoch [70/100], Loss: 0.6067, Val Loss: 0.5975, Accuracy: 68.02%\n",
      "Epoch [80/100], Loss: 0.6073, Val Loss: 0.5978, Accuracy: 68.07%\n",
      "Epoch [90/100], Loss: 0.6075, Val Loss: 0.5975, Accuracy: 68.10%\n",
      "Epoch [100/100], Loss: 0.6067, Val Loss: 0.5973, Accuracy: 68.04%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6184, Val Loss: 0.6148, Accuracy: 64.93%\n",
      "Epoch [20/100], Loss: 0.6114, Val Loss: 0.6155, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6104, Val Loss: 0.6150, Accuracy: 65.08%\n",
      "Epoch [40/100], Loss: 0.6087, Val Loss: 0.6159, Accuracy: 65.08%\n",
      "Epoch [50/100], Loss: 0.6084, Val Loss: 0.6149, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6076, Val Loss: 0.6153, Accuracy: 65.15%\n",
      "Epoch [70/100], Loss: 0.6081, Val Loss: 0.6150, Accuracy: 65.25%\n",
      "Epoch [80/100], Loss: 0.6082, Val Loss: 0.6152, Accuracy: 65.11%\n",
      "Epoch [90/100], Loss: 0.6085, Val Loss: 0.6158, Accuracy: 65.04%\n",
      "Epoch [100/100], Loss: 0.6109, Val Loss: 0.6155, Accuracy: 65.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6242, Val Loss: 0.5925, Accuracy: 67.41%\n",
      "Epoch [20/100], Loss: 0.6195, Val Loss: 0.5909, Accuracy: 67.84%\n",
      "Epoch [30/100], Loss: 0.6188, Val Loss: 0.5906, Accuracy: 68.20%\n",
      "Epoch [40/100], Loss: 0.6140, Val Loss: 0.5905, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6137, Val Loss: 0.5938, Accuracy: 68.20%\n",
      "Epoch [60/100], Loss: 0.6135, Val Loss: 0.5905, Accuracy: 67.84%\n",
      "Epoch [70/100], Loss: 0.6123, Val Loss: 0.5922, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6136, Val Loss: 0.5904, Accuracy: 67.66%\n",
      "Epoch [90/100], Loss: 0.6141, Val Loss: 0.5908, Accuracy: 67.91%\n",
      "Epoch [100/100], Loss: 0.6135, Val Loss: 0.5919, Accuracy: 67.62%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6216, Val Loss: 0.6109, Accuracy: 65.36%\n",
      "Epoch [20/100], Loss: 0.6153, Val Loss: 0.6092, Accuracy: 65.36%\n",
      "Epoch [30/100], Loss: 0.6113, Val Loss: 0.6094, Accuracy: 65.47%\n",
      "Epoch [40/100], Loss: 0.6120, Val Loss: 0.6081, Accuracy: 65.61%\n",
      "Epoch [50/100], Loss: 0.6099, Val Loss: 0.6079, Accuracy: 65.72%\n",
      "Epoch [60/100], Loss: 0.6117, Val Loss: 0.6079, Accuracy: 65.79%\n",
      "Epoch [70/100], Loss: 0.6095, Val Loss: 0.6080, Accuracy: 65.65%\n",
      "Epoch [80/100], Loss: 0.6089, Val Loss: 0.6085, Accuracy: 65.36%\n",
      "Epoch [90/100], Loss: 0.6094, Val Loss: 0.6079, Accuracy: 65.97%\n",
      "Epoch [100/100], Loss: 0.6088, Val Loss: 0.6084, Accuracy: 65.33%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6193, Val Loss: 0.6205, Accuracy: 65.39%\n",
      "Epoch [20/100], Loss: 0.6130, Val Loss: 0.6213, Accuracy: 64.60%\n",
      "Epoch [30/100], Loss: 0.6104, Val Loss: 0.6196, Accuracy: 64.92%\n",
      "Epoch [40/100], Loss: 0.6084, Val Loss: 0.6198, Accuracy: 64.31%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6199, Accuracy: 64.99%\n",
      "Epoch [60/100], Loss: 0.6074, Val Loss: 0.6196, Accuracy: 65.28%\n",
      "Epoch [70/100], Loss: 0.6098, Val Loss: 0.6197, Accuracy: 65.13%\n",
      "Epoch [80/100], Loss: 0.6086, Val Loss: 0.6193, Accuracy: 64.74%\n",
      "Epoch [90/100], Loss: 0.6098, Val Loss: 0.6199, Accuracy: 64.78%\n",
      "Epoch [100/100], Loss: 0.6073, Val Loss: 0.6202, Accuracy: 65.24%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6061, Accuracy: 66.28%\n",
      "Epoch [20/100], Loss: 0.6147, Val Loss: 0.6068, Accuracy: 66.28%\n",
      "Epoch [30/100], Loss: 0.6135, Val Loss: 0.6054, Accuracy: 66.43%\n",
      "Epoch [40/100], Loss: 0.6130, Val Loss: 0.6046, Accuracy: 66.57%\n",
      "Epoch [50/100], Loss: 0.6104, Val Loss: 0.6045, Accuracy: 66.50%\n",
      "Epoch [60/100], Loss: 0.6120, Val Loss: 0.6046, Accuracy: 66.28%\n",
      "Epoch [70/100], Loss: 0.6101, Val Loss: 0.6054, Accuracy: 66.39%\n",
      "Epoch [80/100], Loss: 0.6120, Val Loss: 0.6052, Accuracy: 66.43%\n",
      "Epoch [90/100], Loss: 0.6110, Val Loss: 0.6042, Accuracy: 66.39%\n",
      "Epoch [100/100], Loss: 0.6114, Val Loss: 0.6047, Accuracy: 66.36%\n",
      "Best cross-validated accuracy: 68.27%\n",
      "Epoch [10/100], Loss: 0.6113, Val Loss: 0.5991, Accuracy: 67.53%\n",
      "Epoch [20/100], Loss: 0.6088, Val Loss: 0.5986, Accuracy: 67.99%\n",
      "Epoch [30/100], Loss: 0.6069, Val Loss: 0.5976, Accuracy: 67.99%\n",
      "Epoch [40/100], Loss: 0.6065, Val Loss: 0.5980, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6066, Val Loss: 0.5981, Accuracy: 67.73%\n",
      "Epoch [60/100], Loss: 0.6069, Val Loss: 0.5980, Accuracy: 67.70%\n",
      "Epoch [70/100], Loss: 0.6066, Val Loss: 0.5977, Accuracy: 67.82%\n",
      "Epoch [80/100], Loss: 0.6062, Val Loss: 0.5980, Accuracy: 67.79%\n",
      "Epoch [90/100], Loss: 0.6072, Val Loss: 0.5980, Accuracy: 67.79%\n",
      "Epoch [100/100], Loss: 0.6066, Val Loss: 0.5979, Accuracy: 67.73%\n",
      "Final model accuracy on test set: 68.19%\n",
      "Accuracy 68.19% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6180, Val Loss: 0.6169, Accuracy: 64.72%\n",
      "Epoch [20/100], Loss: 0.6165, Val Loss: 0.6152, Accuracy: 65.18%\n",
      "Epoch [30/100], Loss: 0.6111, Val Loss: 0.6142, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6086, Val Loss: 0.6150, Accuracy: 65.43%\n",
      "Epoch [50/100], Loss: 0.6091, Val Loss: 0.6156, Accuracy: 65.29%\n",
      "Epoch [60/100], Loss: 0.6080, Val Loss: 0.6150, Accuracy: 65.36%\n",
      "Epoch [70/100], Loss: 0.6062, Val Loss: 0.6147, Accuracy: 65.08%\n",
      "Epoch [80/100], Loss: 0.6058, Val Loss: 0.6150, Accuracy: 65.22%\n",
      "Epoch [90/100], Loss: 0.6062, Val Loss: 0.6152, Accuracy: 65.00%\n",
      "Epoch [100/100], Loss: 0.6099, Val Loss: 0.6144, Accuracy: 65.51%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6266, Val Loss: 0.5950, Accuracy: 67.12%\n",
      "Epoch [20/100], Loss: 0.6184, Val Loss: 0.5956, Accuracy: 68.02%\n",
      "Epoch [30/100], Loss: 0.6186, Val Loss: 0.5934, Accuracy: 67.59%\n",
      "Epoch [40/100], Loss: 0.6107, Val Loss: 0.5936, Accuracy: 67.84%\n",
      "Epoch [50/100], Loss: 0.6167, Val Loss: 0.5976, Accuracy: 67.73%\n",
      "Epoch [60/100], Loss: 0.6182, Val Loss: 0.5940, Accuracy: 67.70%\n",
      "Epoch [70/100], Loss: 0.6177, Val Loss: 0.5938, Accuracy: 67.95%\n",
      "Epoch [80/100], Loss: 0.6156, Val Loss: 0.5938, Accuracy: 67.70%\n",
      "Epoch [90/100], Loss: 0.6160, Val Loss: 0.5936, Accuracy: 67.41%\n",
      "Epoch [100/100], Loss: 0.6177, Val Loss: 0.5938, Accuracy: 67.84%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6216, Val Loss: 0.6117, Accuracy: 65.00%\n",
      "Epoch [20/100], Loss: 0.6149, Val Loss: 0.6093, Accuracy: 65.61%\n",
      "Epoch [30/100], Loss: 0.6117, Val Loss: 0.6091, Accuracy: 65.11%\n",
      "Epoch [40/100], Loss: 0.6105, Val Loss: 0.6085, Accuracy: 65.33%\n",
      "Epoch [50/100], Loss: 0.6094, Val Loss: 0.6080, Accuracy: 65.18%\n",
      "Epoch [60/100], Loss: 0.6079, Val Loss: 0.6087, Accuracy: 65.58%\n",
      "Epoch [70/100], Loss: 0.6060, Val Loss: 0.6080, Accuracy: 65.51%\n",
      "Epoch [80/100], Loss: 0.6061, Val Loss: 0.6079, Accuracy: 65.25%\n",
      "Epoch [90/100], Loss: 0.6082, Val Loss: 0.6080, Accuracy: 65.29%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.6082, Accuracy: 65.22%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6174, Val Loss: 0.6232, Accuracy: 64.56%\n",
      "Epoch [20/100], Loss: 0.6134, Val Loss: 0.6185, Accuracy: 65.28%\n",
      "Epoch [30/100], Loss: 0.6116, Val Loss: 0.6186, Accuracy: 64.56%\n",
      "Epoch [40/100], Loss: 0.6106, Val Loss: 0.6189, Accuracy: 64.96%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6188, Accuracy: 64.85%\n",
      "Epoch [60/100], Loss: 0.6103, Val Loss: 0.6200, Accuracy: 64.70%\n",
      "Epoch [70/100], Loss: 0.6137, Val Loss: 0.6190, Accuracy: 64.78%\n",
      "Epoch [80/100], Loss: 0.6100, Val Loss: 0.6197, Accuracy: 64.67%\n",
      "Epoch [90/100], Loss: 0.6121, Val Loss: 0.6189, Accuracy: 65.24%\n",
      "Epoch [100/100], Loss: 0.6107, Val Loss: 0.6196, Accuracy: 65.13%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6230, Val Loss: 0.6105, Accuracy: 65.82%\n",
      "Epoch [20/100], Loss: 0.6143, Val Loss: 0.6036, Accuracy: 66.46%\n",
      "Epoch [30/100], Loss: 0.6144, Val Loss: 0.6039, Accuracy: 65.85%\n",
      "Epoch [40/100], Loss: 0.6103, Val Loss: 0.6023, Accuracy: 66.32%\n",
      "Epoch [50/100], Loss: 0.6102, Val Loss: 0.6034, Accuracy: 66.10%\n",
      "Epoch [60/100], Loss: 0.6104, Val Loss: 0.6032, Accuracy: 66.57%\n",
      "Epoch [70/100], Loss: 0.6106, Val Loss: 0.6024, Accuracy: 65.92%\n",
      "Epoch [80/100], Loss: 0.6092, Val Loss: 0.6026, Accuracy: 66.57%\n",
      "Epoch [90/100], Loss: 0.6096, Val Loss: 0.6026, Accuracy: 66.00%\n",
      "Epoch [100/100], Loss: 0.6080, Val Loss: 0.6024, Accuracy: 66.14%\n",
      "Best cross-validated accuracy: 68.23%\n",
      "Epoch [10/100], Loss: 0.6115, Val Loss: 0.5980, Accuracy: 68.16%\n",
      "Epoch [20/100], Loss: 0.6073, Val Loss: 0.5975, Accuracy: 67.76%\n",
      "Epoch [30/100], Loss: 0.6062, Val Loss: 0.6007, Accuracy: 67.99%\n",
      "Epoch [40/100], Loss: 0.6066, Val Loss: 0.5977, Accuracy: 68.07%\n",
      "Epoch [50/100], Loss: 0.6045, Val Loss: 0.5979, Accuracy: 67.96%\n",
      "Epoch [60/100], Loss: 0.6063, Val Loss: 0.5981, Accuracy: 68.07%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.5976, Accuracy: 67.96%\n",
      "Epoch [80/100], Loss: 0.6061, Val Loss: 0.5978, Accuracy: 68.07%\n",
      "Epoch [90/100], Loss: 0.6048, Val Loss: 0.5978, Accuracy: 67.99%\n",
      "Epoch [100/100], Loss: 0.6056, Val Loss: 0.5981, Accuracy: 68.07%\n",
      "Final model accuracy on test set: 68.39%\n",
      "Accuracy 68.39% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6190, Val Loss: 0.6146, Accuracy: 65.04%\n",
      "Epoch [20/100], Loss: 0.6145, Val Loss: 0.6152, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6099, Val Loss: 0.6154, Accuracy: 65.04%\n",
      "Epoch [40/100], Loss: 0.6123, Val Loss: 0.6150, Accuracy: 65.22%\n",
      "Epoch [50/100], Loss: 0.6119, Val Loss: 0.6151, Accuracy: 65.29%\n",
      "Epoch [60/100], Loss: 0.6116, Val Loss: 0.6160, Accuracy: 65.00%\n",
      "Epoch [70/100], Loss: 0.6096, Val Loss: 0.6155, Accuracy: 65.33%\n",
      "Epoch [80/100], Loss: 0.6104, Val Loss: 0.6142, Accuracy: 64.97%\n",
      "Epoch [90/100], Loss: 0.6096, Val Loss: 0.6155, Accuracy: 64.93%\n",
      "Epoch [100/100], Loss: 0.6116, Val Loss: 0.6164, Accuracy: 65.18%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6250, Val Loss: 0.5944, Accuracy: 66.94%\n",
      "Epoch [20/100], Loss: 0.6160, Val Loss: 0.5930, Accuracy: 67.98%\n",
      "Epoch [30/100], Loss: 0.6147, Val Loss: 0.5932, Accuracy: 67.84%\n",
      "Epoch [40/100], Loss: 0.6153, Val Loss: 0.5927, Accuracy: 67.52%\n",
      "Epoch [50/100], Loss: 0.6171, Val Loss: 0.5933, Accuracy: 67.26%\n",
      "Epoch [60/100], Loss: 0.6148, Val Loss: 0.5928, Accuracy: 67.59%\n",
      "Epoch [70/100], Loss: 0.6139, Val Loss: 0.5914, Accuracy: 67.88%\n",
      "Epoch [80/100], Loss: 0.6151, Val Loss: 0.5938, Accuracy: 68.31%\n",
      "Epoch [90/100], Loss: 0.6148, Val Loss: 0.5935, Accuracy: 67.52%\n",
      "Epoch [100/100], Loss: 0.6166, Val Loss: 0.5932, Accuracy: 68.13%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6206, Val Loss: 0.6120, Accuracy: 65.00%\n",
      "Epoch [20/100], Loss: 0.6168, Val Loss: 0.6110, Accuracy: 65.51%\n",
      "Epoch [30/100], Loss: 0.6106, Val Loss: 0.6088, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6118, Val Loss: 0.6087, Accuracy: 65.58%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6082, Accuracy: 65.47%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.6079, Accuracy: 65.76%\n",
      "Epoch [70/100], Loss: 0.6091, Val Loss: 0.6078, Accuracy: 65.87%\n",
      "Epoch [80/100], Loss: 0.6083, Val Loss: 0.6085, Accuracy: 65.29%\n",
      "Epoch [90/100], Loss: 0.6102, Val Loss: 0.6078, Accuracy: 65.76%\n",
      "Epoch [100/100], Loss: 0.6107, Val Loss: 0.6079, Accuracy: 65.54%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6178, Val Loss: 0.6193, Accuracy: 64.81%\n",
      "Epoch [20/100], Loss: 0.6113, Val Loss: 0.6204, Accuracy: 64.56%\n",
      "Epoch [30/100], Loss: 0.6101, Val Loss: 0.6207, Accuracy: 64.56%\n",
      "Epoch [40/100], Loss: 0.6117, Val Loss: 0.6206, Accuracy: 64.24%\n",
      "Epoch [50/100], Loss: 0.6125, Val Loss: 0.6210, Accuracy: 64.63%\n",
      "Epoch [60/100], Loss: 0.6089, Val Loss: 0.6193, Accuracy: 64.63%\n",
      "Epoch [70/100], Loss: 0.6107, Val Loss: 0.6194, Accuracy: 65.24%\n",
      "Epoch [80/100], Loss: 0.6133, Val Loss: 0.6207, Accuracy: 64.88%\n",
      "Epoch [90/100], Loss: 0.6097, Val Loss: 0.6221, Accuracy: 64.34%\n",
      "Epoch [100/100], Loss: 0.6091, Val Loss: 0.6210, Accuracy: 64.60%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6193, Val Loss: 0.6073, Accuracy: 65.75%\n",
      "Epoch [20/100], Loss: 0.6159, Val Loss: 0.6069, Accuracy: 65.71%\n",
      "Epoch [30/100], Loss: 0.6122, Val Loss: 0.6049, Accuracy: 65.85%\n",
      "Epoch [40/100], Loss: 0.6102, Val Loss: 0.6043, Accuracy: 66.36%\n",
      "Epoch [50/100], Loss: 0.6097, Val Loss: 0.6048, Accuracy: 66.07%\n",
      "Epoch [60/100], Loss: 0.6092, Val Loss: 0.6038, Accuracy: 65.78%\n",
      "Epoch [70/100], Loss: 0.6073, Val Loss: 0.6043, Accuracy: 66.21%\n",
      "Epoch [80/100], Loss: 0.6068, Val Loss: 0.6032, Accuracy: 66.21%\n",
      "Epoch [90/100], Loss: 0.6073, Val Loss: 0.6033, Accuracy: 65.67%\n",
      "Epoch [100/100], Loss: 0.6065, Val Loss: 0.6032, Accuracy: 66.46%\n",
      "Best cross-validated accuracy: 68.41%\n",
      "Epoch [10/100], Loss: 0.6114, Val Loss: 0.5970, Accuracy: 68.02%\n",
      "Epoch [20/100], Loss: 0.6085, Val Loss: 0.5977, Accuracy: 68.19%\n",
      "Epoch [30/100], Loss: 0.6045, Val Loss: 0.5970, Accuracy: 68.10%\n",
      "Epoch [40/100], Loss: 0.6055, Val Loss: 0.5975, Accuracy: 67.99%\n",
      "Epoch [50/100], Loss: 0.6050, Val Loss: 0.5973, Accuracy: 67.99%\n",
      "Epoch [60/100], Loss: 0.6084, Val Loss: 0.5973, Accuracy: 67.90%\n",
      "Epoch [70/100], Loss: 0.6052, Val Loss: 0.5976, Accuracy: 68.10%\n",
      "Epoch [80/100], Loss: 0.6068, Val Loss: 0.5973, Accuracy: 67.90%\n",
      "Epoch [90/100], Loss: 0.6049, Val Loss: 0.5974, Accuracy: 67.87%\n",
      "Epoch [100/100], Loss: 0.6057, Val Loss: 0.5973, Accuracy: 67.99%\n",
      "Final model accuracy on test set: 68.42%\n",
      "Accuracy 68.42% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6196, Val Loss: 0.6171, Accuracy: 64.61%\n",
      "Epoch [20/100], Loss: 0.6131, Val Loss: 0.6157, Accuracy: 65.15%\n",
      "Epoch [30/100], Loss: 0.6095, Val Loss: 0.6142, Accuracy: 65.15%\n",
      "Epoch [40/100], Loss: 0.6102, Val Loss: 0.6150, Accuracy: 65.33%\n",
      "Epoch [50/100], Loss: 0.6089, Val Loss: 0.6135, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6096, Val Loss: 0.6145, Accuracy: 65.36%\n",
      "Epoch [70/100], Loss: 0.6082, Val Loss: 0.6144, Accuracy: 65.51%\n",
      "Epoch [80/100], Loss: 0.6106, Val Loss: 0.6151, Accuracy: 64.93%\n",
      "Epoch [90/100], Loss: 0.6104, Val Loss: 0.6144, Accuracy: 64.90%\n",
      "Epoch [100/100], Loss: 0.6081, Val Loss: 0.6140, Accuracy: 65.40%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6229, Val Loss: 0.5934, Accuracy: 67.16%\n",
      "Epoch [20/100], Loss: 0.6178, Val Loss: 0.5921, Accuracy: 67.73%\n",
      "Epoch [30/100], Loss: 0.6163, Val Loss: 0.5934, Accuracy: 67.34%\n",
      "Epoch [40/100], Loss: 0.6154, Val Loss: 0.5913, Accuracy: 68.27%\n",
      "Epoch [50/100], Loss: 0.6150, Val Loss: 0.5927, Accuracy: 67.77%\n",
      "Epoch [60/100], Loss: 0.6101, Val Loss: 0.5919, Accuracy: 68.02%\n",
      "Epoch [70/100], Loss: 0.6115, Val Loss: 0.5918, Accuracy: 67.26%\n",
      "Epoch [80/100], Loss: 0.6107, Val Loss: 0.5938, Accuracy: 67.84%\n",
      "Epoch [90/100], Loss: 0.6124, Val Loss: 0.5913, Accuracy: 67.80%\n",
      "Epoch [100/100], Loss: 0.6113, Val Loss: 0.5916, Accuracy: 68.09%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6218, Val Loss: 0.6116, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6139, Val Loss: 0.6085, Accuracy: 65.94%\n",
      "Epoch [30/100], Loss: 0.6110, Val Loss: 0.6081, Accuracy: 65.69%\n",
      "Epoch [40/100], Loss: 0.6094, Val Loss: 0.6074, Accuracy: 65.97%\n",
      "Epoch [50/100], Loss: 0.6085, Val Loss: 0.6080, Accuracy: 65.72%\n",
      "Epoch [60/100], Loss: 0.6083, Val Loss: 0.6075, Accuracy: 65.54%\n",
      "Epoch [70/100], Loss: 0.6086, Val Loss: 0.6077, Accuracy: 65.72%\n",
      "Epoch [80/100], Loss: 0.6090, Val Loss: 0.6072, Accuracy: 65.65%\n",
      "Epoch [90/100], Loss: 0.6078, Val Loss: 0.6074, Accuracy: 65.87%\n",
      "Epoch [100/100], Loss: 0.6060, Val Loss: 0.6077, Accuracy: 65.76%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6161, Val Loss: 0.6182, Accuracy: 65.17%\n",
      "Epoch [20/100], Loss: 0.6112, Val Loss: 0.6199, Accuracy: 64.96%\n",
      "Epoch [30/100], Loss: 0.6058, Val Loss: 0.6199, Accuracy: 64.67%\n",
      "Epoch [40/100], Loss: 0.6055, Val Loss: 0.6193, Accuracy: 64.60%\n",
      "Epoch [50/100], Loss: 0.6090, Val Loss: 0.6198, Accuracy: 64.85%\n",
      "Epoch [60/100], Loss: 0.6077, Val Loss: 0.6190, Accuracy: 64.63%\n",
      "Epoch [70/100], Loss: 0.6082, Val Loss: 0.6197, Accuracy: 64.92%\n",
      "Epoch [80/100], Loss: 0.6076, Val Loss: 0.6204, Accuracy: 64.74%\n",
      "Epoch [90/100], Loss: 0.6068, Val Loss: 0.6191, Accuracy: 64.70%\n",
      "Epoch [100/100], Loss: 0.6080, Val Loss: 0.6201, Accuracy: 64.45%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6184, Val Loss: 0.6062, Accuracy: 66.32%\n",
      "Epoch [20/100], Loss: 0.6163, Val Loss: 0.6058, Accuracy: 66.18%\n",
      "Epoch [30/100], Loss: 0.6140, Val Loss: 0.6054, Accuracy: 66.14%\n",
      "Epoch [40/100], Loss: 0.6122, Val Loss: 0.6048, Accuracy: 65.71%\n",
      "Epoch [50/100], Loss: 0.6124, Val Loss: 0.6044, Accuracy: 66.39%\n",
      "Epoch [60/100], Loss: 0.6083, Val Loss: 0.6048, Accuracy: 66.54%\n",
      "Epoch [70/100], Loss: 0.6103, Val Loss: 0.6049, Accuracy: 66.00%\n",
      "Epoch [80/100], Loss: 0.6099, Val Loss: 0.6053, Accuracy: 66.28%\n",
      "Epoch [90/100], Loss: 0.6109, Val Loss: 0.6043, Accuracy: 66.50%\n",
      "Epoch [100/100], Loss: 0.6101, Val Loss: 0.6042, Accuracy: 66.32%\n",
      "Best cross-validated accuracy: 68.49%\n",
      "Epoch [10/100], Loss: 0.6099, Val Loss: 0.5988, Accuracy: 67.79%\n",
      "Epoch [20/100], Loss: 0.6082, Val Loss: 0.5975, Accuracy: 67.67%\n",
      "Epoch [30/100], Loss: 0.6066, Val Loss: 0.5975, Accuracy: 67.96%\n",
      "Epoch [40/100], Loss: 0.6068, Val Loss: 0.5980, Accuracy: 67.93%\n",
      "Epoch [50/100], Loss: 0.6060, Val Loss: 0.5977, Accuracy: 67.87%\n",
      "Epoch [60/100], Loss: 0.6060, Val Loss: 0.5977, Accuracy: 67.96%\n",
      "Epoch [70/100], Loss: 0.6057, Val Loss: 0.5975, Accuracy: 67.84%\n",
      "Epoch [80/100], Loss: 0.6090, Val Loss: 0.5978, Accuracy: 67.99%\n",
      "Epoch [90/100], Loss: 0.6066, Val Loss: 0.5977, Accuracy: 67.96%\n",
      "Epoch [100/100], Loss: 0.6050, Val Loss: 0.5981, Accuracy: 67.96%\n",
      "Final model accuracy on test set: 68.27%\n",
      "Accuracy 68.27% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6179, Val Loss: 0.6174, Accuracy: 64.93%\n",
      "Epoch [20/100], Loss: 0.6120, Val Loss: 0.6145, Accuracy: 65.29%\n",
      "Epoch [30/100], Loss: 0.6103, Val Loss: 0.6145, Accuracy: 65.15%\n",
      "Epoch [40/100], Loss: 0.6113, Val Loss: 0.6140, Accuracy: 65.25%\n",
      "Epoch [50/100], Loss: 0.6102, Val Loss: 0.6146, Accuracy: 65.00%\n",
      "Epoch [60/100], Loss: 0.6086, Val Loss: 0.6142, Accuracy: 65.18%\n",
      "Epoch [70/100], Loss: 0.6096, Val Loss: 0.6142, Accuracy: 65.43%\n",
      "Epoch [80/100], Loss: 0.6102, Val Loss: 0.6143, Accuracy: 65.25%\n",
      "Epoch [90/100], Loss: 0.6118, Val Loss: 0.6147, Accuracy: 65.18%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6145, Accuracy: 65.15%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.5934, Accuracy: 67.30%\n",
      "Epoch [20/100], Loss: 0.6171, Val Loss: 0.5967, Accuracy: 67.23%\n",
      "Epoch [30/100], Loss: 0.6158, Val Loss: 0.5938, Accuracy: 67.66%\n",
      "Epoch [40/100], Loss: 0.6148, Val Loss: 0.5914, Accuracy: 68.13%\n",
      "Epoch [50/100], Loss: 0.6098, Val Loss: 0.5906, Accuracy: 68.09%\n",
      "Epoch [60/100], Loss: 0.6128, Val Loss: 0.5932, Accuracy: 67.91%\n",
      "Epoch [70/100], Loss: 0.6105, Val Loss: 0.5901, Accuracy: 67.80%\n",
      "Epoch [80/100], Loss: 0.6109, Val Loss: 0.5909, Accuracy: 67.77%\n",
      "Epoch [90/100], Loss: 0.6131, Val Loss: 0.5924, Accuracy: 67.91%\n",
      "Epoch [100/100], Loss: 0.6107, Val Loss: 0.5905, Accuracy: 68.09%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6172, Val Loss: 0.6091, Accuracy: 65.65%\n",
      "Epoch [20/100], Loss: 0.6175, Val Loss: 0.6085, Accuracy: 65.51%\n",
      "Epoch [30/100], Loss: 0.6140, Val Loss: 0.6101, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6121, Val Loss: 0.6087, Accuracy: 65.79%\n",
      "Epoch [50/100], Loss: 0.6080, Val Loss: 0.6079, Accuracy: 65.61%\n",
      "Epoch [60/100], Loss: 0.6092, Val Loss: 0.6082, Accuracy: 65.69%\n",
      "Epoch [70/100], Loss: 0.6081, Val Loss: 0.6080, Accuracy: 65.69%\n",
      "Epoch [80/100], Loss: 0.6084, Val Loss: 0.6076, Accuracy: 65.65%\n",
      "Epoch [90/100], Loss: 0.6068, Val Loss: 0.6079, Accuracy: 65.76%\n",
      "Epoch [100/100], Loss: 0.6079, Val Loss: 0.6076, Accuracy: 65.69%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6242, Val Loss: 0.6198, Accuracy: 64.20%\n",
      "Epoch [20/100], Loss: 0.6148, Val Loss: 0.6217, Accuracy: 64.67%\n",
      "Epoch [30/100], Loss: 0.6084, Val Loss: 0.6197, Accuracy: 64.67%\n",
      "Epoch [40/100], Loss: 0.6066, Val Loss: 0.6204, Accuracy: 64.13%\n",
      "Epoch [50/100], Loss: 0.6081, Val Loss: 0.6199, Accuracy: 64.60%\n",
      "Epoch [60/100], Loss: 0.6089, Val Loss: 0.6207, Accuracy: 63.88%\n",
      "Epoch [70/100], Loss: 0.6075, Val Loss: 0.6219, Accuracy: 63.59%\n",
      "Epoch [80/100], Loss: 0.6090, Val Loss: 0.6214, Accuracy: 64.24%\n",
      "Epoch [90/100], Loss: 0.6077, Val Loss: 0.6204, Accuracy: 64.13%\n",
      "Epoch [100/100], Loss: 0.6078, Val Loss: 0.6218, Accuracy: 64.49%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6237, Val Loss: 0.6084, Accuracy: 65.64%\n",
      "Epoch [20/100], Loss: 0.6180, Val Loss: 0.6059, Accuracy: 66.00%\n",
      "Epoch [30/100], Loss: 0.6152, Val Loss: 0.6054, Accuracy: 66.32%\n",
      "Epoch [40/100], Loss: 0.6109, Val Loss: 0.6042, Accuracy: 66.86%\n",
      "Epoch [50/100], Loss: 0.6087, Val Loss: 0.6043, Accuracy: 66.21%\n",
      "Epoch [60/100], Loss: 0.6097, Val Loss: 0.6037, Accuracy: 66.50%\n",
      "Epoch [70/100], Loss: 0.6102, Val Loss: 0.6048, Accuracy: 66.25%\n",
      "Epoch [80/100], Loss: 0.6073, Val Loss: 0.6037, Accuracy: 66.18%\n",
      "Epoch [90/100], Loss: 0.6103, Val Loss: 0.6038, Accuracy: 66.39%\n",
      "Epoch [100/100], Loss: 0.6084, Val Loss: 0.6040, Accuracy: 66.21%\n",
      "Best cross-validated accuracy: 68.31%\n",
      "Epoch [10/100], Loss: 0.6092, Val Loss: 0.5977, Accuracy: 67.79%\n",
      "Epoch [20/100], Loss: 0.6075, Val Loss: 0.5978, Accuracy: 68.04%\n",
      "Epoch [30/100], Loss: 0.6061, Val Loss: 0.5966, Accuracy: 67.73%\n",
      "Epoch [40/100], Loss: 0.6039, Val Loss: 0.5973, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6050, Val Loss: 0.5972, Accuracy: 67.67%\n",
      "Epoch [60/100], Loss: 0.6051, Val Loss: 0.5973, Accuracy: 67.87%\n",
      "Epoch [70/100], Loss: 0.6045, Val Loss: 0.5973, Accuracy: 67.79%\n",
      "Epoch [80/100], Loss: 0.6034, Val Loss: 0.5972, Accuracy: 67.76%\n",
      "Epoch [90/100], Loss: 0.6027, Val Loss: 0.5970, Accuracy: 67.76%\n",
      "Epoch [100/100], Loss: 0.6033, Val Loss: 0.5970, Accuracy: 67.59%\n",
      "Final model accuracy on test set: 68.36%\n",
      "Accuracy 68.36% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6200, Val Loss: 0.6154, Accuracy: 65.22%\n",
      "Epoch [20/100], Loss: 0.6143, Val Loss: 0.6173, Accuracy: 64.86%\n",
      "Epoch [30/100], Loss: 0.6104, Val Loss: 0.6147, Accuracy: 65.29%\n",
      "Epoch [40/100], Loss: 0.6107, Val Loss: 0.6153, Accuracy: 64.82%\n",
      "Epoch [50/100], Loss: 0.6117, Val Loss: 0.6138, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6140, Accuracy: 65.15%\n",
      "Epoch [70/100], Loss: 0.6082, Val Loss: 0.6156, Accuracy: 65.22%\n",
      "Epoch [80/100], Loss: 0.6078, Val Loss: 0.6153, Accuracy: 65.11%\n",
      "Epoch [90/100], Loss: 0.6100, Val Loss: 0.6148, Accuracy: 65.15%\n",
      "Epoch [100/100], Loss: 0.6089, Val Loss: 0.6151, Accuracy: 65.08%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6251, Val Loss: 0.5944, Accuracy: 67.70%\n",
      "Epoch [20/100], Loss: 0.6157, Val Loss: 0.5909, Accuracy: 67.26%\n",
      "Epoch [30/100], Loss: 0.6164, Val Loss: 0.5905, Accuracy: 67.41%\n",
      "Epoch [40/100], Loss: 0.6123, Val Loss: 0.5928, Accuracy: 68.05%\n",
      "Epoch [50/100], Loss: 0.6139, Val Loss: 0.5919, Accuracy: 67.91%\n",
      "Epoch [60/100], Loss: 0.6150, Val Loss: 0.5921, Accuracy: 68.05%\n",
      "Epoch [70/100], Loss: 0.6123, Val Loss: 0.5921, Accuracy: 68.13%\n",
      "Epoch [80/100], Loss: 0.6121, Val Loss: 0.5915, Accuracy: 67.98%\n",
      "Epoch [90/100], Loss: 0.6143, Val Loss: 0.5918, Accuracy: 67.77%\n",
      "Epoch [100/100], Loss: 0.6116, Val Loss: 0.5922, Accuracy: 68.27%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6219, Val Loss: 0.6122, Accuracy: 64.82%\n",
      "Epoch [20/100], Loss: 0.6165, Val Loss: 0.6105, Accuracy: 64.75%\n",
      "Epoch [30/100], Loss: 0.6121, Val Loss: 0.6092, Accuracy: 65.11%\n",
      "Epoch [40/100], Loss: 0.6107, Val Loss: 0.6089, Accuracy: 65.61%\n",
      "Epoch [50/100], Loss: 0.6095, Val Loss: 0.6075, Accuracy: 65.40%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6086, Accuracy: 65.83%\n",
      "Epoch [70/100], Loss: 0.6079, Val Loss: 0.6071, Accuracy: 65.15%\n",
      "Epoch [80/100], Loss: 0.6070, Val Loss: 0.6071, Accuracy: 65.90%\n",
      "Epoch [90/100], Loss: 0.6027, Val Loss: 0.6073, Accuracy: 65.47%\n",
      "Epoch [100/100], Loss: 0.6044, Val Loss: 0.6070, Accuracy: 65.69%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6204, Val Loss: 0.6188, Accuracy: 64.81%\n",
      "Epoch [20/100], Loss: 0.6117, Val Loss: 0.6190, Accuracy: 64.42%\n",
      "Epoch [30/100], Loss: 0.6107, Val Loss: 0.6183, Accuracy: 64.63%\n",
      "Epoch [40/100], Loss: 0.6104, Val Loss: 0.6206, Accuracy: 64.17%\n",
      "Epoch [50/100], Loss: 0.6105, Val Loss: 0.6202, Accuracy: 64.20%\n",
      "Epoch [60/100], Loss: 0.6101, Val Loss: 0.6201, Accuracy: 63.81%\n",
      "Epoch [70/100], Loss: 0.6088, Val Loss: 0.6197, Accuracy: 64.45%\n",
      "Epoch [80/100], Loss: 0.6084, Val Loss: 0.6190, Accuracy: 64.74%\n",
      "Epoch [90/100], Loss: 0.6110, Val Loss: 0.6188, Accuracy: 64.96%\n",
      "Epoch [100/100], Loss: 0.6073, Val Loss: 0.6186, Accuracy: 64.81%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6225, Val Loss: 0.6081, Accuracy: 65.42%\n",
      "Epoch [20/100], Loss: 0.6166, Val Loss: 0.6075, Accuracy: 65.57%\n",
      "Epoch [30/100], Loss: 0.6136, Val Loss: 0.6045, Accuracy: 65.78%\n",
      "Epoch [40/100], Loss: 0.6113, Val Loss: 0.6043, Accuracy: 66.43%\n",
      "Epoch [50/100], Loss: 0.6101, Val Loss: 0.6041, Accuracy: 66.50%\n",
      "Epoch [60/100], Loss: 0.6082, Val Loss: 0.6072, Accuracy: 65.96%\n",
      "Epoch [70/100], Loss: 0.6052, Val Loss: 0.6043, Accuracy: 66.03%\n",
      "Epoch [80/100], Loss: 0.6052, Val Loss: 0.6040, Accuracy: 66.18%\n",
      "Epoch [90/100], Loss: 0.6073, Val Loss: 0.6041, Accuracy: 66.07%\n",
      "Epoch [100/100], Loss: 0.6063, Val Loss: 0.6032, Accuracy: 66.25%\n",
      "Best cross-validated accuracy: 68.34%\n",
      "Epoch [10/100], Loss: 0.6087, Val Loss: 0.5985, Accuracy: 67.70%\n",
      "Epoch [20/100], Loss: 0.6091, Val Loss: 0.5989, Accuracy: 67.84%\n",
      "Epoch [30/100], Loss: 0.6049, Val Loss: 0.5978, Accuracy: 67.99%\n",
      "Epoch [40/100], Loss: 0.6058, Val Loss: 0.5976, Accuracy: 67.73%\n",
      "Epoch [50/100], Loss: 0.6043, Val Loss: 0.5977, Accuracy: 67.82%\n",
      "Epoch [60/100], Loss: 0.6061, Val Loss: 0.5976, Accuracy: 67.99%\n",
      "Epoch [70/100], Loss: 0.6049, Val Loss: 0.5974, Accuracy: 67.82%\n",
      "Epoch [80/100], Loss: 0.6057, Val Loss: 0.5977, Accuracy: 67.96%\n",
      "Epoch [90/100], Loss: 0.6062, Val Loss: 0.5979, Accuracy: 67.84%\n",
      "Epoch [100/100], Loss: 0.6064, Val Loss: 0.5974, Accuracy: 67.82%\n",
      "Final model accuracy on test set: 68.45%\n",
      "Accuracy 68.45% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6197, Val Loss: 0.6179, Accuracy: 65.15%\n",
      "Epoch [20/100], Loss: 0.6119, Val Loss: 0.6167, Accuracy: 65.51%\n",
      "Epoch [30/100], Loss: 0.6085, Val Loss: 0.6165, Accuracy: 65.18%\n",
      "Epoch [40/100], Loss: 0.6076, Val Loss: 0.6154, Accuracy: 65.40%\n",
      "Epoch [50/100], Loss: 0.6089, Val Loss: 0.6149, Accuracy: 65.47%\n",
      "Epoch [60/100], Loss: 0.6082, Val Loss: 0.6152, Accuracy: 65.11%\n",
      "Epoch [70/100], Loss: 0.6078, Val Loss: 0.6154, Accuracy: 65.11%\n",
      "Epoch [80/100], Loss: 0.6075, Val Loss: 0.6153, Accuracy: 65.36%\n",
      "Epoch [90/100], Loss: 0.6072, Val Loss: 0.6162, Accuracy: 64.90%\n",
      "Epoch [100/100], Loss: 0.6077, Val Loss: 0.6157, Accuracy: 64.97%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6265, Val Loss: 0.5941, Accuracy: 67.48%\n",
      "Epoch [20/100], Loss: 0.6175, Val Loss: 0.5913, Accuracy: 68.09%\n",
      "Epoch [30/100], Loss: 0.6169, Val Loss: 0.5921, Accuracy: 68.20%\n",
      "Epoch [40/100], Loss: 0.6139, Val Loss: 0.5911, Accuracy: 68.23%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6182, Val Loss: 0.6119, Accuracy: 65.33%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6100, Accuracy: 65.90%\n",
      "Epoch [30/100], Loss: 0.6120, Val Loss: 0.6097, Accuracy: 65.22%\n",
      "Epoch [40/100], Loss: 0.6090, Val Loss: 0.6079, Accuracy: 65.87%\n",
      "Epoch [50/100], Loss: 0.6090, Val Loss: 0.6079, Accuracy: 65.25%\n",
      "Epoch [60/100], Loss: 0.6081, Val Loss: 0.6079, Accuracy: 65.25%\n",
      "Epoch [70/100], Loss: 0.6065, Val Loss: 0.6087, Accuracy: 65.47%\n",
      "Epoch [80/100], Loss: 0.6078, Val Loss: 0.6092, Accuracy: 66.08%\n",
      "Epoch [90/100], Loss: 0.6078, Val Loss: 0.6074, Accuracy: 65.61%\n",
      "Epoch [100/100], Loss: 0.6085, Val Loss: 0.6079, Accuracy: 65.83%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6143, Val Loss: 0.6194, Accuracy: 64.70%\n",
      "Epoch [20/100], Loss: 0.6116, Val Loss: 0.6184, Accuracy: 64.60%\n",
      "Epoch [30/100], Loss: 0.6106, Val Loss: 0.6190, Accuracy: 64.17%\n",
      "Epoch [40/100], Loss: 0.6091, Val Loss: 0.6178, Accuracy: 64.56%\n",
      "Epoch [50/100], Loss: 0.6104, Val Loss: 0.6177, Accuracy: 64.92%\n",
      "Epoch [60/100], Loss: 0.6083, Val Loss: 0.6183, Accuracy: 64.92%\n",
      "Epoch [70/100], Loss: 0.6100, Val Loss: 0.6180, Accuracy: 65.06%\n",
      "Epoch [80/100], Loss: 0.6098, Val Loss: 0.6187, Accuracy: 64.24%\n",
      "Epoch [90/100], Loss: 0.6088, Val Loss: 0.6181, Accuracy: 65.13%\n",
      "Epoch [100/100], Loss: 0.6085, Val Loss: 0.6182, Accuracy: 64.78%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6196, Val Loss: 0.6081, Accuracy: 65.35%\n",
      "Epoch [20/100], Loss: 0.6147, Val Loss: 0.6043, Accuracy: 66.25%\n",
      "Epoch [30/100], Loss: 0.6116, Val Loss: 0.6039, Accuracy: 66.43%\n",
      "Epoch [40/100], Loss: 0.6074, Val Loss: 0.6035, Accuracy: 66.50%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6042, Accuracy: 66.32%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6032, Accuracy: 66.50%\n",
      "Epoch [70/100], Loss: 0.6095, Val Loss: 0.6033, Accuracy: 65.89%\n",
      "Epoch [80/100], Loss: 0.6071, Val Loss: 0.6026, Accuracy: 66.50%\n",
      "Epoch [90/100], Loss: 0.6083, Val Loss: 0.6028, Accuracy: 66.36%\n",
      "Epoch [100/100], Loss: 0.6070, Val Loss: 0.6029, Accuracy: 66.39%\n",
      "Best cross-validated accuracy: 68.56%\n",
      "Epoch [10/100], Loss: 0.6087, Val Loss: 0.5976, Accuracy: 68.07%\n",
      "Final model accuracy on test set: 68.59%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.685 # 75%\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        0.0   0.211723    6.73    1.11\n",
      "1        0.0   0.460307    2.15    1.67\n",
      "2        0.0   0.711039    1.51    2.48\n",
      "3        1.0   0.358161    2.70    1.43\n",
      "4        1.0   0.740098    1.71    2.10\n",
      "...      ...        ...     ...     ...\n",
      "3478     0.0   0.244573    2.61    1.47\n",
      "3479     1.0   0.507649    1.48    2.63\n",
      "3480     0.0   0.341957    3.26    1.32\n",
      "3481     0.0   0.412864    2.35    1.58\n",
      "3482     0.0   0.550580    3.05    1.36\n",
      "\n",
      "[3483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $1 bets: -7.05 on a total # bets: 1133 from a total of 3483 games\n",
      "Amount of differing favorites 0.1424059718633362\n",
      "Amount of incorrect bets : 0.4263018534863195\n",
      "Correct Bets: 0.5736981465136805\n",
      "Model % Correct : 0.6859029572207866 Vegas Correct % : 0.7025552684467413\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .5\n",
    "confidence_top_pct = 1\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct and row['Predicted'] > 1/row['A_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            total_won += row['A_Odds']-1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= 1\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            total_won += row['B_Odds']-1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= 1\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on $1 bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
