{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20120101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36157\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('testcsvs/glickoalltest.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "df = df[(df['a_glicko_rd'] <= 150) & (df['b_glicko_rd'] <= 150)]\n",
    "\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug', 'sets', 'games', 'tiebreaks'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff_high</th>\n",
       "      <th>glicko_rating_diff_low</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>point_glicko_rating_diff_high</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_high</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_low</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>a_b_win</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-311.617231</td>\n",
       "      <td>-40.704658</td>\n",
       "      <td>1663.254111</td>\n",
       "      <td>1839.415055</td>\n",
       "      <td>69.063534</td>\n",
       "      <td>66.392752</td>\n",
       "      <td>-145.321147</td>\n",
       "      <td>...</td>\n",
       "      <td>-141.831083</td>\n",
       "      <td>110.643651</td>\n",
       "      <td>1514.815163</td>\n",
       "      <td>1530.408879</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-121.020149</td>\n",
       "      <td>147.093176</td>\n",
       "      <td>1704.618439</td>\n",
       "      <td>1691.581925</td>\n",
       "      <td>66.272335</td>\n",
       "      <td>67.784328</td>\n",
       "      <td>-124.052595</td>\n",
       "      <td>...</td>\n",
       "      <td>-124.710330</td>\n",
       "      <td>124.671282</td>\n",
       "      <td>1524.265289</td>\n",
       "      <td>1524.284813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>3.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-181.835201</td>\n",
       "      <td>123.629608</td>\n",
       "      <td>1570.250330</td>\n",
       "      <td>1599.353126</td>\n",
       "      <td>82.839194</td>\n",
       "      <td>69.893210</td>\n",
       "      <td>-124.286662</td>\n",
       "      <td>...</td>\n",
       "      <td>-126.885213</td>\n",
       "      <td>130.276738</td>\n",
       "      <td>1510.001918</td>\n",
       "      <td>1508.306155</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-225.802369</td>\n",
       "      <td>45.128121</td>\n",
       "      <td>1640.535963</td>\n",
       "      <td>1730.873087</td>\n",
       "      <td>69.682012</td>\n",
       "      <td>65.783232</td>\n",
       "      <td>-130.435050</td>\n",
       "      <td>...</td>\n",
       "      <td>-126.689605</td>\n",
       "      <td>124.858464</td>\n",
       "      <td>1507.533713</td>\n",
       "      <td>1508.449284</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-452.184818</td>\n",
       "      <td>-159.378373</td>\n",
       "      <td>1545.426123</td>\n",
       "      <td>1851.207719</td>\n",
       "      <td>79.493396</td>\n",
       "      <td>66.909826</td>\n",
       "      <td>-168.130661</td>\n",
       "      <td>...</td>\n",
       "      <td>-142.624208</td>\n",
       "      <td>109.143893</td>\n",
       "      <td>1486.596303</td>\n",
       "      <td>1503.336460</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      best_of  a_player_rank  b_player_rank  glicko_rating_diff_high  \\\n",
       "5373      3.0           74.0           15.0              -311.617231   \n",
       "5375      3.0           46.0           65.0              -121.020149   \n",
       "5377      3.0           95.0           89.0              -181.835201   \n",
       "5378      3.0           83.0           48.0              -225.802369   \n",
       "5380      3.0           70.0           22.0              -452.184818   \n",
       "\n",
       "      glicko_rating_diff_low  a_glicko_rating  b_glicko_rating  a_glicko_rd  \\\n",
       "5373              -40.704658      1663.254111      1839.415055    69.063534   \n",
       "5375              147.093176      1704.618439      1691.581925    66.272335   \n",
       "5377              123.629608      1570.250330      1599.353126    82.839194   \n",
       "5378               45.128121      1640.535963      1730.873087    69.682012   \n",
       "5380             -159.378373      1545.426123      1851.207719    79.493396   \n",
       "\n",
       "      b_glicko_rd  point_glicko_rating_diff_high  ...  \\\n",
       "5373    66.392752                    -145.321147  ...   \n",
       "5375    67.784328                    -124.052595  ...   \n",
       "5377    69.893210                    -124.286662  ...   \n",
       "5378    65.783232                    -130.435050  ...   \n",
       "5380    66.909826                    -168.130661  ...   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_high  \\\n",
       "5373                                        -141.831083   \n",
       "5375                                        -124.710330   \n",
       "5377                                        -126.885213   \n",
       "5378                                        -126.689605   \n",
       "5380                                        -142.624208   \n",
       "\n",
       "      surface_return_second_won_glicko_rating_diff_low  \\\n",
       "5373                                        110.643651   \n",
       "5375                                        124.671282   \n",
       "5377                                        130.276738   \n",
       "5378                                        124.858464   \n",
       "5380                                        109.143893   \n",
       "\n",
       "      a_surface_return_second_won_glicko_rating  \\\n",
       "5373                                1514.815163   \n",
       "5375                                1524.265289   \n",
       "5377                                1510.001918   \n",
       "5378                                1507.533713   \n",
       "5380                                1486.596303   \n",
       "\n",
       "      b_surface_second_won_glicko_rating  a_odds  b_odds  a_b_win  \\\n",
       "5373                         1530.408879    3.59    1.28      0.0   \n",
       "5375                         1524.284813     NaN     NaN      0.0   \n",
       "5377                         1508.306155    1.59    2.29      1.0   \n",
       "5378                         1508.449284    1.54    2.40      1.0   \n",
       "5380                         1503.336460    4.44    1.19      0.0   \n",
       "\n",
       "      surface_Clay  surface_Grass  surface_Hard  \n",
       "5373           0.0            0.0           1.0  \n",
       "5375           0.0            0.0           1.0  \n",
       "5377           0.0            0.0           1.0  \n",
       "5378           0.0            0.0           1.0  \n",
       "5380           0.0            0.0           1.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.shortcut = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn(self.fc(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x + residual\n",
    "\n",
    "class OutcomeProbabilityV6(nn.Module):\n",
    "    def __init__(self, input_dim=172, dropout_prob=0.33):\n",
    "        hidden_dim1=input_dim * 2\n",
    "        hidden_dim2=input_dim\n",
    "        hidden_dim3=round(input_dim/2) \n",
    "        super(OutcomeProbabilityV6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(hidden_dim1, hidden_dim1)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim1, hidden_dim2)\n",
    "        self.res_block3 = ResidualBlock(hidden_dim2, hidden_dim3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, epochs=100, learning_rate=0.0001, weight_decay=1e-5, target_accuracy=0.75):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_weights = None\n",
    "    early_stopping_patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while best_acc < target_accuracy and current_epoch < epochs:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                predicted = y_pred.round()\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        acc = correct / total\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "        if (current_epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{current_epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {acc*100:.2f}%')\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def cross_validate(model_class, X, y, folds=5, epochs=100, batch_size=128, target_accuracy=0.75):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    best_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{folds}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_data = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(input_dim=X.shape[1])\n",
    "        fold_acc, fold_weights = train_and_evaluate(model, train_loader, val_loader, epochs=epochs, target_accuracy=target_accuracy)\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model_weights = fold_weights\n",
    "\n",
    "    return best_acc, best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6201, Val Loss: 0.6134, Accuracy: 65.55%\n",
      "Epoch [20/100], Loss: 0.6157, Val Loss: 0.6139, Accuracy: 65.02%\n",
      "Epoch [30/100], Loss: 0.6135, Val Loss: 0.6127, Accuracy: 65.34%\n",
      "Epoch [40/100], Loss: 0.6111, Val Loss: 0.6130, Accuracy: 65.34%\n",
      "Epoch [50/100], Loss: 0.6119, Val Loss: 0.6127, Accuracy: 65.52%\n",
      "Epoch [60/100], Loss: 0.6104, Val Loss: 0.6138, Accuracy: 65.41%\n",
      "Epoch [70/100], Loss: 0.6118, Val Loss: 0.6126, Accuracy: 65.27%\n",
      "Epoch [80/100], Loss: 0.6102, Val Loss: 0.6125, Accuracy: 65.20%\n",
      "Epoch [90/100], Loss: 0.6107, Val Loss: 0.6130, Accuracy: 65.16%\n",
      "Epoch [100/100], Loss: 0.6119, Val Loss: 0.6125, Accuracy: 65.66%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6201, Val Loss: 0.6162, Accuracy: 65.45%\n",
      "Epoch [20/100], Loss: 0.6162, Val Loss: 0.6143, Accuracy: 65.73%\n",
      "Epoch [30/100], Loss: 0.6102, Val Loss: 0.6148, Accuracy: 65.91%\n",
      "Epoch [40/100], Loss: 0.6105, Val Loss: 0.6141, Accuracy: 66.06%\n",
      "Epoch [50/100], Loss: 0.6100, Val Loss: 0.6143, Accuracy: 65.95%\n",
      "Epoch [60/100], Loss: 0.6126, Val Loss: 0.6140, Accuracy: 66.06%\n",
      "Epoch [70/100], Loss: 0.6126, Val Loss: 0.6146, Accuracy: 66.06%\n",
      "Epoch [80/100], Loss: 0.6131, Val Loss: 0.6143, Accuracy: 65.70%\n",
      "Epoch [90/100], Loss: 0.6124, Val Loss: 0.6145, Accuracy: 65.98%\n",
      "Epoch [100/100], Loss: 0.6122, Val Loss: 0.6142, Accuracy: 66.06%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6251, Val Loss: 0.6087, Accuracy: 65.88%\n",
      "Epoch [20/100], Loss: 0.6188, Val Loss: 0.6077, Accuracy: 64.87%\n",
      "Epoch [30/100], Loss: 0.6133, Val Loss: 0.6059, Accuracy: 64.80%\n",
      "Epoch [40/100], Loss: 0.6138, Val Loss: 0.6062, Accuracy: 64.94%\n",
      "Epoch [50/100], Loss: 0.6136, Val Loss: 0.6054, Accuracy: 65.09%\n",
      "Epoch [60/100], Loss: 0.6146, Val Loss: 0.6055, Accuracy: 65.55%\n",
      "Epoch [70/100], Loss: 0.6134, Val Loss: 0.6062, Accuracy: 64.80%\n",
      "Epoch [80/100], Loss: 0.6144, Val Loss: 0.6057, Accuracy: 65.05%\n",
      "Epoch [90/100], Loss: 0.6150, Val Loss: 0.6057, Accuracy: 64.94%\n",
      "Epoch [100/100], Loss: 0.6129, Val Loss: 0.6055, Accuracy: 65.55%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6237, Val Loss: 0.6054, Accuracy: 65.55%\n",
      "Epoch [20/100], Loss: 0.6171, Val Loss: 0.6062, Accuracy: 66.06%\n",
      "Epoch [30/100], Loss: 0.6152, Val Loss: 0.6042, Accuracy: 66.16%\n",
      "Epoch [40/100], Loss: 0.6116, Val Loss: 0.6027, Accuracy: 66.67%\n",
      "Epoch [50/100], Loss: 0.6132, Val Loss: 0.6042, Accuracy: 66.09%\n",
      "Epoch [60/100], Loss: 0.6117, Val Loss: 0.6034, Accuracy: 66.70%\n",
      "Epoch [70/100], Loss: 0.6096, Val Loss: 0.6033, Accuracy: 66.88%\n",
      "Epoch [80/100], Loss: 0.6093, Val Loss: 0.6029, Accuracy: 66.67%\n",
      "Epoch [90/100], Loss: 0.6099, Val Loss: 0.6027, Accuracy: 66.56%\n",
      "Epoch [100/100], Loss: 0.6106, Val Loss: 0.6030, Accuracy: 66.59%\n",
      "Fold 5/5\n",
      "Epoch [10/100], Loss: 0.6222, Val Loss: 0.6154, Accuracy: 64.94%\n",
      "Epoch [20/100], Loss: 0.6161, Val Loss: 0.6130, Accuracy: 65.59%\n",
      "Epoch [30/100], Loss: 0.6127, Val Loss: 0.6115, Accuracy: 65.27%\n",
      "Epoch [40/100], Loss: 0.6116, Val Loss: 0.6153, Accuracy: 64.59%\n",
      "Epoch [50/100], Loss: 0.6123, Val Loss: 0.6132, Accuracy: 65.37%\n",
      "Epoch [60/100], Loss: 0.6097, Val Loss: 0.6129, Accuracy: 65.05%\n",
      "Epoch [70/100], Loss: 0.6102, Val Loss: 0.6129, Accuracy: 65.02%\n",
      "Epoch [80/100], Loss: 0.6120, Val Loss: 0.6129, Accuracy: 64.76%\n",
      "Epoch [90/100], Loss: 0.6121, Val Loss: 0.6139, Accuracy: 65.45%\n",
      "Epoch [100/100], Loss: 0.6114, Val Loss: 0.6129, Accuracy: 65.23%\n",
      "Best cross-validated accuracy: 67.03%\n",
      "Epoch [10/100], Loss: 0.6091, Val Loss: 0.5921, Accuracy: 67.37%\n",
      "Epoch [20/100], Loss: 0.6086, Val Loss: 0.5964, Accuracy: 67.51%\n",
      "Epoch [30/100], Loss: 0.6065, Val Loss: 0.5923, Accuracy: 67.85%\n",
      "Epoch [40/100], Loss: 0.6049, Val Loss: 0.5932, Accuracy: 67.62%\n",
      "Epoch [50/100], Loss: 0.6046, Val Loss: 0.5933, Accuracy: 67.57%\n",
      "Epoch [60/100], Loss: 0.6056, Val Loss: 0.5931, Accuracy: 67.68%\n",
      "Epoch [70/100], Loss: 0.6043, Val Loss: 0.5935, Accuracy: 67.51%\n",
      "Epoch [80/100], Loss: 0.6064, Val Loss: 0.5935, Accuracy: 67.37%\n",
      "Epoch [90/100], Loss: 0.6051, Val Loss: 0.5932, Accuracy: 67.42%\n",
      "Epoch [100/100], Loss: 0.6073, Val Loss: 0.5936, Accuracy: 67.51%\n",
      "Final model accuracy on test set: 68.23%\n",
      "Accuracy 68.23% not met, restarting the process.\n",
      "Fold 1/5\n",
      "Epoch [10/100], Loss: 0.6197, Val Loss: 0.6130, Accuracy: 65.77%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6132, Accuracy: 65.81%\n",
      "Epoch [30/100], Loss: 0.6109, Val Loss: 0.6143, Accuracy: 65.05%\n",
      "Epoch [40/100], Loss: 0.6129, Val Loss: 0.6136, Accuracy: 65.23%\n",
      "Epoch [50/100], Loss: 0.6118, Val Loss: 0.6134, Accuracy: 65.27%\n",
      "Epoch [60/100], Loss: 0.6122, Val Loss: 0.6135, Accuracy: 65.05%\n",
      "Epoch [70/100], Loss: 0.6110, Val Loss: 0.6136, Accuracy: 65.23%\n",
      "Epoch [80/100], Loss: 0.6116, Val Loss: 0.6137, Accuracy: 65.20%\n",
      "Epoch [90/100], Loss: 0.6111, Val Loss: 0.6135, Accuracy: 65.16%\n",
      "Epoch [100/100], Loss: 0.6095, Val Loss: 0.6135, Accuracy: 65.20%\n",
      "Fold 2/5\n",
      "Epoch [10/100], Loss: 0.6230, Val Loss: 0.6165, Accuracy: 64.80%\n",
      "Epoch [20/100], Loss: 0.6147, Val Loss: 0.6164, Accuracy: 65.73%\n",
      "Epoch [30/100], Loss: 0.6128, Val Loss: 0.6151, Accuracy: 65.91%\n",
      "Epoch [40/100], Loss: 0.6110, Val Loss: 0.6150, Accuracy: 65.70%\n",
      "Epoch [50/100], Loss: 0.6097, Val Loss: 0.6152, Accuracy: 66.42%\n",
      "Epoch [60/100], Loss: 0.6094, Val Loss: 0.6152, Accuracy: 65.52%\n",
      "Epoch [70/100], Loss: 0.6090, Val Loss: 0.6142, Accuracy: 65.91%\n",
      "Epoch [80/100], Loss: 0.6094, Val Loss: 0.6143, Accuracy: 65.81%\n",
      "Epoch [90/100], Loss: 0.6098, Val Loss: 0.6145, Accuracy: 65.91%\n",
      "Epoch [100/100], Loss: 0.6072, Val Loss: 0.6147, Accuracy: 65.88%\n",
      "Fold 3/5\n",
      "Epoch [10/100], Loss: 0.6222, Val Loss: 0.6086, Accuracy: 65.30%\n",
      "Epoch [20/100], Loss: 0.6142, Val Loss: 0.6069, Accuracy: 64.76%\n",
      "Epoch [30/100], Loss: 0.6155, Val Loss: 0.6055, Accuracy: 65.34%\n",
      "Epoch [40/100], Loss: 0.6145, Val Loss: 0.6061, Accuracy: 64.98%\n",
      "Epoch [50/100], Loss: 0.6119, Val Loss: 0.6060, Accuracy: 64.98%\n",
      "Epoch [60/100], Loss: 0.6105, Val Loss: 0.6067, Accuracy: 64.91%\n",
      "Epoch [70/100], Loss: 0.6123, Val Loss: 0.6058, Accuracy: 64.98%\n",
      "Epoch [80/100], Loss: 0.6114, Val Loss: 0.6068, Accuracy: 65.16%\n",
      "Epoch [90/100], Loss: 0.6135, Val Loss: 0.6055, Accuracy: 65.12%\n",
      "Epoch [100/100], Loss: 0.6114, Val Loss: 0.6060, Accuracy: 64.91%\n",
      "Fold 4/5\n",
      "Epoch [10/100], Loss: 0.6214, Val Loss: 0.6076, Accuracy: 65.45%\n",
      "Epoch [20/100], Loss: 0.6180, Val Loss: 0.6036, Accuracy: 66.49%\n",
      "Epoch [30/100], Loss: 0.6140, Val Loss: 0.6042, Accuracy: 66.45%\n",
      "Epoch [40/100], Loss: 0.6109, Val Loss: 0.6043, Accuracy: 66.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m final_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m final_acc \u001b[38;5;241m<\u001b[39m target_accuracy:\n\u001b[0;32m---> 21\u001b[0m     best_acc, best_model_weights \u001b[38;5;241m=\u001b[39m cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, target_accuracy\u001b[38;5;241m=\u001b[39mtarget_accuracy)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validated accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Final training on full training data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 23\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(model_class, X, y, folds, epochs, batch_size, target_accuracy)\u001b[0m\n\u001b[1;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(input_dim\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m fold_acc, fold_weights \u001b[38;5;241m=\u001b[39m train_and_evaluate(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39mepochs, target_accuracy\u001b[38;5;241m=\u001b[39mtarget_accuracy)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fold_acc \u001b[38;5;241m>\u001b[39m best_acc:\n\u001b[1;32m     25\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m fold_acc\n",
      "Cell \u001b[0;32mIn[65], line 30\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, weight_decay, target_accuracy)\u001b[0m\n\u001b[1;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     31\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     32\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "y = df['a_b_win'].values\n",
    "X = df.drop('a_b_win', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set target accuracy\n",
    "target_accuracy = 0.687 # 75%\n",
    "\n",
    "final_acc = 0.0\n",
    "while final_acc < target_accuracy:\n",
    "    best_acc, best_model_weights = cross_validate(OutcomeProbabilityV6, X_train, y_train, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Best cross-validated accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # Final training on full training data\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    val_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = OutcomeProbabilityV6(input_dim=X.shape[1])\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    final_acc, _ = train_and_evaluate(model, train_loader, val_loader, epochs=100, target_accuracy=target_accuracy)\n",
    "    print(f\"Final model accuracy on test set: {final_acc*100:.2f}%\")\n",
    "\n",
    "    if final_acc < target_accuracy:\n",
    "        print(f\"Accuracy {final_acc*100:.2f}% not met, restarting the process.\")\n",
    "\n",
    "# Evaluate and store predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    odds_test = odds_df.loc[idx_test].values\n",
    "\n",
    "    # Create DataFrame with predictions and actual values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_np.flatten(),\n",
    "        'Predicted': y_pred.flatten(),\n",
    "        'A_Odds': odds_test[:, 0],\n",
    "        'B_Odds': odds_test[:, 1]\n",
    "    })\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    results_df.to_csv('predictions_with_odds.csv', index=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "with torch.no_grad():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristics\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  A_Odds  B_Odds\n",
      "0        0.0   0.190110    6.73    1.11\n",
      "1        0.0   0.425913    2.15    1.67\n",
      "2        0.0   0.690066    1.51    2.48\n",
      "3        1.0   0.352727    2.70    1.43\n",
      "4        1.0   0.707952    1.71    2.10\n",
      "...      ...        ...     ...     ...\n",
      "3478     0.0   0.256129    2.61    1.47\n",
      "3479     1.0   0.496119    1.48    2.63\n",
      "3480     0.0   0.354244    3.26    1.32\n",
      "3481     0.0   0.404170    2.35    1.58\n",
      "3482     0.0   0.615572    3.05    1.36\n",
      "\n",
      "[3483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $10 bets: 204.00 on a total # bets: 268 from a total of 3483 games\n",
      "Amount of differing favorites 0.14786103933390754\n",
      "Amount of upset correct 0.4679611650485437 won $204.7 on 515 bets\n",
      "Amount of incorrect bets : 0.5410447761194029\n",
      "Correct Bets: 0.458955223880597\n",
      "Model % Correct : 0.6873385012919897 Vegas Correct % : 0.7025552684467413\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "upset_predict = 0\n",
    "upset_correct = 0\n",
    "upset_won = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = results_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .5\n",
    "confidence_top_pct = .55\n",
    "\n",
    "UNIT = 10\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct and row['Predicted'] > 1/row['A_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['A_Odds']-1) * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            total_won += (row['B_Odds']-1) * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        upset_predict += 1\n",
    "        if round(row['Predicted']) == round(row['Actual']):\n",
    "            upset_correct += 1\n",
    "            if(row['Actual'] == 1):\n",
    "                upset_won += (row['A_Odds']-1) * UNIT\n",
    "            else:\n",
    "                upset_won += (row['B_Odds']-1) * UNIT\n",
    "        else:\n",
    "            upset_won -= UNIT\n",
    "\n",
    "    if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "        diff_fav += 1\n",
    "\n",
    "    if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "        # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "        vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on ${UNIT} bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of upset correct {upset_correct/upset_predict} won ${upset_won} on {upset_predict} bets\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
