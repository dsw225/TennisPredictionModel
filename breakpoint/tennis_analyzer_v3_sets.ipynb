{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tennis Analyzer ML Model V2</h1>\n",
    "<h3 style=\"text-align: center;\">Dan Warnick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To start we will begin by selecting the data points we want to analyze with existing known results. For each data entry we will have two players each with the following data entries.</p>\n",
    "<table style=\"font-size: .8em;\">\n",
    "    <tr>\n",
    "        <th>Player Name</th>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Along with match facts like Clay/Hard/Grass Court or Indoor/Outdoor. In the future may want to add weather and adjust certain parameters for more accuracy and more data points to train from, however for now this seems a good start.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.) Collect Data Efficiently</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "import os\n",
    "from django.db import models\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.utils as utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from asgiref.sync import sync_to_async # type: ignore\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'breakpoint.settings')\n",
    "# django.setup()\n",
    "\n",
    "# from render.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20130101'\n",
    "end = '20231231'\n",
    "match_type = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21365\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(start, '%Y%m%d').date()\n",
    "end_date = datetime.strptime(end, '%Y%m%d').date()\n",
    "\n",
    "# if match_type == 'm':   \n",
    "#     typer = MensTennisMatch\n",
    "#     insert_db = MensTennisMatchStats\n",
    "# else:\n",
    "#     typer = WomensTennisMatch\n",
    "#     insert_db = WomensTennisMatchStats\n",
    "\n",
    "# query = insert_db.objects.filter(\n",
    "#         tourney_date__range=(start_date, end_date)\n",
    "#     ).order_by('tourney_date')\n",
    "    \n",
    "# games = await sync_to_async(list)(query.all().values())\n",
    "\n",
    "df = pd.read_csv('testcsvs/glickoalltestw.csv')\n",
    "df['tourney_date'] = pd.to_datetime(df['tourney_date']).dt.date\n",
    "df = df[(df['tourney_date'] >= start_date) & (df['tourney_date'] <= end_date)]\n",
    "\n",
    "df = df.drop(['tourney_id', 'tourney_name', 'match_num', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug', 'a_b_win', 'games', 'tiebreaks'], axis=1)\n",
    "\n",
    "# df = pd.DataFrame(games).drop(['tourney_id', 'tourney_name', 'tourney_date', 'a_player_name', 'b_player_name', 'a_player_id', 'a_player_slug', 'b_player_id', 'b_player_slug','a_win_percent', 'a_serve_rating', 'a_return_rating', 'a_pressure_rating', 'a_avg_vs_elo', 'a_matches_played', 'b_win_percent', 'b_serve_rating', 'b_return_rating', 'b_pressure_rating', 'b_avg_vs_elo', 'b_matches_played', 'A_Odds', 'b_odds'], axis=1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot Encode the 'category_text' column\n",
    "category_encoded = one_hot_encoder.fit_transform(df[['surface']])\n",
    "\n",
    "# Convert to DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['surface']))\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame\n",
    "df = pd.concat([df, category_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'category_text' column\n",
    "df.drop('surface', axis=1, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_of</th>\n",
       "      <th>a_player_rank</th>\n",
       "      <th>b_player_rank</th>\n",
       "      <th>glicko_rating_diff_high</th>\n",
       "      <th>glicko_rating_diff_low</th>\n",
       "      <th>a_glicko_rating</th>\n",
       "      <th>b_glicko_rating</th>\n",
       "      <th>a_glicko_rd</th>\n",
       "      <th>b_glicko_rd</th>\n",
       "      <th>point_glicko_rating_diff_high</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_high</th>\n",
       "      <th>surface_return_second_won_glicko_rating_diff_low</th>\n",
       "      <th>a_surface_return_second_won_glicko_rating</th>\n",
       "      <th>b_surface_second_won_glicko_rating</th>\n",
       "      <th>sets</th>\n",
       "      <th>a_odds</th>\n",
       "      <th>b_odds</th>\n",
       "      <th>surface_Clay</th>\n",
       "      <th>surface_Grass</th>\n",
       "      <th>surface_Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>...</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>...</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>...</td>\n",
       "      <td>-700.317227</td>\n",
       "      <td>700.317227</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-835.992193</td>\n",
       "      <td>379.540857</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1728.225668</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>257.607912</td>\n",
       "      <td>-621.960211</td>\n",
       "      <td>...</td>\n",
       "      <td>-545.537787</td>\n",
       "      <td>531.389034</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1507.074376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-777.493699</td>\n",
       "      <td>439.223422</td>\n",
       "      <td>1330.864862</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>258.199947</td>\n",
       "      <td>350.158613</td>\n",
       "      <td>-604.508547</td>\n",
       "      <td>...</td>\n",
       "      <td>-548.874561</td>\n",
       "      <td>527.275497</td>\n",
       "      <td>1489.200468</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_of  a_player_rank  b_player_rank  glicko_rating_diff_high  \\\n",
       "0      3.0            7.0            8.0              -700.317227   \n",
       "1      3.0            2.0            5.0              -700.317227   \n",
       "2      3.0            4.0            1.0              -700.317227   \n",
       "3      3.0            6.0            1.0              -835.992193   \n",
       "4      3.0            4.0            9.0              -777.493699   \n",
       "\n",
       "   glicko_rating_diff_low  a_glicko_rating  b_glicko_rating  a_glicko_rd  \\\n",
       "0              700.317227      1500.000000      1500.000000   350.158613   \n",
       "1              700.317227      1500.000000      1500.000000   350.158613   \n",
       "2              700.317227      1500.000000      1500.000000   350.158613   \n",
       "3              379.540857      1500.000000      1728.225668   350.158613   \n",
       "4              439.223422      1330.864862      1500.000000   258.199947   \n",
       "\n",
       "   b_glicko_rd  point_glicko_rating_diff_high  ...  \\\n",
       "0   350.158613                    -700.317227  ...   \n",
       "1   350.158613                    -700.317227  ...   \n",
       "2   350.158613                    -700.317227  ...   \n",
       "3   257.607912                    -621.960211  ...   \n",
       "4   350.158613                    -604.508547  ...   \n",
       "\n",
       "   surface_return_second_won_glicko_rating_diff_high  \\\n",
       "0                                        -700.317227   \n",
       "1                                        -700.317227   \n",
       "2                                        -700.317227   \n",
       "3                                        -545.537787   \n",
       "4                                        -548.874561   \n",
       "\n",
       "   surface_return_second_won_glicko_rating_diff_low  \\\n",
       "0                                        700.317227   \n",
       "1                                        700.317227   \n",
       "2                                        700.317227   \n",
       "3                                        531.389034   \n",
       "4                                        527.275497   \n",
       "\n",
       "   a_surface_return_second_won_glicko_rating  \\\n",
       "0                                1500.000000   \n",
       "1                                1500.000000   \n",
       "2                                1500.000000   \n",
       "3                                1500.000000   \n",
       "4                                1489.200468   \n",
       "\n",
       "   b_surface_second_won_glicko_rating  sets  a_odds  b_odds  surface_Clay  \\\n",
       "0                         1500.000000   0.0     NaN     NaN           0.0   \n",
       "1                         1500.000000   0.0     NaN     NaN           0.0   \n",
       "2                         1500.000000   0.0     NaN     NaN           0.0   \n",
       "3                         1507.074376   0.0     NaN     NaN           0.0   \n",
       "4                         1500.000000   0.0     NaN     NaN           0.0   \n",
       "\n",
       "   surface_Grass  surface_Hard  \n",
       "0            0.0           1.0  \n",
       "1            0.0           1.0  \n",
       "2            0.0           1.0  \n",
       "3            0.0           1.0  \n",
       "4            0.0           1.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "# df = df[~(df == 1500).any(axis=1)]\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Plot a_elo_rating vs b_elo_rating for a_b_win == 1\n",
    "# x1 = df[df['a_b_win'] == 1]['a_recent_elo_rating']\n",
    "# y1 = df[df['a_b_win'] == 1]['b_recent_elo_rating']\n",
    "# plt.scatter(x1, y1, color='blue', label='Favorite Wins', s=.5, alpha=0.5)\n",
    "\n",
    "# # Plot b_elo_rating vs a_elo_rating for a_b_win == 0\n",
    "# x2 = df[df['a_b_win'] == 0]['b_recent_elo_rating']\n",
    "# y2 = df[df['a_b_win'] == 0]['a_recent_elo_rating']\n",
    "# plt.scatter(x2, y2, color='orange', label='Upset', s=.5, alpha=0.5)\n",
    "\n",
    "# # Combine data for a single trendline\n",
    "# combined_x = np.concatenate([x1, x2])\n",
    "# combined_y = np.concatenate([y1, y2])\n",
    "\n",
    "# # Fit a polynomial of degree 2 to the combined data\n",
    "# print(combined_x)\n",
    "# coefficients = np.polyfit(combined_x, combined_y, 3)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(combined_x.min(), combined_x.max(), 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Quadratic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(1100, 2300)\n",
    "# plt.ylim(1100, 2300)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Winner')\n",
    "# plt.ylabel('ELO Rating Loser')\n",
    "# plt.title('Scatter Plot of ELO Ratings based on Win/Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# df['elo_diff'] = df['a_recent_elo_rating']\n",
    "\n",
    "# # Create bins for ELO difference in intervals of 10\n",
    "# bins = np.arange(df['elo_diff'].min(), df['elo_diff'].max() + 10, 10)\n",
    "# labels = (bins[:-1] + bins[1:]) / 2\n",
    "# df['elo_diff_bin'] = pd.cut(df['elo_diff'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Calculate average win rate at each ELO difference bin\n",
    "# average_win_rate = df.groupby('elo_diff_bin')['a_b_win'].mean().reset_index()\n",
    "# average_win_rate.columns = ['elo_diff_bin', 'avg_win_rate']\n",
    "\n",
    "# # Convert the bin labels to numeric values\n",
    "# average_win_rate['elo_diff_bin'] = average_win_rate['elo_diff_bin'].astype(float)\n",
    "\n",
    "# average_win_rate = average_win_rate[~np.isnan(average_win_rate).any(axis=1)]\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.scatter(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], color='blue', label='Average Win Rate', s=10, alpha=0.5)\n",
    "\n",
    "# # Fit a polynomial of degree 3 to the average win rate data\n",
    "# coefficients = np.polyfit(average_win_rate['elo_diff_bin'], average_win_rate['avg_win_rate'], 5)\n",
    "# polynomial = np.poly1d(coefficients)\n",
    "# trendline_x = np.linspace(-600, 600, 100)\n",
    "# trendline_y = polynomial(trendline_x)\n",
    "# plt.plot(trendline_x, trendline_y, color='green', linewidth=1, label='Cubic Trendline')\n",
    "\n",
    "# # Setting the limits for x and y axis\n",
    "# plt.xlim(-600, 600)\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.xlabel('ELO Rating Difference (Winner - Loser)')\n",
    "# plt.ylabel('Average Win Rate')\n",
    "# plt.title('Scatter Plot of ELO Rating Difference vs. Win Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutcomeProbability(nn.Module):\n",
    "    def __init__(self, input_dim=36, dropout_prob=0.33):\n",
    "        hidden_dim = int(input_dim * (2/3))\n",
    "        super(OutcomeProbability, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # No sigmoid activation here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/427], Loss: 0.5632\n",
      "Epoch [50/427], Loss: 0.5935\n",
      "Epoch [75/427], Loss: 0.5977\n",
      "Epoch [100/427], Loss: 0.6401\n",
      "Epoch [125/427], Loss: 0.5222\n",
      "Epoch [150/427], Loss: 0.5964\n",
      "Epoch [175/427], Loss: 0.5672\n",
      "Epoch [200/427], Loss: 0.5044\n",
      "Epoch [225/427], Loss: 0.5455\n",
      "Epoch [250/427], Loss: 0.6694\n",
      "Epoch [275/427], Loss: 0.4711\n",
      "Epoch [300/427], Loss: 0.6684\n",
      "Epoch [325/427], Loss: 0.5101\n",
      "Epoch [350/427], Loss: 0.5536\n",
      "Epoch [375/427], Loss: 0.5970\n",
      "Epoch [400/427], Loss: 0.5439\n",
      "Epoch [425/427], Loss: 0.7325\n",
      "Test Loss: 0.6452, Accuracy: 66.18%\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df == .5).any(axis=1)]\n",
    "df = df[~(df == -20).any(axis=1)]\n",
    "\n",
    "# Extract odds and keep track of original indices\n",
    "odds_df = df[['a_odds', 'b_odds']].copy()\n",
    "odds_df['index'] = df.index\n",
    "\n",
    "# Drop odds columns from the main DataFrame\n",
    "df = df.drop(columns=['a_odds', 'b_odds'])\n",
    "\n",
    "# Separate features and target variable\n",
    "y = df['sets'].values\n",
    "X = df.drop('sets', axis=1).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, odds_df['index'].values, test_size=1/5, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_dataset = data.TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32), \n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "test_dataset = data.TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = OutcomeProbability(input_dim=X.shape[1])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = ceil(len(X_train) / batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        predictions.extend(torch.sigmoid(outputs).numpy())\n",
    "        actuals.extend(labels.numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Convert predictions and actuals to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Create a DataFrame with actuals and predictions\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': actuals,\n",
    "    'Predicted': predictions\n",
    "})\n",
    "comparison_df['index'] = idx_test\n",
    "\n",
    "# Merge the predictions with the original odds DataFrame\n",
    "comparison_test_values = odds_df.set_index('index').loc[idx_test]\n",
    "comparison_df = comparison_df.merge(\n",
    "    comparison_test_values.reset_index(),\n",
    "    on='index',\n",
    "    suffixes=('_model', '_original')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  index  a_odds  b_odds\n",
      "0        0.0   0.299394   2723    1.64    2.21\n",
      "1        0.0   0.339032   9870    1.48    2.68\n",
      "2        0.0   0.394834  21167    3.38    1.31\n",
      "3        0.0   0.306912  11047    1.70    2.13\n",
      "4        0.0   0.228379   5261    2.93    1.38\n",
      "...      ...        ...    ...     ...     ...\n",
      "3410     0.0   0.300419   3809    1.99    1.79\n",
      "3411     1.0   0.372759   4148    2.23    1.64\n",
      "3412     0.0   0.278514   6092    2.39    1.58\n",
      "3413     0.0   0.395016   8291    1.75    2.08\n",
      "3414     0.0   0.316745  11198    1.38    3.07\n",
      "\n",
      "[3415 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.head()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly Criterion\n",
    "def kelly_criterion(vegas_odds, calculated_probability):\n",
    "    # Calculate the Kelly fraction\n",
    "    kelly_fraction = (vegas_odds * calculated_probability - (1 - calculated_probability)) / vegas_odds\n",
    "    \n",
    "    # Ensure that the fraction is not negative\n",
    "    kelly_fraction = max(0, kelly_fraction)\n",
    "    \n",
    "    return kelly_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total won on $1 bets: 101.50 on a total # bets: 42 from a total of 3415 games\n",
      "Amount of differing favorites 0.0\n",
      "Amount of incorrect bets : 0.16666666666666666\n",
      "Correct Bets: 0.8333333333333334\n",
      "Model % Correct : 0.6617862371888726 Vegas Correct % : 0.0\n"
     ]
    }
   ],
   "source": [
    "better = 0\n",
    "total_won = 0\n",
    "diff_fav = 0\n",
    "bet_correct = 0\n",
    "\n",
    "model_correct = 0\n",
    "vegas_correct = 0\n",
    "vegas_total = 0\n",
    "\n",
    "wrong = 0\n",
    "comparison_df = comparison_df.dropna()\n",
    "length = len(comparison_df)\n",
    "\n",
    "confidence_pct = .90\n",
    "confidence_top_pct = 1\n",
    "UNIT = 10\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "\n",
    "    # Games\n",
    "    # if confidence_top_pct > row['Predicted'] > confidence_pct :#and row['Predicted'] > 1/row['A_Odds']:\n",
    "    #     better += 1\n",
    "    #     if(row['Actual'] == 1):\n",
    "    #         bet_correct += 1\n",
    "    #         total_won +=  1.56 * UNIT#(row['A_Odds']-1) * UNIT\n",
    "    #     else:\n",
    "    #         wrong += 1\n",
    "    #         total_won -= UNIT\n",
    "\n",
    "    # if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct :#and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "    #     better += 1\n",
    "    #     if(row['Actual'] == 0):\n",
    "    #         bet_correct += 1\n",
    "    #         total_won += .49 * UNIT #(row['B_Odds']-1) * UNIT\n",
    "    #     else:\n",
    "    #         wrong += 1\n",
    "    #         total_won -= UNIT\n",
    "\n",
    "    # Sets\n",
    "    if confidence_top_pct > row['Predicted'] > confidence_pct: #and row['Predicted'] > 1/row['A_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 1):\n",
    "            bet_correct += 1\n",
    "            # total_won += row['A_Odds']-1\n",
    "            total_won += 1.55 * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    if 1-confidence_top_pct < row['Predicted'] < 1-confidence_pct :#and 1-row['Predicted'] > 1/row['B_Odds']:\n",
    "        better += 1\n",
    "        if(row['Actual'] == 0):\n",
    "            bet_correct += 1\n",
    "            # total_won += row['B_Odds']-1\n",
    "            total_won += .49 * UNIT\n",
    "        else:\n",
    "            wrong += 1\n",
    "            total_won -= UNIT\n",
    "\n",
    "    # if round(row['Predicted']) != round(1/row['A_Odds']):\n",
    "    #     diff_fav += 1\n",
    "\n",
    "    # if row['Actual']==1 and row['A_Odds'] < row['B_Odds']:\n",
    "    #     # print(f\"A odds : {row['a_odds']} B odds : {row['b_odds']}\")\n",
    "    #     vegas_correct += 1\n",
    "\n",
    "    # if row['Actual']==0 and row['A_Odds'] > row['B_Odds']:\n",
    "    #     vegas_correct += 1\n",
    "\n",
    "    if round(row['Predicted']) == round(row['Actual']):\n",
    "        model_correct += 1\n",
    "\n",
    "\n",
    "print(f\"Total won on $1 bets: {total_won:.2f} on a total # bets: {better} from a total of {length} games\")\n",
    "print(f\"Amount of differing favorites {diff_fav/length}\")\n",
    "print(f\"Amount of incorrect bets : {wrong/better}\")\n",
    "print(f\"Correct Bets: {bet_correct/better}\")\n",
    "print(f\"Model % Correct : {model_correct/length} Vegas Correct % : {vegas_correct/length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
